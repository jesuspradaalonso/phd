{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DeepSVR Experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select Dataset for Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select 'housing' or 'space_ga'\n",
    "dataset='housing'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Standard Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/falendor/.conda/envs/tensorflow/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np # General numerical operations\n",
    "from sklearn.neural_network import MLPRegressor # Standard Neural Net\n",
    "import tensorflow as tf # Deep Learning\n",
    "from tensorflow.python.framework import ops # Tensorflow options\n",
    "import pandas as pd # Data frame\n",
    "import matplotlib.pyplot as plt # Plotting function\n",
    "import sklearn.datasets as sk_dat;\n",
    "from sklearn.utils import shuffle;\n",
    "from sklearn.metrics import make_scorer, mean_absolute_error,accuracy_score, r2_score\n",
    "from sklearn.model_selection import GridSearchCV;\n",
    "from sklearn.pipeline import Pipeline;\n",
    "from sklearn.preprocessing import StandardScaler;\n",
    "from sklearn.svm import SVC, SVR;\n",
    "from sklearn.base import (BaseEstimator, ClassifierMixin, RegressorMixin,TransformerMixin);\n",
    "from sklearn.utils.validation import check_array, check_is_fitted, check_X_y;\n",
    "from keras.layers import Dense, Input, TimeDistributed;\n",
    "from keras.regularizers import l1 as l1_, l2 as l2_, l1_l2 as l1_l2_;\n",
    "from keras.models import Model, load_model, save_model;\n",
    "from keras.optimizers import (Adadelta, Adagrad, Adam, Adamax, Nadam, RMSprop,\n",
    "SGD);\n",
    "from keras import backend as K;\n",
    "from keras.callbacks import EarlyStopping;\n",
    "from sklearn.datasets import (get_data_home, load_svmlight_file,\n",
    "                              load_svmlight_files);\n",
    "from os.path import basename, exists, expanduser, join, normpath, splitext;\n",
    "from os import environ, makedirs, remove;\n",
    "from bz2 import decompress\n",
    "from os import environ, makedirs, remove\n",
    "from os.path import basename, exists, expanduser, join, normpath, splitext\n",
    "from shutil import copyfileobj\n",
    "from sklearn.datasets import (get_data_home, load_svmlight_file,\n",
    "                              load_svmlight_files)\n",
    "from sklearn.datasets.base import Bunch\n",
    "from urllib.error import HTTPError\n",
    "from urllib.request import urlopen\n",
    "from zipfile import ZipFile\n",
    "from sklearn.model_selection import cross_val_score;\n",
    "from skopt import BayesSearchCV;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Custom Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Architecture(BaseEstimator):\n",
    "\n",
    "    \"\"\"Architecture.\n",
    "    Architecture class.\n",
    "    Parameters\n",
    "    ----------\n",
    "    transformer: keras function, default=None\n",
    "                 Feature transformation.\n",
    "    activation: string/function, default='linear'/'softmax'\n",
    "                Activation function to use.\n",
    "    use_bias: boolean, default=True\n",
    "              Whether the layer uses a bias vector.\n",
    "    kernel_initializer: string/function, default='glorot_uniform'\n",
    "                        Initializer for the kernel weights matrix.\n",
    "    bias_initializer: string/function, default='zeros'\n",
    "                      Initializer for the bias vector.\n",
    "    kernel_regularizer_l1: float, default=None\n",
    "                           L1 regularization factor applied to the kernel\n",
    "                           weights matrix.\n",
    "    kernel_regularizer_l2: float, default=None\n",
    "                           L2 regularization factor applied to the kernel\n",
    "                           weights matrix.\n",
    "    bias_regularizer_l1: float, default=None\n",
    "                         L1 regularization factor applied to the bias vector.\n",
    "    bias_regularizer_l2: float, default=None\n",
    "                         L2 regularization factor applied to the bias vector.\n",
    "    activity_regularizer_l1: float, default=None\n",
    "                             L1 regularization factor applied to the output of\n",
    "                             the layer.\n",
    "    activity_regularizer_l2: float, default=None\n",
    "                             L2 regularization factor applied to the output of\n",
    "                             the layer.\n",
    "    kernel_constraint: function, default=None\n",
    "                       Constraint function applied to the kernel weights matrix.\n",
    "    bias_constraint: function, default=None\n",
    "                     Constraint function applied to the bias vector.\n",
    "    return_sequences: boolean, default=False\n",
    "                      Whether to return the last output in the output sequence,\n",
    "                      or the full sequence.\n",
    "    Returns\n",
    "    -------\n",
    "    Model\n",
    "    \"\"\"\n",
    "\n",
    "    def __new__(cls, X, y, transformer=None, activation='linear', use_bias=True,\n",
    "                kernel_initializer='glorot_uniform', bias_initializer='zeros',\n",
    "                kernel_regularizer_l1=None, kernel_regularizer_l2=None,\n",
    "                bias_regularizer_l1=None, bias_regularizer_l2=None,\n",
    "                activity_regularizer_l1=None, activity_regularizer_l2=None,\n",
    "                kernel_constraint=None, bias_constraint=None,\n",
    "                return_sequences=False):\n",
    "        z = inputs = Input(shape=X.shape[1:])\n",
    "        if transformer is not None: z = transformer(z)\n",
    "        layer = Dense(int(np.prod(y.shape[1:])), activation=activation,\n",
    "                      use_bias=use_bias,\n",
    "                      kernel_initializer=kernel_initializer,\n",
    "                      bias_initializer=bias_initializer,\n",
    "                      kernel_regularizer=Regularizer(l1=kernel_regularizer_l1,\n",
    "                                                     l2=kernel_regularizer_l2),\n",
    "                      bias_regularizer=Regularizer(l1=bias_regularizer_l1,\n",
    "                                                   l2=bias_regularizer_l2),\n",
    "                      activity_regularizer=Regularizer(l1=activity_regularizer_l1,\n",
    "                                                       l2=activity_regularizer_l2),\n",
    "                      kernel_constraint=kernel_constraint,\n",
    "                      bias_constraint=bias_constraint)\n",
    "        if return_sequences: layer = TimeDistributed(layer)\n",
    "        output = layer(z)\n",
    "        return Model(inputs, output)\n",
    "\n",
    "\n",
    "def _time_series(X, y=None, window=None, return_sequences=False):\n",
    "    \"\"\"Time series transformation.\n",
    "    Transform X, y tensors to time series tensors.\n",
    "    Parameters\n",
    "    ----------\n",
    "    X: numpy array of shape [n_samples, n_features]\n",
    "       Training set.\n",
    "    y: numpy array of shape [n_samples]\n",
    "       Target values.\n",
    "    window: integer, default=None\n",
    "            Time window length.\n",
    "    return_sequences: boolean, default=False\n",
    "                      Whether to return the last output in the output sequence,\n",
    "                      or the full sequence.\n",
    "    Returns\n",
    "    -------\n",
    "    Time series tensors.\n",
    "    \"\"\"\n",
    "    if window is not None:\n",
    "        X = np.array([X[i:i + window] for i in range(X.shape[0] - window + 1)])\n",
    "        if y is not None:\n",
    "            y = np.array([y[i:i + window] for i in range(y.shape[0] - window + 1)]) if return_sequences else y[window - 1:]\n",
    "    return X, y\n",
    "\n",
    "class Optimizer():\n",
    "\n",
    "    \"\"\"Optimizer.\n",
    "    Optimizer class.\n",
    "    Parameters\n",
    "    ----------\n",
    "    optimizer: {\"sgd\", \"rmsprop\", \"adagrad\", \"adadelta\", \"adam\", \"adamax\",\n",
    "               \"nadam\"}, default='adam'\n",
    "               Optimizer\n",
    "    lr: float>=0, default=0.001\n",
    "        Learning rate.\n",
    "    momentum: float>=0, default=0.0\n",
    "              Parameter updates momentum.\n",
    "    nesterov: boolean, default=False\n",
    "              Whether to apply Nesterov momentum.\n",
    "    decay: float>=0, default=0.0\n",
    "           Learning rate decay over each update.\n",
    "    rho: float>=0, default=0.9\n",
    "    epsilon: float>=0, default=1e-08\n",
    "             Fuzz factor.\n",
    "    beta_1: float in (0, 1), default=0.9\n",
    "    beta_2: float in (0, 1), default=0.999\n",
    "    schedule_decay: , default=0.004\n",
    "    Returns\n",
    "    -------\n",
    "    Optimizer\n",
    "    \"\"\"\n",
    "\n",
    "    def __new__(cls, optimizer='adam', lr=0.001, momentum=0.0, nesterov=False,\n",
    "                decay=0.0, rho=0.9, epsilon=1e-08, beta_1=0.9, beta_2=0.999,\n",
    "                schedule_decay=0.004):\n",
    "        optimizers = {'sgd':  SGD(lr=lr, momentum=momentum, decay=decay,\n",
    "                                  nesterov=nesterov),\n",
    "                      'rmsprop': RMSprop(lr=lr, rho=rho, epsilon=epsilon,\n",
    "                                         decay=decay),\n",
    "                      'adagrad': Adagrad(lr=lr, epsilon=epsilon, decay=decay),\n",
    "                      'adadelta': Adadelta(lr=lr, rho=rho, epsilon=epsilon,\n",
    "                                           decay=decay),\n",
    "                      'adam': Adam(lr=lr, beta_1=beta_1, beta_2=beta_2,\n",
    "                                   epsilon=epsilon, decay=decay),\n",
    "                      'adamax': Adamax(lr=lr, beta_1=beta_1, beta_2=beta_2,\n",
    "                                       epsilon=epsilon, decay=decay),\n",
    "                      'nadam': Nadam(lr=lr, beta_1=beta_1, beta_2=beta_2,\n",
    "                                     epsilon=epsilon,\n",
    "                                     schedule_decay=schedule_decay)}\n",
    "        return optimizers[optimizer]\n",
    "    \n",
    "class Regularizer():\n",
    "\n",
    "    \"\"\"Regularizer.\n",
    "    Regularizer class.\n",
    "    Parameters\n",
    "    ----------\n",
    "    l1: float, default=None\n",
    "        L1 regularization factor.\n",
    "    l2: float, default=None\n",
    "        L2 regularization factor.\n",
    "    Returns\n",
    "    -------\n",
    "    Regularizer.\n",
    "    \"\"\"\n",
    "\n",
    "    def __new__(cls, l1=None, l2=None):\n",
    "        if (l1 is None) and (l2 is not None):\n",
    "            regularizer = l2_(l=l2)\n",
    "        elif (l1 is not None) and (l2 is None):\n",
    "            regularizer = l1_(l=l1)\n",
    "        elif (l1 is not None) and (l2 is not None):\n",
    "            regularizer = l1_l2_(l1=l1, l2=l2)\n",
    "        else:\n",
    "            regularizer = None\n",
    "        return regularizer\n",
    "\n",
    "class BaseFeedForward(BaseEstimator, TransformerMixin):\n",
    "\n",
    "    \"\"\"Feed-forward regressor/classifier.\n",
    "    This model optimizes the MSE/categorical-crossentropy function using\n",
    "    back-propagation.\n",
    "    Parameters\n",
    "    ----------\n",
    "    transformer: keras function, default=None\n",
    "                 Feature transformation.\n",
    "    activation: string/function, default='linear'/'softmax'\n",
    "                Activation function to use.\n",
    "    use_bias: boolean, default=True\n",
    "              Whether the layer uses a bias vector.\n",
    "    kernel_initializer: string/function, default='glorot_uniform'\n",
    "                        Initializer for the kernel weights matrix.\n",
    "    bias_initializer: string/function, default='zeros'\n",
    "                      Initializer for the bias vector.\n",
    "    kernel_regularizer_l1: float, default=None\n",
    "                           L1 regularization factor applied to the kernel\n",
    "                           weights matrix.\n",
    "    kernel_regularizer_l2: float, default=None\n",
    "                           L2 regularization factor applied to the kernel\n",
    "                           weights matrix.\n",
    "    bias_regularizer_l1: float, default=None\n",
    "                         L1 regularization factor applied to the bias vector.\n",
    "    bias_regularizer_l2: float, default=None\n",
    "                         L2 regularization factor applied to the bias vector.\n",
    "    activity_regularizer_l1: float, default=None\n",
    "                             L1 regularization factor applied to the output of\n",
    "                             the layer.\n",
    "    activity_regularizer_l2: float, default=None\n",
    "                             L2 regularization factor applied to the output of\n",
    "                             the layer.\n",
    "    kernel_constraint: function, default=None\n",
    "                       Constraint function applied to the kernel weights matrix.\n",
    "    bias_constraint: function, default=None\n",
    "                     Constraint function applied to the bias vector.\n",
    "    optimizer: {\"sgd\", \"rmsprop\", \"adagrad\", \"adadelta\", \"adam\", \"adamax\",\n",
    "               \"nadam\"}, default='adam'\n",
    "               Optimizer\n",
    "    lr: float>=0, default=0.001\n",
    "        Learning rate.\n",
    "    momentum: float>=0, default=0.0\n",
    "              Parameter updates momentum.\n",
    "    nesterov: boolean, default=False\n",
    "              Whether to apply Nesterov momentum.\n",
    "    decay: float>=0, default=0.0\n",
    "           Learning rate decay over each update.\n",
    "    rho: float>=0, default=0.9\n",
    "    epsilon: float>=0, default=1e-08\n",
    "             Fuzz factor.\n",
    "    beta_1: float in (0, 1), default=0.9\n",
    "    beta_2: float in (0, 1), default=0.999\n",
    "    schedule_decay: , default=0.004\n",
    "    loss: string/function, default='mse'/'categorical_crossentropy'\n",
    "          Loss function.\n",
    "    metrics: list, default=None\n",
    "             List of metrics to be evaluated by the model during training and\n",
    "             testing.\n",
    "    loss_weights: list or dictionary, default=None\n",
    "                  Scalar coefficients to weight the loss contributions of\n",
    "                  different model outputs.\n",
    "    sample_weight_mode: {\"temporal\", None}, default=None\n",
    "                        Timestep-wise sample weighting.\n",
    "    batch_size: integer, default='auto'\n",
    "                Number of samples per gradient update.\n",
    "    epochs: integer, default=200\n",
    "            The number of times to iterate over the training data arrays.\n",
    "    verbose: {0, 1, 2}, default=2\n",
    "             Verbosity mode. 0=silent, 1=verbose, 2=one log line per epoch.\n",
    "    early_stopping: bool, default True\n",
    "                    Whether to use early stopping to terminate training when\n",
    "                    validation score is not improving.\n",
    "    tol: float, default 1e-4\n",
    "         Tolerance for the optimization.\n",
    "    patience: integer, default 2\n",
    "              Number of epochs with no improvement after which training will\n",
    "              be stopped.\n",
    "    validation_split: float in [0, 1], default=0.1\n",
    "                      Fraction of the training data to be used as validation\n",
    "                      data.\n",
    "    validation_data: array-like, shape ((n_samples, features_shape),\n",
    "                                        (n_samples, targets_shape)),\n",
    "                     default=None\n",
    "                     Data on which to evaluate the loss and any model metrics at\n",
    "                     the end of each epoch.\n",
    "    shuffle: boolean, default=True\n",
    "             Whether to shuffle the training data before each epoch.\n",
    "    class_weight: dictionary, default=None\n",
    "                  class indices to weights to apply to the model's loss for the\n",
    "                  samples from each class during training.\n",
    "    sample_weight: array-like, shape (n_samples), default=None\n",
    "                   Weights to apply to the model's loss for each sample.\n",
    "    initial_epoch: integer, default=0\n",
    "                   Epoch at which to start training.\n",
    "    window: integer, default=None\n",
    "            Time window length.\n",
    "    return_sequences: boolean, default=False\n",
    "                      Whether to return the last output in the output sequence,\n",
    "                      or the full sequence.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, transformer=None, activation='relu', use_bias=True,\n",
    "                 kernel_initializer='glorot_uniform', bias_initializer='zeros',\n",
    "                 kernel_regularizer_l1=None, kernel_regularizer_l2=None,\n",
    "                 bias_regularizer_l1=None, bias_regularizer_l2=None,\n",
    "                 activity_regularizer_l1=None, activity_regularizer_l2=None,\n",
    "                 kernel_constraint=None, bias_constraint=None, optimizer='adam',\n",
    "                 lr=0.001, momentum=0.0, nesterov=False, decay=0.0, rho=0.9,\n",
    "                 epsilon=1e-08, beta_1=0.9, beta_2=0.999, schedule_decay=0.004,\n",
    "                 loss='mse', metrics=None, loss_weights=None,\n",
    "                 sample_weight_mode=None, batch_size='auto', epochs=200,\n",
    "                 verbose=1, early_stopping=True, tol=0.0001, patience=2,\n",
    "                 validation_split=0.1, validation_data=None, shuffle=True,\n",
    "                 class_weight=None, sample_weight=None, initial_epoch=0,\n",
    "                 window=None, return_sequences=False):\n",
    "        for k, v in locals().items():\n",
    "            if k != 'self': self.__dict__[k] = v\n",
    "\n",
    "    def fit(self, X, y, optimizer=None, lr=None, momentum=None, nesterov=None,\n",
    "            decay=None, rho=None, epsilon=None, beta_1=None, beta_2=None,\n",
    "            schedule_decay=None, loss=None, metrics=None, loss_weights=None,\n",
    "            sample_weight_mode=None, batch_size=None, epochs=None, verbose=1,\n",
    "            early_stopping=None, tol=None, patience=None, validation_split=None,\n",
    "            validation_data=None, shuffle=None, class_weight=None,\n",
    "            sample_weight=None, initial_epoch=None):\n",
    "        \"\"\"Fit to data.\n",
    "        Fit model to X.\n",
    "        Parameters\n",
    "        ----------\n",
    "        X: numpy array of shape [n_samples, n_features]\n",
    "           Training set.\n",
    "        y: numpy array of shape [n_samples]\n",
    "           Target values.\n",
    "        optimizer: {\"sgd\", \"rmsprop\", \"adagrad\", \"adadelta\", \"adam\", \"adamax\",\n",
    "                   \"nadam\"}, default='adam'\n",
    "                   Optimizer\n",
    "        lr: float>=0, default=0.001\n",
    "            Learning rate.\n",
    "        momentum: float>=0, default=0.0\n",
    "                  Parameter updates momentum.\n",
    "        nesterov: boolean, default=False\n",
    "                  Whether to apply Nesterov momentum.\n",
    "        decay: float>=0, default=0.0\n",
    "               Learning rate decay over each update.\n",
    "        rho: float>=0, default=0.9\n",
    "        epsilon: float>=0, default=1e-08\n",
    "                 Fuzz factor.\n",
    "        beta_1: float in (0, 1), default=0.9\n",
    "        beta_2: float in (0, 1), default=0.999\n",
    "        schedule_decay: , default=0.004\n",
    "        loss: string/function, default='mse'/'categorical_crossentropy'\n",
    "              Loss function.\n",
    "        metrics: list, default=None\n",
    "                 List of metrics to be evaluated by the model during training\n",
    "                 and testing.\n",
    "        loss_weights: list or dictionary, default=None\n",
    "                      Scalar coefficients to weight the loss contributions of\n",
    "                      different model outputs.\n",
    "        sample_weight_mode: {\"temporal\", None}, default=None\n",
    "                            Timestep-wise sample weighting.\n",
    "        batch_size: integer, default='auto'\n",
    "                    Number of samples per gradient update.\n",
    "        epochs: integer, default=200\n",
    "                The number of times to iterate over the training data arrays.\n",
    "        verbose: {0, 1, 2}, default=1\n",
    "                 Verbosity mode. 0=silent, 1=verbose, 2=one log line per epoch.\n",
    "        early_stopping: bool, default True\n",
    "                        Whether to use early stopping to terminate training\n",
    "                        when validation score is not improving.\n",
    "        tol: float, default 1e-4\n",
    "             Tolerance for the optimization.\n",
    "        patience: integer, default 2\n",
    "                  Number of epochs with no improvement after which training will\n",
    "                  be stopped.\n",
    "        validation_split: float in [0, 1], default=0.1\n",
    "                          Fraction of the training data to be used as validation\n",
    "                          data.\n",
    "        validation_data: array-like, shape ((n_samples, features_shape),\n",
    "                                            (n_samples, targets_shape)),\n",
    "                         default=None\n",
    "                         Data on which to evaluate the loss and any model\n",
    "                         metrics at the end of each epoch.\n",
    "        shuffle: boolean, default=True\n",
    "                 Whether to shuffle the training data before each epoch.\n",
    "        class_weight: dictionary, default=None\n",
    "                      class indices to weights to apply to the model's loss for\n",
    "                      the samples from each class during training.\n",
    "        sample_weight: array-like, shape (n_samples), default=None\n",
    "                       Weights to apply to the model's loss for each sample.\n",
    "        initial_epoch: integer, default=0\n",
    "                       Epoch at which to start training.\n",
    "        Returns\n",
    "        -------\n",
    "        self\n",
    "        \"\"\"\n",
    "        for k, v in locals().items():\n",
    "            if (k != 'self') and (v is not None): self.__dict__[k] = v\n",
    "        X, y = check_X_y(X, y, allow_nd=True, multi_output=True)\n",
    "        if len(y.shape) == 1: y = y.reshape((len(y), 1))\n",
    "        X, y = _time_series(X, y=y, window=self.window,\n",
    "                            return_sequences=self.return_sequences)\n",
    "        self.model_ = Architecture(X, y, transformer=None,\n",
    "                                   activation=self.activation,\n",
    "                                   use_bias=self.use_bias,\n",
    "                                   kernel_initializer=self.kernel_initializer,\n",
    "                                   bias_initializer=self.bias_initializer,\n",
    "                                   kernel_regularizer_l1=self.kernel_regularizer_l1,\n",
    "                                   kernel_regularizer_l2=self.kernel_regularizer_l2,\n",
    "                                   bias_regularizer_l1=self.bias_regularizer_l1,\n",
    "                                   bias_regularizer_l2=self.bias_regularizer_l2,\n",
    "                                   activity_regularizer_l1=self.activity_regularizer_l1,\n",
    "                                   activity_regularizer_l2=self.activity_regularizer_l2,\n",
    "                                   kernel_constraint=self.kernel_constraint,\n",
    "                                   bias_constraint=self.bias_constraint,\n",
    "                                   return_sequences=self.return_sequences)\n",
    "        optimizer = Optimizer(optimizer=self.optimizer, lr=self.lr,\n",
    "                              momentum=self.momentum, nesterov=self.nesterov,\n",
    "                              decay=self.decay, rho=self.rho,\n",
    "                              epsilon=self.epsilon, beta_1=self.beta_1,\n",
    "                              beta_2=self.beta_2,\n",
    "                              schedule_decay=self.schedule_decay)\n",
    "        self.model_.compile(optimizer, self.loss, metrics=self.metrics,\n",
    "                            loss_weights=self.loss_weights,\n",
    "                            sample_weight_mode=self.sample_weight_mode)\n",
    "        callbacks = [EarlyStopping(monitor='val_loss' if (self.validation_split > 0.0 or self.validation_data is not None) else 'loss',\n",
    "                                   min_delta=self.tol, patience=self.patience)] if self.early_stopping and (self.tol > 0.0) else []\n",
    "        self.history_ = self.model_.fit(X, y,\n",
    "                                        batch_size=min(200, len(X)) if self.batch_size == 'auto' else self.batch_size,\n",
    "                                        epochs=self.epochs,\n",
    "                                        verbose=1,\n",
    "                                        callbacks=callbacks,\n",
    "                                        validation_split=self.validation_split,\n",
    "                                        validation_data=self.validation_data,\n",
    "                                        shuffle=self.shuffle,\n",
    "                                        class_weight=self.class_weight,\n",
    "                                        sample_weight=np.asarray(self.sample_weight) if type(self.sample_weight) in (list, tuple) else self.sample_weight,\n",
    "                                        initial_epoch=self.initial_epoch)\n",
    "        return self\n",
    "\n",
    "    def predict(self, X, batch_size=32, verbose=0):\n",
    "        \"\"\"Predict using the trained model.\n",
    "        Parameters\n",
    "        ----------\n",
    "        X: array-like, shape (n_samples, features_shape)\n",
    "           The input data.\n",
    "        batch_size: integer, default=32\n",
    "                    Batch size.\n",
    "        verbose: {0, 1}, default=0\n",
    "                 Verbosity mode.\n",
    "        Returns\n",
    "        -------\n",
    "        y_pred: array-like, shape (n_samples, targets_shape)\n",
    "                Target predictions for X.\n",
    "        \"\"\"\n",
    "        check_is_fitted(self, ['model_', 'history_'])\n",
    "        X = check_array(X, allow_nd=True)\n",
    "        X, _ = _time_series(X, y=None, window=self.window,\n",
    "                            return_sequences=self.return_sequences)\n",
    "        preds = self.model_.predict(X, batch_size=batch_size, verbose=verbose)\n",
    "        return preds.reshape((len(preds))) if (len(preds.shape) == 2 and preds.shape[1] == 1) else preds\n",
    "\n",
    "    def transform(self, X):\n",
    "        \"\"\"Transform using the trained model.\n",
    "        Parameters\n",
    "        ----------\n",
    "        X: array-like, shape (n_samples, features_shape)\n",
    "           The input data.\n",
    "        Returns\n",
    "        -------\n",
    "        Z: array-like, shape (n_samples, last_hidden_layer_shape)\n",
    "           Transformations for X.\n",
    "        \"\"\"\n",
    "        check_is_fitted(self, ['model_', 'history_'])\n",
    "        X = check_array(X, allow_nd=True)\n",
    "        X, _ = _time_series(X, y=None, window=self.window,\n",
    "                            return_sequences=self.return_sequences)\n",
    "        propagate = K.function([self.model_.layers[0].input],\n",
    "                               [self.model_.layers[-2].output])\n",
    "        return propagate([X])[0]\n",
    "\n",
    "    def score(self, X, y, sample_weight=None, metric=r2_score):\n",
    "        \"\"\"Return the score of the model on the data X.\n",
    "        Parameters\n",
    "        ----------\n",
    "        X: array-like, shape (n_samples, features_shape)\n",
    "           Test samples.\n",
    "        y: array-like, shape (n_samples, targets_shape)\n",
    "           Targets for X.\n",
    "        sample_weight: array-like, shape [n_samples], default=None\n",
    "                       Sample weights.\n",
    "        metric: function, default=r2_score/accuracy_score\n",
    "                Metric to be evaluated.\n",
    "        Returns\n",
    "        -------\n",
    "        score: float\n",
    "               r2_score/accuracy of self.predict(X) wrt. y.\n",
    "        \"\"\"\n",
    "        check_is_fitted(self, ['model_', 'history_'])\n",
    "        X, y = check_X_y(X, y, allow_nd=True, multi_output=True)\n",
    "        if len(y.shape) == 1: y = y.reshape((len(y), 1))\n",
    "        _, y = _time_series(X, y=y, window=self.window,\n",
    "                            return_sequences=self.return_sequences)\n",
    "        return metric(y, self.predict(X), sample_weight=sample_weight)\n",
    "\n",
    "class FFRegressor(BaseFeedForward, RegressorMixin):\n",
    "\n",
    "    __doc__ = BaseFeedForward.__doc__\n",
    "    \n",
    "class Straight(BaseEstimator):\n",
    "\n",
    "    \"\"\"Straight feed-forward architecture.\n",
    "    Basic straight feed-forward model architecture.\n",
    "    Parameters\n",
    "    ----------\n",
    "    convolution_filters: integer, default=None\n",
    "                         Dimensionality of the output space.\n",
    "    convolution_kernel_size: integer/tuple/list, default=None\n",
    "                             Dimensionality of the convolution window.\n",
    "    convolution_strides: integer/tuple/list, default=None\n",
    "                         Strides of the convolution.\n",
    "    convolution_padding: {\"valid\", \"same\"}, default='valid'\n",
    "    convolution_dilation_rate: integer/tuple/list, default=None\n",
    "                               Dilation rate to use for dilated convolution.\n",
    "    convolution_activation: string/function, default=None\n",
    "                            Activation function.\n",
    "    convolution_use_bias: boolean, default=True\n",
    "                          Whether the layer uses a bias vector.\n",
    "    convolution_kernel_initializer: string/function, default='glorot_uniform'\n",
    "                                    Initializer for the kernel weights matrix.\n",
    "    convolution_bias_initializer: string/function, default='zeros'\n",
    "                                  Initializer for the bias vector.\n",
    "    convolution_kernel_constraint: function, default=None\n",
    "                                   Constraint function applied to the kernel\n",
    "                                   matrix.\n",
    "    convolution_bias_constraint: function, default=None\n",
    "                                 Constraint function applied to the bias vector.\n",
    "    pooling_type: {\"max\", \"average}, default='max'\n",
    "    pooling_pool_size: integer/tuple/list, default=None\n",
    "                       Factors by which to downscale.\n",
    "    pooling_strides: integer/tuple/list, default=None\n",
    "                     Strides values.\n",
    "    pooling_padding: {\"valid\", \"same\"}, default='valid'\n",
    "    recurrent_type: {\"lstm\", \"gru\"}, default='lstm'\n",
    "    recurrent_units: integer, default=None\n",
    "                     Dimensionality of the output space.\n",
    "    recurrent_activation: string/function, default='tanh'\n",
    "                          Activation function to use.\n",
    "    recurrent_recurrent_activation: string/function, default='hard_sigmoid'\n",
    "                                    Activation function to use for the recurrent\n",
    "                                    step.\n",
    "    recurrent_use_bias: boolean, default=True\n",
    "                        Whether the layer uses a bias vector.\n",
    "    recurrent_kernel_initializer: string/function, default='glorot_uniform'\n",
    "                                  Initializer for the kernel weights matrix.\n",
    "    recurrent_recurrent_initializer: string/function, default='orthogonal'\n",
    "                                     Initializer for the recurrent_kernel\n",
    "                                     weights matrix.\n",
    "    recurrent_bias_initializer: string/function, default='zeros'\n",
    "                                Initializer for the bias vector.\n",
    "    recurrent_unit_forget_bias: boolean, default=True\n",
    "                                If True, add 1 to the bias of the forget gate\n",
    "                                at initialization.\n",
    "    recurrent_kernel_constraint: function, default=None\n",
    "                                 Constraint function applied to the kernel\n",
    "                                 weights matrix.\n",
    "    recurrent_recurrent_constraint: function, default=None\n",
    "                                    Constraint function applied to the\n",
    "                                    recurrent_kernel weights matrix.\n",
    "    recurrent_bias_constraint: function, default=None\n",
    "                               Constraint function applied to the bias vector.\n",
    "    recurrent_dropout: float in [0, 1], default=0.0\n",
    "                       Fraction of the units to drop for the linear\n",
    "                       transformation of the inputs.\n",
    "    recurrent_recurrent_dropout: float in [0, 1], default=0.0\n",
    "                                 Fraction of the units to drop for the linear\n",
    "                                 transformation of the recurrent state.\n",
    "    recurrent_return_sequences: boolean, default=False\n",
    "                                Whether to return the last output in the output\n",
    "                                sequence, or the full sequence.\n",
    "    recurrent_go_backwards: boolean, default=False\n",
    "                            If True, process the input sequence backwards and\n",
    "                            return the reversed sequence.\n",
    "    recurrent_stateful: boolean, default=False\n",
    "                        If True, the last state for each sample at index i in a\n",
    "                        batch will be used as initial state for the sample of\n",
    "                        index i in the following batch.\n",
    "    recurrent_unroll: boolean, default=False\n",
    "                      If True, the network will be unrolled, else a symbolic\n",
    "                      loop will be used.\n",
    "    recurrent_implementation: {0, 1, 2}, default=0\n",
    "    batchnormalization: boolean, default=False\n",
    "                        Whether to perform batch normalization or not.\n",
    "    batchnormalization_axis: integer, default=-1\n",
    "                             The axis that should be normalized (typically the\n",
    "                             features axis).\n",
    "    batchnormalization_momentum: float, default=0.99\n",
    "                                 Momentum for the moving average.\n",
    "    batchnormalization_epsilon: float, default=0.001\n",
    "                                Small float added to variance to avoid dividing\n",
    "                                by zero.\n",
    "    batchnormalization_center: boolean, default=True\n",
    "                               If True, add offset of beta to normalized tensor.\n",
    "                               If False, beta is ignored.\n",
    "    batchnormalization_scale: boolean, default=True\n",
    "                              If True, multiply by gamma. If False, gamma is not\n",
    "                              used.\n",
    "    batchnormalization_beta_initializer: string/function, default='zeros'\n",
    "                                         Initializer for the beta weight.\n",
    "    batchnormalization_gamma_initializer: string/function, default='ones'\n",
    "                                          Initializer for the gamma weight.\n",
    "    batchnormalization_moving_mean_initializer: string/function, default='zeros'\n",
    "                                                Initializer for the moving mean.\n",
    "    batchnormalization_moving_variance_initializer: string/function,\n",
    "                                                    default='ones'\n",
    "                                                    Initializer for the moving\n",
    "                                                    variance.\n",
    "    batchnormalization_beta_constraint: function, default=None\n",
    "                                        Optional constraint for the beta weight.\n",
    "    batchnormalization_gamma_constraint: function, default=None\n",
    "                                         Optional constraint for the gamma\n",
    "                                         weight.\n",
    "    dense_units: integer, default=None\n",
    "                 Dimensionality of the output space.\n",
    "    dense_activation: string/function, default='relu'\n",
    "                      Activation function to use.\n",
    "    dense_use_bias: boolean, default=True\n",
    "                    Whether the layer uses a bias vector.\n",
    "    dense_kernel_initializer: string/function, default='he_uniform'\n",
    "                              Initializer for the kernel weights matrix.\n",
    "    dense_bias_initializer: string/function, default='zeros'\n",
    "                            Initializer for the bias vector.\n",
    "    dense_kernel_constraint: function, default=None\n",
    "                             Constraint function applied to the kernel weights\n",
    "                             matrix.\n",
    "    dense_bias_constraint: function, default=None\n",
    "                           Constraint function applied to the bias vector.\n",
    "    dropout_rate: float in [0, 1], default=0.0\n",
    "                  Fraction of the input units to drop.\n",
    "    dropout_noise_shape: array-like, default=None\n",
    "                         shape of the binary dropout mask that will be\n",
    "                         multiplied with the input.\n",
    "    dropout_seed: integer, default=None\n",
    "                  Random seed.\n",
    "    kernel_regularizer_l1: float, default=None\n",
    "                           L1 regularization factor applied to the kernel\n",
    "                           weights matrix.\n",
    "    kernel_regularizer_l2: float, default=None\n",
    "                           L2 regularization factor applied to the kernel\n",
    "                           weights matrix.\n",
    "    bias_regularizer_l1: float, default=None\n",
    "                         L1 regularization factor applied to the bias vector.\n",
    "    bias_regularizer_l2: float, default=None\n",
    "                         L2 regularization factor applied to the bias vector.\n",
    "    activity_regularizer_l1: float, default=None\n",
    "                             L1 regularization factor applied to the output of\n",
    "                             the layer.\n",
    "    activity_regularizer_l2: float, default=None\n",
    "                             L2 regularization factor applied to the output of\n",
    "                             the layer.\n",
    "    recurrent_regularizer_l1: float, default=None\n",
    "                              L1 regularization factor applied to the\n",
    "                              recurrent_kernel weights matrix.\n",
    "    recurrent_regularizer_l2: float, default=None\n",
    "                              L2 regularization factor applied to the\n",
    "                              recurrent_kernel weights matrix.\n",
    "    beta_regularizer_l1: float, default=None\n",
    "                         L1 regularization factor applied to the beta weight.\n",
    "    beta_regularizer_l2: float, default=None\n",
    "                         L2 regularization factor applied to the beta weight.\n",
    "    gamma_regularizer_l1: float, default=None\n",
    "                          L1 regularization factor applied to the gamma  weight.\n",
    "    gamma_regularizer_l2: float, default=None\n",
    "                          L2 regularization factor applied to the gamma  weight.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, convolution_filters=None, convolution_kernel_size=None,\n",
    "                 convolution_strides=None, convolution_padding='valid',\n",
    "                 convolution_dilation_rate=None, convolution_activation=None,\n",
    "                 convolution_use_bias=True,\n",
    "                 convolution_kernel_initializer='glorot_uniform',\n",
    "                 convolution_bias_initializer='zeros',\n",
    "                 convolution_kernel_constraint=None,\n",
    "                 convolution_bias_constraint=None, pooling_type='max',\n",
    "                 pooling_pool_size=None, pooling_strides=None,\n",
    "                 pooling_padding='valid', recurrent_type='lstm',\n",
    "                 recurrent_units=None, recurrent_activation='tanh',\n",
    "                 recurrent_recurrent_activation='hard_sigmoid',\n",
    "                 recurrent_use_bias=True,\n",
    "                 recurrent_kernel_initializer='glorot_uniform',\n",
    "                 recurrent_recurrent_initializer='orthogonal',\n",
    "                 recurrent_bias_initializer='zeros',\n",
    "                 recurrent_unit_forget_bias=True,\n",
    "                 recurrent_kernel_constraint=None,\n",
    "                 recurrent_recurrent_constraint=None,\n",
    "                 recurrent_bias_constraint=None, recurrent_dropout=0.0,\n",
    "                 recurrent_recurrent_dropout=0.0,\n",
    "                 recurrent_return_sequences=False, recurrent_go_backwards=False,\n",
    "                 recurrent_stateful=False, recurrent_unroll=False,\n",
    "                 recurrent_implementation=0, batchnormalization=False,\n",
    "                 batchnormalization_axis=-1, batchnormalization_momentum=0.99,\n",
    "                 batchnormalization_epsilon=0.001,\n",
    "                 batchnormalization_center=True, batchnormalization_scale=True,\n",
    "                 batchnormalization_beta_initializer='zeros',\n",
    "                 batchnormalization_gamma_initializer='ones',\n",
    "                 batchnormalization_moving_mean_initializer='zeros',\n",
    "                 batchnormalization_moving_variance_initializer='ones',\n",
    "                 batchnormalization_beta_constraint=None,\n",
    "                 batchnormalization_gamma_constraint=None, dense_units=None,\n",
    "                 dense_activation='relu', dense_use_bias=True,\n",
    "                 dense_kernel_initializer='he_uniform',\n",
    "                 dense_bias_initializer='zeros', kernel_regularizer_l1=None,\n",
    "                 kernel_regularizer_l2=None, bias_regularizer_l1=None,\n",
    "                 bias_regularizer_l2=None, activity_regularizer_l1=None,\n",
    "                 activity_regularizer_l2=None, recurrent_regularizer_l1=None,\n",
    "                 recurrent_regularizer_l2=None, beta_regularizer_l1=None,\n",
    "                 beta_regularizer_l2=None, gamma_regularizer_l1=None,\n",
    "                 gamma_regularizer_l2=None, dense_kernel_constraint=None,\n",
    "                 dense_bias_constraint=None, dropout_rate=0.0,\n",
    "                 dropout_noise_shape=None, dropout_seed=None):\n",
    "        for k, v in locals().items():\n",
    "            if k != 'self': self.__dict__[k] = v\n",
    "\n",
    "    def _convolve_and_pool(self, x, convolution_filters,\n",
    "                           convolution_kernel_size, convolution_strides,\n",
    "                           convolution_dilation_rate, pooling_pool_size,\n",
    "                           pooling_strides, return_tensors=True,\n",
    "                           return_sequences=False):\n",
    "        if convolution_kernel_size is not None:\n",
    "            conv = {1: Conv1D, 2: Conv2D, 3: Conv3D}\n",
    "            layer = conv[len(convolution_kernel_size)](convolution_filters, convolution_kernel_size,\n",
    "                                                       strides=convolution_strides,\n",
    "                                                       padding=self.convolution_padding,\n",
    "                                                       dilation_rate=convolution_dilation_rate,\n",
    "                                                       activation=self.convolution_activation,\n",
    "                                                       use_bias=self.convolution_use_bias,\n",
    "                                                       kernel_initializer=self.convolution_kernel_initializer,\n",
    "                                                       bias_initializer=self.convolution_bias_initializer,\n",
    "                                                       kernel_regularizer=self._kernel_regularizer,\n",
    "                                                       bias_regularizer=self._bias_regularizer,\n",
    "                                                       activity_regularizer=self._activity_regularizer,\n",
    "                                                       kernel_constraint=self.convolution_kernel_constraint,\n",
    "                                                       bias_constraint=self.convolution_bias_constraint)\n",
    "            if return_sequences: layer = TimeDistributed(layer)\n",
    "            x = layer(x)\n",
    "        if pooling_pool_size is not None:\n",
    "            pool = {'max': {1: MaxPooling1D, 2: MaxPooling2D, 3: MaxPooling3D},\n",
    "                    'average': {1: AveragePooling1D, 2: AveragePooling2D,\n",
    "                                3: AveragePooling3D}}\n",
    "            layer = pool[self.pooling_type][len(pooling_pool_size)](pool_size=pooling_pool_size,\n",
    "                                                                    strides=pooling_strides,\n",
    "                                                                    padding=self.pooling_padding)\n",
    "            if return_sequences: layer = TimeDistributed(layer)\n",
    "            x = layer(x)\n",
    "        if not return_tensors:\n",
    "            layer = Flatten()\n",
    "            if return_sequences: layer = TimeDistributed(layer)\n",
    "            x = layer(x)\n",
    "        return x\n",
    "\n",
    "    def _recur(self, x, units, return_sequences=True):\n",
    "        recur = {'lstm': LSTM, 'gru': GRU}\n",
    "        layer = recur[self.recurrent_type](units, activation=self.recurrent_activation,\n",
    "                                           recurrent_activation=self.recurrent_recurrent_activation,\n",
    "                                           use_bias=self.recurrent_use_bias,\n",
    "                                           kernel_initializer=self.recurrent_kernel_initializer,\n",
    "                                           recurrent_initializer=self.recurrent_recurrent_initializer,\n",
    "                                           bias_initializer=self.recurrent_bias_initializer,\n",
    "                                           unit_forget_bias=self.recurrent_unit_forget_bias,\n",
    "                                           kernel_regularizer=self._kernel_regularizer,\n",
    "                                           recurrent_regularizer=self._recurrent_regularizer,\n",
    "                                           bias_regularizer=self._bias_regularizer,\n",
    "                                           activity_regularizer=self._activity_regularizer,\n",
    "                                           kernel_constraint=self.recurrent_kernel_constraint,\n",
    "                                           recurrent_constraint=self.recurrent_recurrent_constraint,\n",
    "                                           bias_constraint=self.recurrent_bias_constraint,\n",
    "                                           dropout=self.recurrent_dropout,\n",
    "                                           recurrent_dropout=self.recurrent_recurrent_dropout,\n",
    "                                           return_sequences=return_sequences,\n",
    "                                           go_backwards=self.recurrent_go_backwards,\n",
    "                                           stateful=self.recurrent_stateful,\n",
    "                                           unroll=self.recurrent_unroll,\n",
    "                                           implementation=self.recurrent_implementation)\n",
    "        x = layer(x)\n",
    "        return x\n",
    "\n",
    "    def _connect(self, x, units, dropout_noise_shape=None):\n",
    "        if self.batchnormalization:\n",
    "            layer= BatchNormalization(axis=self.batchnormalization_axis,\n",
    "                                      momentum=self.batchnormalization_momentum,\n",
    "                                      epsilon=self.batchnormalization_epsilon,\n",
    "                                      center=self.batchnormalization_center,\n",
    "                                      scale=self.batchnormalization_scale,\n",
    "                                      beta_initializer=self.batchnormalization_beta_initializer,\n",
    "                                      gamma_initializer=self.batchnormalization_gamma_initializer,\n",
    "                                      moving_mean_initializer=self.batchnormalization_moving_mean_initializer,\n",
    "                                      moving_variance_initializer=self.batchnormalization_moving_variance_initializer,\n",
    "                                      beta_regularizer=self._beta_regularizer,\n",
    "                                      gamma_regularizer=self._gamma_regularizer,\n",
    "                                      beta_constraint=self.batchnormalization_beta_constraint,\n",
    "                                      gamma_constraint=self.batchnormalization_gamma_constraint)\n",
    "            if self.recurrent_return_sequences: layer = TimeDistributed(layer)\n",
    "            x = layer(x)\n",
    "        layer = Dense(units, activation=self.dense_activation,\n",
    "                      use_bias=self.dense_use_bias,\n",
    "                      kernel_initializer=self.dense_kernel_initializer,\n",
    "                      bias_initializer=self.dense_bias_initializer,\n",
    "                      kernel_regularizer=self._kernel_regularizer,\n",
    "                      bias_regularizer=self._bias_regularizer,\n",
    "                      activity_regularizer=self._activity_regularizer,\n",
    "                      kernel_constraint=self.dense_kernel_constraint,\n",
    "                      bias_constraint=self.dense_bias_constraint)\n",
    "        if self.recurrent_return_sequences: layer = TimeDistributed(layer)\n",
    "        x = layer(x)\n",
    "        if 0.0 < self.dropout_rate < 1.0:\n",
    "            layer = Dropout(self.dropout_rate, noise_shape=dropout_noise_shape,\n",
    "                            seed=self.dropout_seed)\n",
    "            if self.recurrent_return_sequences: layer = TimeDistributed(layer)\n",
    "            x = layer(x)\n",
    "        return x\n",
    "\n",
    "    def __call__(self, z):\n",
    "        self._kernel_regularizer = Regularizer(l1=self.kernel_regularizer_l1,\n",
    "                                               l2=self.kernel_regularizer_l2)\n",
    "        self._bias_regularizer = Regularizer(l1=self.bias_regularizer_l1,\n",
    "                                             l2=self.bias_regularizer_l2)\n",
    "        self._activity_regularizer = Regularizer(l1=self.activity_regularizer_l1,\n",
    "                                                 l2=self.activity_regularizer_l2)\n",
    "        self._recurrent_regularizer = Regularizer(l1=self.recurrent_regularizer_l1,\n",
    "                                                  l2=self.recurrent_regularizer_l2)\n",
    "        self._beta_regularizer = Regularizer(l1=self.beta_regularizer_l1,\n",
    "                                             l2=self.beta_regularizer_l2)\n",
    "        self._gamma_regularizer = Regularizer(l1=self.gamma_regularizer_l1,\n",
    "                                              l2=self.gamma_regularizer_l2)\n",
    "        if (self.convolution_filters is not None) or (self.convolution_kernel_size is not None):\n",
    "            if len(self.convolution_filters) == len(self.convolution_kernel_size):\n",
    "                if self.convolution_strides is None: self.convolution_strides = [[1] * len(k) for k in self.convolution_kernel_size]\n",
    "                if self.convolution_dilation_rate is None: self.convolution_dilation_rate = [[1] * len(k) for k in self.convolution_kernel_size]\n",
    "                if self.pooling_pool_size is None: self.pooling_pool_size = [None] * len(self.convolution_filters)\n",
    "                if self.pooling_strides is None: self.pooling_strides = [None] * len(self.convolution_filters)\n",
    "                for i, (cf, cks, cs, cdr, pps, ps) in enumerate(zip(self.convolution_filters,\n",
    "                                                                    self.convolution_kernel_size,\n",
    "                                                                    self.convolution_strides,\n",
    "                                                                    self.convolution_dilation_rate,\n",
    "                                                                    self.pooling_pool_size,\n",
    "                                                                    self.pooling_strides)):\n",
    "                    z = self._convolve_and_pool(z, cf, cks, cs, cdr, pps, ps,\n",
    "                                                return_tensors=i < len(self.convolution_filters) - 1,\n",
    "                                                return_sequences=self.recurrent_units is not None)\n",
    "        if self.recurrent_units is not None:\n",
    "            for i, ru in enumerate(self.recurrent_units):\n",
    "                z = self._recur(z, ru,\n",
    "                                return_sequences=i < len(self.recurrent_units) - 1)\n",
    "        if self.dense_units is not None:\n",
    "            if self.dropout_noise_shape is None: self.dropout_noise_shape = [None] * len(self.dense_units)\n",
    "            for (du, dns) in zip(self.dense_units, self.dropout_noise_shape):\n",
    "                z = self._connect(z, du, dropout_noise_shape=dns)\n",
    "        return z\n",
    "    \n",
    "class DSVR(FFRegressor):\n",
    "\n",
    "    def __init__(self, architecture=None, activation='linear', use_bias=True,\n",
    "                 kernel_initializer='glorot_uniform', bias_initializer='zeros',\n",
    "                 kernel_regularizer_l1=None, kernel_regularizer_l2=None,\n",
    "                 bias_regularizer_l1=None, bias_regularizer_l2=None,\n",
    "                 activity_regularizer_l1=None, activity_regularizer_l2=None,\n",
    "                 kernel_constraint=None, bias_constraint=None, optimizer='adam',\n",
    "                 lr=0.001, momentum=0.0, nesterov=False, decay=0.0, rho=0.9,\n",
    "                 epsilon=1e-08, beta_1=0.9, beta_2=0.999, schedule_decay=0.004,\n",
    "                 loss='mse', metrics=None, loss_weights=None,\n",
    "                 sample_weight_mode=None, batch_size='auto', epochs=200,\n",
    "                 verbose=2, early_stopping=True, tol=0.0001, patience=2,\n",
    "                 validation_split=0.1, validation_data=None, shuffle=True,\n",
    "                 class_weight=None, sample_weight=None, initial_epoch=0,\n",
    "                 window=None, return_sequences=False, loss_epsilon=0.1):\n",
    "        for k, v in locals().items():\n",
    "            if k != 'self':\n",
    "                self.__dict__[k] = v\n",
    "\n",
    "    def _epsilon_insensitive(self, y_true, y_pred):\n",
    "        return K.mean(K.maximum(K.abs(y_pred - y_true) - self.loss_epsilon,\n",
    "                                0.0), axis=-1)\n",
    "\n",
    "    def fit(self, X, y, **kwargs):\n",
    "        return FFRegressor.fit(self, X, y, loss=self._epsilon_insensitive,\n",
    "                               **kwargs)\n",
    "    \n",
    "def fetch_file(dataname, urlname, data_home=None):\n",
    "    \"\"\"Fetch dataset.\n",
    "\n",
    "    Fetch a file from a given url and stores it in a given directory.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    dataname: string\n",
    "              Dataset name.\n",
    "    urlname: string\n",
    "             Dataset url.\n",
    "    data_home: string, default=None\n",
    "               Dataset directory.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    filename: string\n",
    "              Name of the file.\n",
    "\n",
    "    \"\"\"\n",
    "    # check if this data set has been already downloaded\n",
    "    data_home = get_data_home(data_home=data_home)\n",
    "    data_home = join(data_home, dataname)\n",
    "    if not exists(data_home):\n",
    "        makedirs(data_home)\n",
    "    filename = join(data_home, basename(normpath(urlname)))\n",
    "    # if the file does not exist, download it\n",
    "    if not exists(filename):\n",
    "        try:\n",
    "            data_url = urlopen(urlname)\n",
    "        except HTTPError as e:\n",
    "            if e.code == 404:\n",
    "                e.msg = \"Dataset '%s' not found.\" % dataname\n",
    "            raise\n",
    "        # store file\n",
    "        try:\n",
    "            with open(filename, 'w+b') as data_file:\n",
    "                copyfileobj(data_url, data_file)\n",
    "        except:\n",
    "            remove(filename)\n",
    "            raise\n",
    "        data_url.close()\n",
    "    return filename\n",
    "\n",
    "def load_train_scale(name, url, url_scale, fetch_file=fetch_file,\n",
    "                     return_X_y=False):\n",
    "    \"\"\"Load dataset with scaled version.\n",
    "\n",
    "    Load a dataset with scaled version.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    name: string\n",
    "          Dataset name.\n",
    "    url: string\n",
    "         Dataset url.\n",
    "    url_scale: string\n",
    "               Scaled dataset url.\n",
    "    fetch_file: function, default=fetch_file\n",
    "                Dataset fetching function.\n",
    "    return_X_y: bool, default=False\n",
    "                If True, returns (data, target) instead of a Bunch object..\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    data: Bunch\n",
    "          Dictionary-like object with all the data and metadata.\n",
    "    X, y: arrays\n",
    "          If return_X_y is True\n",
    "\n",
    "    \"\"\"\n",
    "    filename = fetch_file(name, url)\n",
    "    filename_scale = fetch_file(name, url_scale)\n",
    "    X, y, X_scale, y_scale = load_svmlight_files([filename, filename_scale])\n",
    "    X = X.todense()\n",
    "    X_scale = X_scale.todense()\n",
    "\n",
    "    if return_X_y:\n",
    "        return X, y\n",
    "\n",
    "    return Bunch(data=X, target=y, data_scale=X_scale, target_scale=y_scale)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Global Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "perc_train=0.7;\n",
    "perc_val=0.2;\n",
    "cv=3;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (dataset=='housing'):\n",
    "    X,y=load_train_scale('housing',\n",
    "                            'https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/regression/housing',\n",
    "                            'https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/regression/housing_scale',\n",
    "                            return_X_y=True)\n",
    "    X=pd.DataFrame(X);\n",
    "    X[\"y\"]=y;\n",
    "elif (dataset=='space_ga'):\n",
    "    X,y=load_train_scale('space_ga',\n",
    "                                'https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/regression/space_ga',\n",
    "                                'https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/regression/space_ga_scale',\n",
    "                                return_X_y=True)\n",
    "    X=pd.DataFrame(X);\n",
    "    X[\"y\"]=y;\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(506, 14)\n",
      "            0    1      2    3      4      5     6       7     8      9    10  \\\n",
      "155   3.53501  0.0  19.58  1.0  0.871  6.152  82.6  1.7455   5.0  403.0  14.7   \n",
      "313   0.26938  0.0   9.90  0.0  0.544  6.266  82.8  3.2628   4.0  304.0  18.4   \n",
      "477  15.02340  0.0  18.10  0.0  0.614  5.304  97.3  2.1007  24.0  666.0  20.2   \n",
      "175   0.06664  0.0   4.05  0.0  0.510  6.546  33.1  3.1323   5.0  296.0  16.6   \n",
      "44    0.12269  0.0   6.91  0.0  0.448  6.069  40.0  5.7209   3.0  233.0  17.9   \n",
      "\n",
      "         11     12     y  \n",
      "155   88.01  15.02  15.6  \n",
      "313  393.39   7.90  21.6  \n",
      "477  349.48  24.91  12.0  \n",
      "175  390.96   5.33  29.4  \n",
      "44   389.39   9.55  21.2  \n"
     ]
    }
   ],
   "source": [
    "X=shuffle(X);\n",
    "print(X.shape)\n",
    "print(X.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train/validation/test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(354, 13)\n",
      "(101, 13)\n",
      "(51, 13)\n",
      "(354,)\n",
      "(101,)\n",
      "(51,)\n"
     ]
    }
   ],
   "source": [
    "X_train=X[0:int(perc_train*X.shape[0])]\n",
    "X_val=X[(int(perc_train*X.shape[0])):(int((perc_train+perc_val)*X.shape[0]))]\n",
    "X_test=X[(int((perc_train+perc_val)*X.shape[0])):]\n",
    "y_train=X_train[\"y\"];\n",
    "y_val=X_val[\"y\"];\n",
    "y_test=X_test[\"y\"];\n",
    "X=X.drop(columns=['y']);\n",
    "X_train=X_train.drop(columns=['y']);\n",
    "X_val=X_val.drop(columns=['y']);\n",
    "X_test=X_test.drop(columns=['y']);\n",
    "\n",
    "print(X_train.shape);\n",
    "print(X_val.shape);\n",
    "print(X_test.shape);\n",
    "print(y_train.shape);\n",
    "print(y_val.shape);\n",
    "print(y_test.shape);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standard Neural Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/falendor/.conda/envs/tensorflow/lib/python3.6/site-packages/sklearn/neural_network/multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "hidden_layer_sizes_values=[5,7,10];\n",
    "learning_rate_init_values=[np.float_power(10,-10),0.0001,0.001,0.01,0.1];\n",
    "val_errors=pd.DataFrame();\n",
    "for hidden_layer_sizes in hidden_layer_sizes_values:\n",
    "    for learning_rate_init in learning_rate_init_values:\n",
    "        clf = MLPRegressor(solver='sgd', \n",
    "                            hidden_layer_sizes=hidden_layer_sizes,learning_rate_init=learning_rate_init,\n",
    "                            alpha=1e-5,random_state=1);\n",
    "        clf.fit(X_train, y_train);\n",
    "        mae=np.mean(np.absolute(clf.predict(X_val)-y_val))\n",
    "        val_errors = val_errors.append(pd.DataFrame(data={'hidden_layer_sizes':[hidden_layer_sizes],'learning_rate_init':[learning_rate_init],'score':[mae]}), ignore_index=True)\n",
    "val_errors=val_errors.sort_values(['score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hidden_layer_sizes</th>\n",
       "      <th>learning_rate_init</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>1.000000e-03</td>\n",
       "      <td>6.492090e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>1.000000e-01</td>\n",
       "      <td>7.822429e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>1.000000e-02</td>\n",
       "      <td>8.017087e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>1.000000e-04</td>\n",
       "      <td>1.570930e+01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>1.000000e-04</td>\n",
       "      <td>2.427472e+01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>7</td>\n",
       "      <td>1.000000e-10</td>\n",
       "      <td>3.974522e+01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>10</td>\n",
       "      <td>1.000000e-04</td>\n",
       "      <td>5.629462e+01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>1.000000e-10</td>\n",
       "      <td>1.742959e+02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10</td>\n",
       "      <td>1.000000e-10</td>\n",
       "      <td>2.232732e+02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>1.000000e-03</td>\n",
       "      <td>9.621949e+02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>10</td>\n",
       "      <td>1.000000e-03</td>\n",
       "      <td>3.076648e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>7</td>\n",
       "      <td>1.000000e-02</td>\n",
       "      <td>6.559474e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>10</td>\n",
       "      <td>1.000000e-02</td>\n",
       "      <td>2.388481e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>7</td>\n",
       "      <td>1.000000e-01</td>\n",
       "      <td>1.249204e+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>10</td>\n",
       "      <td>1.000000e-01</td>\n",
       "      <td>4.601738e+09</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    hidden_layer_sizes  learning_rate_init         score\n",
       "2                    5        1.000000e-03  6.492090e+00\n",
       "4                    5        1.000000e-01  7.822429e+00\n",
       "3                    5        1.000000e-02  8.017087e+00\n",
       "1                    5        1.000000e-04  1.570930e+01\n",
       "6                    7        1.000000e-04  2.427472e+01\n",
       "5                    7        1.000000e-10  3.974522e+01\n",
       "11                  10        1.000000e-04  5.629462e+01\n",
       "0                    5        1.000000e-10  1.742959e+02\n",
       "10                  10        1.000000e-10  2.232732e+02\n",
       "7                    7        1.000000e-03  9.621949e+02\n",
       "12                  10        1.000000e-03  3.076648e+04\n",
       "8                    7        1.000000e-02  6.559474e+05\n",
       "13                  10        1.000000e-02  2.388481e+07\n",
       "9                    7        1.000000e-01  1.249204e+08\n",
       "14                  10        1.000000e-01  4.601738e+09"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/falendor/.conda/envs/tensorflow/lib/python3.6/site-packages/sklearn/neural_network/multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "9.1344008472462068"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = MLPRegressor(solver='sgd', \n",
    "                            hidden_layer_sizes=val_errors[\"hidden_layer_sizes\"].iloc[0],\n",
    "                            learning_rate_init=val_errors[\"learning_rate_init\"].iloc[0],\n",
    "                            alpha=1e-5,random_state=1);\n",
    "clf.fit(X_train, y_train);\n",
    "MLP_scores=np.mean(np.absolute(clf.predict(X_test)-y_test))\n",
    "MLP_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVR and DeepSVR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine Train and Validation (we will use CV from now on)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(455, 13)\n",
      "(455,)\n"
     ]
    }
   ],
   "source": [
    "X_train=X_train.append(X_val);\n",
    "y_train=y_train.append(y_val);\n",
    "print(X_train.shape);\n",
    "print(y_train.shape);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           0    1      2    3      4      5     6       7     8      9     10  \\\n",
      "155   3.53501  0.0  19.58  1.0  0.871  6.152  82.6  1.7455   5.0  403.0  14.7   \n",
      "313   0.26938  0.0   9.90  0.0  0.544  6.266  82.8  3.2628   4.0  304.0  18.4   \n",
      "477  15.02340  0.0  18.10  0.0  0.614  5.304  97.3  2.1007  24.0  666.0  20.2   \n",
      "175   0.06664  0.0   4.05  0.0  0.510  6.546  33.1  3.1323   5.0  296.0  16.6   \n",
      "44    0.12269  0.0   6.91  0.0  0.448  6.069  40.0  5.7209   3.0  233.0  17.9   \n",
      "\n",
      "         11     12  \n",
      "155   88.01  15.02  \n",
      "313  393.39   7.90  \n",
      "477  349.48  24.91  \n",
      "175  390.96   5.33  \n",
      "44   389.39   9.55  \n",
      "155    15.6\n",
      "313    21.6\n",
      "477    12.0\n",
      "175    29.4\n",
      "44     21.2\n",
      "51     20.5\n",
      "341    32.7\n",
      "142    13.4\n",
      "10     15.0\n",
      "503    23.9\n",
      "389    11.5\n",
      "Name: y, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(X_train.head())\n",
    "print(y_train[0:11])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Model Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs=1000;\n",
    "n_hidden=100;\n",
    "scoring = {'mean_absolute_error': make_scorer(mean_absolute_error)};\n",
    "sigma=np.std(y_train);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Model Hyperparameters (Grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.01]\n",
      "[ 0.001]\n",
      "[ 0.13784907]\n",
      "[ 0.001]\n"
     ]
    }
   ],
   "source": [
    "learning_rate=[0.01];\n",
    "cs = np.float_power(10,np.arange(-3,-2));\n",
    "epsilons = sigma * np.logspace(-6, 3, base=2.0,num=1);\n",
    "gammas = np.logspace(-3, 6, base=10.0,num=1);\n",
    "print(learning_rate);\n",
    "print(cs);\n",
    "print(epsilons);\n",
    "print(gammas);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "dsvr = lambda n: GridSearchCV(Pipeline([('scaler', StandardScaler()), ('regressor', DSVR(architecture=Straight(dense_units=[n_hidden]*n), epochs=epochs))]),\n",
    "                                   {'regressor__kernel_regularizer_l2': cs,\n",
    "                                    'regressor__lr': learning_rate,\n",
    "                                    'regressor__loss_epsilon': epsilons},\n",
    "                                   scoring=scoring['mean_absolute_error'], cv=cv, error_score=np.nan, fit_params={'regressor__epochs': epochs})\n",
    " \n",
    "\n",
    "\n",
    "estimator = {'SVR': GridSearchCV(Pipeline([('scaler', StandardScaler()), ('regressor', SVR())]),\n",
    "                                      {'regressor__C': cs,\n",
    "                                       'regressor__epsilon': epsilons,\n",
    "                                       'regressor__gamma': gammas},\n",
    "                                      scoring=scoring['mean_absolute_error'], cv=cv, error_score=np.nan),\n",
    "                 'DSVR0': dsvr(0), 'DSVR1': dsvr(1), 'DSVR2': dsvr(2), 'DSVR3': dsvr(3), 'DSVR4': dsvr(4), 'DSVR5': dsvr(5)}   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train SVR Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_estimator=estimator['SVR'];\n",
    "selected_estimator.fit(X_train,y_train);\n",
    "SVR_scores = selected_estimator.score(X_test, y_test);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train DeepSVR Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/falendor/.conda/envs/tensorflow/lib/python3.6/site-packages/sklearn/model_selection/_search.py:584: DeprecationWarning: \"fit_params\" as a constructor argument was deprecated in version 0.19 and will be removed in version 0.21. Pass fit parameters to the \"fit\" method instead.\n",
      "  '\"fit\" method instead.', DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 272 samples, validate on 31 samples\n",
      "Epoch 1/1000\n",
      "272/272 [==============================] - 0s 317us/step - loss: 22.5322 - val_loss: 23.2261\n",
      "Epoch 2/1000\n",
      "272/272 [==============================] - 0s 23us/step - loss: 22.5080 - val_loss: 23.2167\n",
      "Epoch 3/1000\n",
      "272/272 [==============================] - 0s 19us/step - loss: 22.4867 - val_loss: 23.2080\n",
      "Epoch 4/1000\n",
      "272/272 [==============================] - 0s 19us/step - loss: 22.4655 - val_loss: 23.1957\n",
      "Epoch 5/1000\n",
      "272/272 [==============================] - 0s 21us/step - loss: 22.4443 - val_loss: 23.1815\n",
      "Epoch 6/1000\n",
      "272/272 [==============================] - 0s 20us/step - loss: 22.4235 - val_loss: 23.1666\n",
      "Epoch 7/1000\n",
      "272/272 [==============================] - 0s 15us/step - loss: 22.4029 - val_loss: 23.1509\n",
      "Epoch 8/1000\n",
      "272/272 [==============================] - 0s 17us/step - loss: 22.3824 - val_loss: 23.1366\n",
      "Epoch 9/1000\n",
      "272/272 [==============================] - 0s 19us/step - loss: 22.3617 - val_loss: 23.1240\n",
      "Epoch 10/1000\n",
      "272/272 [==============================] - 0s 16us/step - loss: 22.3408 - val_loss: 23.1112\n",
      "Epoch 11/1000\n",
      "272/272 [==============================] - 0s 18us/step - loss: 22.3203 - val_loss: 23.0986\n",
      "Epoch 12/1000\n",
      "272/272 [==============================] - 0s 18us/step - loss: 22.2993 - val_loss: 23.0862\n",
      "Epoch 13/1000\n",
      "272/272 [==============================] - 0s 20us/step - loss: 22.2783 - val_loss: 23.0737\n",
      "Epoch 14/1000\n",
      "272/272 [==============================] - 0s 26us/step - loss: 22.2578 - val_loss: 23.0621\n",
      "Epoch 15/1000\n",
      "272/272 [==============================] - 0s 19us/step - loss: 22.2367 - val_loss: 23.0503\n",
      "Epoch 16/1000\n",
      "272/272 [==============================] - 0s 18us/step - loss: 22.2160 - val_loss: 23.0386\n",
      "Epoch 17/1000\n",
      "272/272 [==============================] - 0s 16us/step - loss: 22.1945 - val_loss: 23.0261\n",
      "Epoch 18/1000\n",
      "272/272 [==============================] - 0s 17us/step - loss: 22.1734 - val_loss: 23.0147\n",
      "Epoch 19/1000\n",
      "272/272 [==============================] - 0s 19us/step - loss: 22.1533 - val_loss: 23.0039\n",
      "Epoch 20/1000\n",
      "272/272 [==============================] - 0s 26us/step - loss: 22.1313 - val_loss: 22.9920\n",
      "Epoch 21/1000\n",
      "272/272 [==============================] - 0s 19us/step - loss: 22.1110 - val_loss: 22.9819\n",
      "Epoch 22/1000\n",
      "272/272 [==============================] - 0s 20us/step - loss: 22.0897 - val_loss: 22.9709\n",
      "Epoch 23/1000\n",
      "272/272 [==============================] - 0s 21us/step - loss: 22.0683 - val_loss: 22.9603\n",
      "Epoch 24/1000\n",
      "272/272 [==============================] - 0s 26us/step - loss: 22.0476 - val_loss: 22.9502\n",
      "Epoch 25/1000\n",
      "272/272 [==============================] - 0s 20us/step - loss: 22.0267 - val_loss: 22.9390\n",
      "Epoch 26/1000\n",
      "272/272 [==============================] - 0s 18us/step - loss: 22.0056 - val_loss: 22.9270\n",
      "Epoch 27/1000\n",
      "272/272 [==============================] - 0s 15us/step - loss: 21.9844 - val_loss: 22.9153\n",
      "Epoch 28/1000\n",
      "272/272 [==============================] - 0s 20us/step - loss: 21.9633 - val_loss: 22.9041\n",
      "Epoch 29/1000\n",
      "272/272 [==============================] - 0s 20us/step - loss: 21.9430 - val_loss: 22.8934\n",
      "Epoch 30/1000\n",
      "272/272 [==============================] - 0s 27us/step - loss: 21.9215 - val_loss: 22.8816\n",
      "Epoch 31/1000\n",
      "272/272 [==============================] - 0s 24us/step - loss: 21.9008 - val_loss: 22.8699\n",
      "Epoch 32/1000\n",
      "272/272 [==============================] - 0s 23us/step - loss: 21.8797 - val_loss: 22.8573\n",
      "Epoch 33/1000\n",
      "272/272 [==============================] - 0s 19us/step - loss: 21.8588 - val_loss: 22.8452\n",
      "Epoch 34/1000\n",
      "272/272 [==============================] - 0s 20us/step - loss: 21.8384 - val_loss: 22.8326\n",
      "Epoch 35/1000\n",
      "272/272 [==============================] - 0s 20us/step - loss: 21.8170 - val_loss: 22.8192\n",
      "Epoch 36/1000\n",
      "272/272 [==============================] - 0s 27us/step - loss: 21.7968 - val_loss: 22.8056\n",
      "Epoch 37/1000\n",
      "272/272 [==============================] - 0s 18us/step - loss: 21.7758 - val_loss: 22.7905\n",
      "Epoch 38/1000\n",
      "272/272 [==============================] - 0s 17us/step - loss: 21.7552 - val_loss: 22.7755\n",
      "Epoch 39/1000\n",
      "272/272 [==============================] - 0s 24us/step - loss: 21.7344 - val_loss: 22.7617\n",
      "Epoch 40/1000\n",
      "272/272 [==============================] - 0s 21us/step - loss: 21.7142 - val_loss: 22.7486\n",
      "Epoch 41/1000\n",
      "272/272 [==============================] - 0s 19us/step - loss: 21.6930 - val_loss: 22.7353\n",
      "Epoch 42/1000\n",
      "272/272 [==============================] - 0s 19us/step - loss: 21.6720 - val_loss: 22.7236\n",
      "Epoch 43/1000\n",
      "272/272 [==============================] - 0s 21us/step - loss: 21.6508 - val_loss: 22.7133\n",
      "Epoch 44/1000\n",
      "272/272 [==============================] - 0s 16us/step - loss: 21.6307 - val_loss: 22.7037\n",
      "Epoch 45/1000\n",
      "272/272 [==============================] - 0s 27us/step - loss: 21.6089 - val_loss: 22.6927\n",
      "Epoch 46/1000\n",
      "272/272 [==============================] - 0s 16us/step - loss: 21.5886 - val_loss: 22.6815\n",
      "Epoch 47/1000\n",
      "272/272 [==============================] - 0s 19us/step - loss: 21.5671 - val_loss: 22.6689\n",
      "Epoch 48/1000\n",
      "272/272 [==============================] - 0s 20us/step - loss: 21.5466 - val_loss: 22.6565\n",
      "Epoch 49/1000\n",
      "272/272 [==============================] - 0s 19us/step - loss: 21.5247 - val_loss: 22.6462\n",
      "Epoch 50/1000\n",
      "272/272 [==============================] - ETA: 0s - loss: 21.48 - 0s 22us/step - loss: 21.5043 - val_loss: 22.6368\n",
      "Epoch 51/1000\n",
      "272/272 [==============================] - 0s 23us/step - loss: 21.4827 - val_loss: 22.6270\n",
      "Epoch 52/1000\n",
      "272/272 [==============================] - 0s 17us/step - loss: 21.4620 - val_loss: 22.6168\n",
      "Epoch 53/1000\n",
      "272/272 [==============================] - 0s 17us/step - loss: 21.4409 - val_loss: 22.6049\n",
      "Epoch 54/1000\n",
      "272/272 [==============================] - 0s 23us/step - loss: 21.4209 - val_loss: 22.5914\n",
      "Epoch 55/1000\n",
      "272/272 [==============================] - 0s 21us/step - loss: 21.3990 - val_loss: 22.5765\n",
      "Epoch 56/1000\n",
      "272/272 [==============================] - 0s 16us/step - loss: 21.3785 - val_loss: 22.5625\n",
      "Epoch 57/1000\n",
      "272/272 [==============================] - 0s 17us/step - loss: 21.3573 - val_loss: 22.5496\n",
      "Epoch 58/1000\n",
      "272/272 [==============================] - 0s 22us/step - loss: 21.3376 - val_loss: 22.5375\n",
      "Epoch 59/1000\n",
      "272/272 [==============================] - 0s 23us/step - loss: 21.3161 - val_loss: 22.5241\n",
      "Epoch 60/1000\n",
      "272/272 [==============================] - 0s 18us/step - loss: 21.2949 - val_loss: 22.5123\n",
      "Epoch 61/1000\n",
      "272/272 [==============================] - 0s 25us/step - loss: 21.2742 - val_loss: 22.5019\n",
      "Epoch 62/1000\n",
      "272/272 [==============================] - 0s 25us/step - loss: 21.2532 - val_loss: 22.4915\n",
      "Epoch 63/1000\n",
      "272/272 [==============================] - 0s 37us/step - loss: 21.2328 - val_loss: 22.4806\n",
      "Epoch 64/1000\n",
      "272/272 [==============================] - 0s 24us/step - loss: 21.2109 - val_loss: 22.4689\n",
      "Epoch 65/1000\n",
      "272/272 [==============================] - 0s 23us/step - loss: 21.1901 - val_loss: 22.4582\n",
      "Epoch 66/1000\n",
      "272/272 [==============================] - 0s 16us/step - loss: 21.1700 - val_loss: 22.4468\n",
      "Epoch 67/1000\n",
      "272/272 [==============================] - 0s 25us/step - loss: 21.1486 - val_loss: 22.4334\n",
      "Epoch 68/1000\n",
      "272/272 [==============================] - 0s 26us/step - loss: 21.1275 - val_loss: 22.4207\n",
      "Epoch 69/1000\n",
      "272/272 [==============================] - 0s 22us/step - loss: 21.1067 - val_loss: 22.4079\n",
      "Epoch 70/1000\n",
      "272/272 [==============================] - 0s 23us/step - loss: 21.0864 - val_loss: 22.3949\n",
      "Epoch 71/1000\n",
      "272/272 [==============================] - 0s 28us/step - loss: 21.0652 - val_loss: 22.3809\n",
      "Epoch 72/1000\n",
      "272/272 [==============================] - ETA: 0s - loss: 21.61 - 0s 27us/step - loss: 21.0446 - val_loss: 22.3683\n",
      "Epoch 73/1000\n",
      "272/272 [==============================] - 0s 27us/step - loss: 21.0235 - val_loss: 22.3574\n",
      "Epoch 74/1000\n",
      "272/272 [==============================] - 0s 31us/step - loss: 21.0027 - val_loss: 22.3469\n",
      "Epoch 75/1000\n",
      "272/272 [==============================] - 0s 35us/step - loss: 20.9819 - val_loss: 22.3354\n",
      "Epoch 76/1000\n",
      "272/272 [==============================] - 0s 24us/step - loss: 20.9607 - val_loss: 22.3227\n",
      "Epoch 77/1000\n",
      "272/272 [==============================] - 0s 17us/step - loss: 20.9397 - val_loss: 22.3102\n",
      "Epoch 78/1000\n",
      "272/272 [==============================] - 0s 25us/step - loss: 20.9190 - val_loss: 22.2981\n",
      "Epoch 79/1000\n",
      "272/272 [==============================] - ETA: 0s - loss: 21.18 - 0s 23us/step - loss: 20.8985 - val_loss: 22.2863\n",
      "Epoch 80/1000\n",
      "272/272 [==============================] - 0s 43us/step - loss: 20.8773 - val_loss: 22.2750\n",
      "Epoch 81/1000\n",
      "272/272 [==============================] - 0s 23us/step - loss: 20.8561 - val_loss: 22.2640\n",
      "Epoch 82/1000\n",
      "272/272 [==============================] - 0s 29us/step - loss: 20.8349 - val_loss: 22.2539\n",
      "Epoch 83/1000\n",
      "272/272 [==============================] - 0s 19us/step - loss: 20.8146 - val_loss: 22.2440\n",
      "Epoch 84/1000\n",
      "272/272 [==============================] - 0s 23us/step - loss: 20.7931 - val_loss: 22.2328\n",
      "Epoch 85/1000\n",
      "272/272 [==============================] - ETA: 0s - loss: 20.92 - 0s 20us/step - loss: 20.7719 - val_loss: 22.2216\n",
      "Epoch 86/1000\n",
      "272/272 [==============================] - 0s 19us/step - loss: 20.7509 - val_loss: 22.2112\n",
      "Epoch 87/1000\n",
      "272/272 [==============================] - 0s 24us/step - loss: 20.7301 - val_loss: 22.2014\n",
      "Epoch 88/1000\n",
      "272/272 [==============================] - 0s 20us/step - loss: 20.7088 - val_loss: 22.1913\n",
      "Epoch 89/1000\n",
      "272/272 [==============================] - 0s 18us/step - loss: 20.6875 - val_loss: 22.1818\n",
      "Epoch 90/1000\n",
      "272/272 [==============================] - 0s 22us/step - loss: 20.6664 - val_loss: 22.1733\n",
      "Epoch 91/1000\n",
      "272/272 [==============================] - 0s 20us/step - loss: 20.6461 - val_loss: 22.1640\n",
      "Epoch 92/1000\n",
      "272/272 [==============================] - 0s 21us/step - loss: 20.6247 - val_loss: 22.1524\n",
      "Epoch 93/1000\n",
      "272/272 [==============================] - 0s 21us/step - loss: 20.6032 - val_loss: 22.1405\n",
      "Epoch 94/1000\n",
      "272/272 [==============================] - 0s 19us/step - loss: 20.5833 - val_loss: 22.1295\n",
      "Epoch 95/1000\n",
      "272/272 [==============================] - 0s 17us/step - loss: 20.5615 - val_loss: 22.1175\n",
      "Epoch 96/1000\n",
      "272/272 [==============================] - 0s 21us/step - loss: 20.5409 - val_loss: 22.1054\n",
      "Epoch 97/1000\n",
      "272/272 [==============================] - 0s 15us/step - loss: 20.5204 - val_loss: 22.0925\n",
      "Epoch 98/1000\n",
      "272/272 [==============================] - 0s 17us/step - loss: 20.4993 - val_loss: 22.0788\n",
      "Epoch 99/1000\n",
      "272/272 [==============================] - 0s 19us/step - loss: 20.4782 - val_loss: 22.0652\n",
      "Epoch 100/1000\n",
      "272/272 [==============================] - 0s 18us/step - loss: 20.4578 - val_loss: 22.0522\n",
      "Epoch 101/1000\n",
      "272/272 [==============================] - 0s 17us/step - loss: 20.4371 - val_loss: 22.0386\n",
      "Epoch 102/1000\n",
      "272/272 [==============================] - 0s 21us/step - loss: 20.4162 - val_loss: 22.0246\n",
      "Epoch 103/1000\n",
      "272/272 [==============================] - 0s 21us/step - loss: 20.3959 - val_loss: 22.0108\n",
      "Epoch 104/1000\n",
      "272/272 [==============================] - 0s 24us/step - loss: 20.3746 - val_loss: 21.9977\n",
      "Epoch 105/1000\n",
      "272/272 [==============================] - 0s 25us/step - loss: 20.3541 - val_loss: 21.9866\n",
      "Epoch 106/1000\n",
      "272/272 [==============================] - 0s 31us/step - loss: 20.3334 - val_loss: 21.9763\n",
      "Epoch 107/1000\n",
      "272/272 [==============================] - 0s 26us/step - loss: 20.3120 - val_loss: 21.9661\n",
      "Epoch 108/1000\n",
      "272/272 [==============================] - 0s 18us/step - loss: 20.2910 - val_loss: 21.9568\n",
      "Epoch 109/1000\n",
      "272/272 [==============================] - 0s 20us/step - loss: 20.2692 - val_loss: 21.9472\n",
      "Epoch 110/1000\n",
      "272/272 [==============================] - 0s 23us/step - loss: 20.2477 - val_loss: 21.9382\n",
      "Epoch 111/1000\n",
      "272/272 [==============================] - 0s 20us/step - loss: 20.2281 - val_loss: 21.9290\n",
      "Epoch 112/1000\n",
      "272/272 [==============================] - 0s 23us/step - loss: 20.2058 - val_loss: 21.9188\n",
      "Epoch 113/1000\n",
      "272/272 [==============================] - 0s 22us/step - loss: 20.1854 - val_loss: 21.9088\n",
      "Epoch 114/1000\n",
      "272/272 [==============================] - 0s 26us/step - loss: 20.1635 - val_loss: 21.8987\n",
      "Epoch 115/1000\n",
      "272/272 [==============================] - 0s 21us/step - loss: 20.1430 - val_loss: 21.8890\n",
      "Epoch 116/1000\n",
      "272/272 [==============================] - 0s 32us/step - loss: 20.1212 - val_loss: 21.8791\n",
      "Epoch 117/1000\n",
      "272/272 [==============================] - 0s 22us/step - loss: 20.0999 - val_loss: 21.8711\n",
      "Epoch 118/1000\n",
      "272/272 [==============================] - 0s 19us/step - loss: 20.0786 - val_loss: 21.8643\n",
      "Epoch 119/1000\n",
      "272/272 [==============================] - 0s 24us/step - loss: 20.0567 - val_loss: 21.8583\n",
      "Epoch 120/1000\n",
      "272/272 [==============================] - 0s 18us/step - loss: 20.0359 - val_loss: 21.8528\n",
      "Epoch 121/1000\n",
      "272/272 [==============================] - 0s 19us/step - loss: 20.0153 - val_loss: 21.8465\n",
      "Epoch 122/1000\n",
      "272/272 [==============================] - 0s 17us/step - loss: 19.9941 - val_loss: 21.8382\n",
      "Epoch 123/1000\n",
      "272/272 [==============================] - 0s 35us/step - loss: 19.9727 - val_loss: 21.8281\n",
      "Epoch 124/1000\n",
      "272/272 [==============================] - 0s 31us/step - loss: 19.9509 - val_loss: 21.8170\n",
      "Epoch 125/1000\n",
      "272/272 [==============================] - 0s 20us/step - loss: 19.9298 - val_loss: 21.8072\n",
      "Epoch 126/1000\n",
      "272/272 [==============================] - 0s 21us/step - loss: 19.9119 - val_loss: 21.7980\n",
      "Epoch 127/1000\n",
      "272/272 [==============================] - 0s 26us/step - loss: 19.8892 - val_loss: 21.7869\n",
      "Epoch 128/1000\n",
      "272/272 [==============================] - 0s 20us/step - loss: 19.8684 - val_loss: 21.7754\n",
      "Epoch 129/1000\n",
      "272/272 [==============================] - 0s 29us/step - loss: 19.8481 - val_loss: 21.7641\n",
      "Epoch 130/1000\n",
      "272/272 [==============================] - 0s 19us/step - loss: 19.8282 - val_loss: 21.7522\n",
      "Epoch 131/1000\n",
      "272/272 [==============================] - 0s 35us/step - loss: 19.8072 - val_loss: 21.7387\n",
      "Epoch 132/1000\n",
      "272/272 [==============================] - 0s 32us/step - loss: 19.7870 - val_loss: 21.7248\n",
      "Epoch 133/1000\n",
      "272/272 [==============================] - 0s 31us/step - loss: 19.7669 - val_loss: 21.7094\n",
      "Epoch 134/1000\n",
      "272/272 [==============================] - 0s 18us/step - loss: 19.7465 - val_loss: 21.6930\n",
      "Epoch 135/1000\n",
      "272/272 [==============================] - 0s 29us/step - loss: 19.7258 - val_loss: 21.6777\n",
      "Epoch 136/1000\n",
      "272/272 [==============================] - 0s 26us/step - loss: 19.7054 - val_loss: 21.6637\n",
      "Epoch 137/1000\n",
      "272/272 [==============================] - 0s 18us/step - loss: 19.6848 - val_loss: 21.6500\n",
      "Epoch 138/1000\n",
      "272/272 [==============================] - 0s 25us/step - loss: 19.6650 - val_loss: 21.6365\n",
      "Epoch 139/1000\n",
      "272/272 [==============================] - 0s 24us/step - loss: 19.6443 - val_loss: 21.6221\n",
      "Epoch 140/1000\n",
      "272/272 [==============================] - 0s 34us/step - loss: 19.6235 - val_loss: 21.6074\n",
      "Epoch 141/1000\n",
      "272/272 [==============================] - 0s 20us/step - loss: 19.6035 - val_loss: 21.5926\n",
      "Epoch 142/1000\n",
      "272/272 [==============================] - 0s 27us/step - loss: 19.5833 - val_loss: 21.5769\n",
      "Epoch 143/1000\n",
      "272/272 [==============================] - 0s 23us/step - loss: 19.5621 - val_loss: 21.5612\n",
      "Epoch 144/1000\n",
      "272/272 [==============================] - 0s 18us/step - loss: 19.5423 - val_loss: 21.5461\n",
      "Epoch 145/1000\n",
      "272/272 [==============================] - 0s 21us/step - loss: 19.5215 - val_loss: 21.5312\n",
      "Epoch 146/1000\n",
      "272/272 [==============================] - 0s 27us/step - loss: 19.5013 - val_loss: 21.5171\n",
      "Epoch 147/1000\n",
      "272/272 [==============================] - 0s 27us/step - loss: 19.4805 - val_loss: 21.5024\n",
      "Epoch 148/1000\n",
      "272/272 [==============================] - 0s 25us/step - loss: 19.4606 - val_loss: 21.4877\n",
      "Epoch 149/1000\n",
      "272/272 [==============================] - 0s 23us/step - loss: 19.4401 - val_loss: 21.4737\n",
      "Epoch 150/1000\n",
      "272/272 [==============================] - 0s 22us/step - loss: 19.4196 - val_loss: 21.4599\n",
      "Epoch 151/1000\n",
      "272/272 [==============================] - 0s 22us/step - loss: 19.3991 - val_loss: 21.4457\n",
      "Epoch 152/1000\n",
      "272/272 [==============================] - 0s 29us/step - loss: 19.3791 - val_loss: 21.4312\n",
      "Epoch 153/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "272/272 [==============================] - 0s 19us/step - loss: 19.3582 - val_loss: 21.4156\n",
      "Epoch 154/1000\n",
      "272/272 [==============================] - 0s 26us/step - loss: 19.3381 - val_loss: 21.4006\n",
      "Epoch 155/1000\n",
      "272/272 [==============================] - 0s 25us/step - loss: 19.3177 - val_loss: 21.3864\n",
      "Epoch 156/1000\n",
      "272/272 [==============================] - 0s 21us/step - loss: 19.2969 - val_loss: 21.3724\n",
      "Epoch 157/1000\n",
      "272/272 [==============================] - 0s 19us/step - loss: 19.2765 - val_loss: 21.3587\n",
      "Epoch 158/1000\n",
      "272/272 [==============================] - 0s 27us/step - loss: 19.2559 - val_loss: 21.3443\n",
      "Epoch 159/1000\n",
      "272/272 [==============================] - 0s 22us/step - loss: 19.2360 - val_loss: 21.3296\n",
      "Epoch 160/1000\n",
      "272/272 [==============================] - 0s 20us/step - loss: 19.2151 - val_loss: 21.3145\n",
      "Epoch 161/1000\n",
      "272/272 [==============================] - 0s 19us/step - loss: 19.1948 - val_loss: 21.3011\n",
      "Epoch 162/1000\n",
      "272/272 [==============================] - 0s 25us/step - loss: 19.1740 - val_loss: 21.2890\n",
      "Epoch 163/1000\n",
      "272/272 [==============================] - 0s 28us/step - loss: 19.1536 - val_loss: 21.2770\n",
      "Epoch 164/1000\n",
      "272/272 [==============================] - 0s 23us/step - loss: 19.1332 - val_loss: 21.2644\n",
      "Epoch 165/1000\n",
      "272/272 [==============================] - 0s 16us/step - loss: 19.1127 - val_loss: 21.2519\n",
      "Epoch 166/1000\n",
      "272/272 [==============================] - 0s 20us/step - loss: 19.0925 - val_loss: 21.2398\n",
      "Epoch 167/1000\n",
      "272/272 [==============================] - 0s 16us/step - loss: 19.0717 - val_loss: 21.2281\n",
      "Epoch 168/1000\n",
      "272/272 [==============================] - 0s 17us/step - loss: 19.0519 - val_loss: 21.2174\n",
      "Epoch 169/1000\n",
      "272/272 [==============================] - 0s 16us/step - loss: 19.0318 - val_loss: 21.2055\n",
      "Epoch 170/1000\n",
      "272/272 [==============================] - 0s 18us/step - loss: 19.0120 - val_loss: 21.1935\n",
      "Epoch 171/1000\n",
      "272/272 [==============================] - 0s 19us/step - loss: 18.9919 - val_loss: 21.1804\n",
      "Epoch 172/1000\n",
      "272/272 [==============================] - 0s 22us/step - loss: 18.9722 - val_loss: 21.1664\n",
      "Epoch 173/1000\n",
      "272/272 [==============================] - 0s 18us/step - loss: 18.9524 - val_loss: 21.1506\n",
      "Epoch 174/1000\n",
      "272/272 [==============================] - 0s 17us/step - loss: 18.9320 - val_loss: 21.1340\n",
      "Epoch 175/1000\n",
      "272/272 [==============================] - 0s 18us/step - loss: 18.9119 - val_loss: 21.1181\n",
      "Epoch 176/1000\n",
      "272/272 [==============================] - 0s 20us/step - loss: 18.8914 - val_loss: 21.1015\n",
      "Epoch 177/1000\n",
      "272/272 [==============================] - 0s 19us/step - loss: 18.8717 - val_loss: 21.0850\n",
      "Epoch 178/1000\n",
      "272/272 [==============================] - 0s 19us/step - loss: 18.8510 - val_loss: 21.0682\n",
      "Epoch 179/1000\n",
      "272/272 [==============================] - 0s 21us/step - loss: 18.8308 - val_loss: 21.0515\n",
      "Epoch 180/1000\n",
      "272/272 [==============================] - 0s 20us/step - loss: 18.8104 - val_loss: 21.0337\n",
      "Epoch 181/1000\n",
      "272/272 [==============================] - 0s 23us/step - loss: 18.7896 - val_loss: 21.0154\n",
      "Epoch 182/1000\n",
      "272/272 [==============================] - 0s 21us/step - loss: 18.7698 - val_loss: 20.9970\n",
      "Epoch 183/1000\n",
      "272/272 [==============================] - 0s 19us/step - loss: 18.7489 - val_loss: 20.9782\n",
      "Epoch 184/1000\n",
      "272/272 [==============================] - 0s 19us/step - loss: 18.7286 - val_loss: 20.9601\n",
      "Epoch 185/1000\n",
      "272/272 [==============================] - 0s 17us/step - loss: 18.7083 - val_loss: 20.9432\n",
      "Epoch 186/1000\n",
      "272/272 [==============================] - 0s 20us/step - loss: 18.6882 - val_loss: 20.9267\n",
      "Epoch 187/1000\n",
      "272/272 [==============================] - 0s 18us/step - loss: 18.6677 - val_loss: 20.9114\n",
      "Epoch 188/1000\n",
      "272/272 [==============================] - 0s 15us/step - loss: 18.6476 - val_loss: 20.8949\n",
      "Epoch 189/1000\n",
      "272/272 [==============================] - 0s 20us/step - loss: 18.6267 - val_loss: 20.8765\n",
      "Epoch 190/1000\n",
      "272/272 [==============================] - 0s 25us/step - loss: 18.6066 - val_loss: 20.8582\n",
      "Epoch 191/1000\n",
      "272/272 [==============================] - 0s 21us/step - loss: 18.5858 - val_loss: 20.8398\n",
      "Epoch 192/1000\n",
      "272/272 [==============================] - 0s 25us/step - loss: 18.5658 - val_loss: 20.8225\n",
      "Epoch 193/1000\n",
      "272/272 [==============================] - 0s 23us/step - loss: 18.5451 - val_loss: 20.8069\n",
      "Epoch 194/1000\n",
      "272/272 [==============================] - 0s 20us/step - loss: 18.5249 - val_loss: 20.7921\n",
      "Epoch 195/1000\n",
      "272/272 [==============================] - 0s 21us/step - loss: 18.5046 - val_loss: 20.7762\n",
      "Epoch 196/1000\n",
      "272/272 [==============================] - 0s 21us/step - loss: 18.4841 - val_loss: 20.7609\n",
      "Epoch 197/1000\n",
      "272/272 [==============================] - 0s 30us/step - loss: 18.4638 - val_loss: 20.7451\n",
      "Epoch 198/1000\n",
      "272/272 [==============================] - 0s 31us/step - loss: 18.4440 - val_loss: 20.7287\n",
      "Epoch 199/1000\n",
      "272/272 [==============================] - 0s 16us/step - loss: 18.4235 - val_loss: 20.7107\n",
      "Epoch 200/1000\n",
      "272/272 [==============================] - 0s 17us/step - loss: 18.4029 - val_loss: 20.6925\n",
      "Epoch 201/1000\n",
      "272/272 [==============================] - 0s 12us/step - loss: 18.3828 - val_loss: 20.6750\n",
      "Epoch 202/1000\n",
      "272/272 [==============================] - 0s 14us/step - loss: 18.3628 - val_loss: 20.6570\n",
      "Epoch 203/1000\n",
      "272/272 [==============================] - 0s 15us/step - loss: 18.3432 - val_loss: 20.6398\n",
      "Epoch 204/1000\n",
      "272/272 [==============================] - 0s 14us/step - loss: 18.3226 - val_loss: 20.6223\n",
      "Epoch 205/1000\n",
      "272/272 [==============================] - 0s 12us/step - loss: 18.3019 - val_loss: 20.6033\n",
      "Epoch 206/1000\n",
      "272/272 [==============================] - 0s 13us/step - loss: 18.2830 - val_loss: 20.5865\n",
      "Epoch 207/1000\n",
      "272/272 [==============================] - 0s 14us/step - loss: 18.2618 - val_loss: 20.5723\n",
      "Epoch 208/1000\n",
      "272/272 [==============================] - 0s 12us/step - loss: 18.2415 - val_loss: 20.5586\n",
      "Epoch 209/1000\n",
      "272/272 [==============================] - 0s 13us/step - loss: 18.2212 - val_loss: 20.5432\n",
      "Epoch 210/1000\n",
      "272/272 [==============================] - 0s 13us/step - loss: 18.2004 - val_loss: 20.5265\n",
      "Epoch 211/1000\n",
      "272/272 [==============================] - 0s 13us/step - loss: 18.1806 - val_loss: 20.5107\n",
      "Epoch 212/1000\n",
      "272/272 [==============================] - 0s 14us/step - loss: 18.1600 - val_loss: 20.4952\n",
      "Epoch 213/1000\n",
      "272/272 [==============================] - 0s 15us/step - loss: 18.1398 - val_loss: 20.4789\n",
      "Epoch 214/1000\n",
      "272/272 [==============================] - 0s 13us/step - loss: 18.1190 - val_loss: 20.4605\n",
      "Epoch 215/1000\n",
      "272/272 [==============================] - 0s 13us/step - loss: 18.1008 - val_loss: 20.4421\n",
      "Epoch 216/1000\n",
      "272/272 [==============================] - 0s 14us/step - loss: 18.0789 - val_loss: 20.4262\n",
      "Epoch 217/1000\n",
      "272/272 [==============================] - 0s 12us/step - loss: 18.0586 - val_loss: 20.4106\n",
      "Epoch 218/1000\n",
      "272/272 [==============================] - 0s 14us/step - loss: 18.0389 - val_loss: 20.3950\n",
      "Epoch 219/1000\n",
      "272/272 [==============================] - 0s 14us/step - loss: 18.0182 - val_loss: 20.3794\n",
      "Epoch 220/1000\n",
      "272/272 [==============================] - 0s 16us/step - loss: 17.9982 - val_loss: 20.3643\n",
      "Epoch 221/1000\n",
      "272/272 [==============================] - 0s 18us/step - loss: 17.9782 - val_loss: 20.3510\n",
      "Epoch 222/1000\n",
      "272/272 [==============================] - 0s 15us/step - loss: 17.9585 - val_loss: 20.3368\n",
      "Epoch 223/1000\n",
      "272/272 [==============================] - 0s 16us/step - loss: 17.9385 - val_loss: 20.3209\n",
      "Epoch 224/1000\n",
      "272/272 [==============================] - 0s 16us/step - loss: 17.9182 - val_loss: 20.3045\n",
      "Epoch 225/1000\n",
      "272/272 [==============================] - 0s 21us/step - loss: 17.8982 - val_loss: 20.2883\n",
      "Epoch 226/1000\n",
      "272/272 [==============================] - 0s 15us/step - loss: 17.8784 - val_loss: 20.2721\n",
      "Epoch 227/1000\n",
      "272/272 [==============================] - 0s 19us/step - loss: 17.8588 - val_loss: 20.2556\n",
      "Epoch 228/1000\n",
      "272/272 [==============================] - 0s 17us/step - loss: 17.8385 - val_loss: 20.2398\n",
      "Epoch 229/1000\n",
      "272/272 [==============================] - 0s 19us/step - loss: 17.8182 - val_loss: 20.2223\n",
      "Epoch 230/1000\n",
      "272/272 [==============================] - 0s 22us/step - loss: 17.7981 - val_loss: 20.2044\n",
      "Epoch 231/1000\n",
      "272/272 [==============================] - 0s 17us/step - loss: 17.7784 - val_loss: 20.1864\n",
      "Epoch 232/1000\n",
      "272/272 [==============================] - 0s 20us/step - loss: 17.7582 - val_loss: 20.1683\n",
      "Epoch 233/1000\n",
      "272/272 [==============================] - 0s 17us/step - loss: 17.7384 - val_loss: 20.1498\n",
      "Epoch 234/1000\n",
      "272/272 [==============================] - 0s 22us/step - loss: 17.7178 - val_loss: 20.1310\n",
      "Epoch 235/1000\n",
      "272/272 [==============================] - 0s 22us/step - loss: 17.6976 - val_loss: 20.1111\n",
      "Epoch 236/1000\n",
      "272/272 [==============================] - 0s 20us/step - loss: 17.6780 - val_loss: 20.0918\n",
      "Epoch 237/1000\n",
      "272/272 [==============================] - 0s 20us/step - loss: 17.6572 - val_loss: 20.0748\n",
      "Epoch 238/1000\n",
      "272/272 [==============================] - 0s 20us/step - loss: 17.6370 - val_loss: 20.0587\n",
      "Epoch 239/1000\n",
      "272/272 [==============================] - 0s 20us/step - loss: 17.6171 - val_loss: 20.0426\n",
      "Epoch 240/1000\n",
      "272/272 [==============================] - 0s 17us/step - loss: 17.5966 - val_loss: 20.0259\n",
      "Epoch 241/1000\n",
      "272/272 [==============================] - 0s 14us/step - loss: 17.5763 - val_loss: 20.0086\n",
      "Epoch 242/1000\n",
      "272/272 [==============================] - 0s 16us/step - loss: 17.5564 - val_loss: 19.9919\n",
      "Epoch 243/1000\n",
      "272/272 [==============================] - 0s 15us/step - loss: 17.5360 - val_loss: 19.9740\n",
      "Epoch 244/1000\n",
      "272/272 [==============================] - 0s 16us/step - loss: 17.5163 - val_loss: 19.9554\n",
      "Epoch 245/1000\n",
      "272/272 [==============================] - 0s 14us/step - loss: 17.4954 - val_loss: 19.9387\n",
      "Epoch 246/1000\n",
      "272/272 [==============================] - 0s 16us/step - loss: 17.4757 - val_loss: 19.9221\n",
      "Epoch 247/1000\n",
      "272/272 [==============================] - 0s 13us/step - loss: 17.4550 - val_loss: 19.9060\n",
      "Epoch 248/1000\n",
      "272/272 [==============================] - 0s 16us/step - loss: 17.4350 - val_loss: 19.8896\n",
      "Epoch 249/1000\n",
      "272/272 [==============================] - 0s 18us/step - loss: 17.4146 - val_loss: 19.8720\n",
      "Epoch 250/1000\n",
      "272/272 [==============================] - 0s 20us/step - loss: 17.3954 - val_loss: 19.8540\n",
      "Epoch 251/1000\n",
      "272/272 [==============================] - 0s 21us/step - loss: 17.3745 - val_loss: 19.8365\n",
      "Epoch 252/1000\n",
      "272/272 [==============================] - 0s 17us/step - loss: 17.3547 - val_loss: 19.8191\n",
      "Epoch 253/1000\n",
      "272/272 [==============================] - 0s 40us/step - loss: 17.3344 - val_loss: 19.8001\n",
      "Epoch 254/1000\n",
      "272/272 [==============================] - 0s 42us/step - loss: 17.3143 - val_loss: 19.7802\n",
      "Epoch 255/1000\n",
      "272/272 [==============================] - 0s 39us/step - loss: 17.2951 - val_loss: 19.7623\n",
      "Epoch 256/1000\n",
      "272/272 [==============================] - 0s 40us/step - loss: 17.2744 - val_loss: 19.7449\n",
      "Epoch 257/1000\n",
      "272/272 [==============================] - 0s 30us/step - loss: 17.2540 - val_loss: 19.7259\n",
      "Epoch 258/1000\n",
      "272/272 [==============================] - 0s 27us/step - loss: 17.2339 - val_loss: 19.7068\n",
      "Epoch 259/1000\n",
      "272/272 [==============================] - 0s 28us/step - loss: 17.2143 - val_loss: 19.6880\n",
      "Epoch 260/1000\n",
      "272/272 [==============================] - 0s 24us/step - loss: 17.1940 - val_loss: 19.6704\n",
      "Epoch 261/1000\n",
      "272/272 [==============================] - 0s 28us/step - loss: 17.1739 - val_loss: 19.6514\n",
      "Epoch 262/1000\n",
      "272/272 [==============================] - 0s 20us/step - loss: 17.1538 - val_loss: 19.6320\n",
      "Epoch 263/1000\n",
      "272/272 [==============================] - 0s 21us/step - loss: 17.1343 - val_loss: 19.6122\n",
      "Epoch 264/1000\n",
      "272/272 [==============================] - ETA: 0s - loss: 17.26 - 0s 20us/step - loss: 17.1134 - val_loss: 19.5941\n",
      "Epoch 265/1000\n",
      "272/272 [==============================] - 0s 26us/step - loss: 17.0940 - val_loss: 19.5773\n",
      "Epoch 266/1000\n",
      "272/272 [==============================] - 0s 26us/step - loss: 17.0735 - val_loss: 19.5624\n",
      "Epoch 267/1000\n",
      "272/272 [==============================] - 0s 24us/step - loss: 17.0529 - val_loss: 19.5487\n",
      "Epoch 268/1000\n",
      "272/272 [==============================] - 0s 21us/step - loss: 17.0329 - val_loss: 19.5346\n",
      "Epoch 269/1000\n",
      "272/272 [==============================] - 0s 22us/step - loss: 17.0124 - val_loss: 19.5211\n",
      "Epoch 270/1000\n",
      "272/272 [==============================] - 0s 27us/step - loss: 16.9923 - val_loss: 19.5083\n",
      "Epoch 271/1000\n",
      "272/272 [==============================] - 0s 21us/step - loss: 16.9722 - val_loss: 19.4949\n",
      "Epoch 272/1000\n",
      "272/272 [==============================] - 0s 25us/step - loss: 16.9522 - val_loss: 19.4813\n",
      "Epoch 273/1000\n",
      "272/272 [==============================] - 0s 25us/step - loss: 16.9326 - val_loss: 19.4676\n",
      "Epoch 274/1000\n",
      "272/272 [==============================] - 0s 30us/step - loss: 16.9120 - val_loss: 19.4539\n",
      "Epoch 275/1000\n",
      "272/272 [==============================] - 0s 19us/step - loss: 16.8919 - val_loss: 19.4390\n",
      "Epoch 276/1000\n",
      "272/272 [==============================] - 0s 22us/step - loss: 16.8723 - val_loss: 19.4247\n",
      "Epoch 277/1000\n",
      "272/272 [==============================] - 0s 21us/step - loss: 16.8527 - val_loss: 19.4101\n",
      "Epoch 278/1000\n",
      "272/272 [==============================] - 0s 25us/step - loss: 16.8330 - val_loss: 19.3948\n",
      "Epoch 279/1000\n",
      "272/272 [==============================] - 0s 26us/step - loss: 16.8134 - val_loss: 19.3792\n",
      "Epoch 280/1000\n",
      "272/272 [==============================] - 0s 23us/step - loss: 16.7936 - val_loss: 19.3630\n",
      "Epoch 281/1000\n",
      "272/272 [==============================] - 0s 27us/step - loss: 16.7736 - val_loss: 19.3463\n",
      "Epoch 282/1000\n",
      "272/272 [==============================] - 0s 22us/step - loss: 16.7539 - val_loss: 19.3286\n",
      "Epoch 283/1000\n",
      "272/272 [==============================] - ETA: 0s - loss: 17.28 - 0s 22us/step - loss: 16.7339 - val_loss: 19.3093\n",
      "Epoch 284/1000\n",
      "272/272 [==============================] - 0s 22us/step - loss: 16.7137 - val_loss: 19.2908\n",
      "Epoch 285/1000\n",
      "272/272 [==============================] - 0s 24us/step - loss: 16.6934 - val_loss: 19.2722\n",
      "Epoch 286/1000\n",
      "272/272 [==============================] - 0s 27us/step - loss: 16.6735 - val_loss: 19.2535\n",
      "Epoch 287/1000\n",
      "272/272 [==============================] - 0s 25us/step - loss: 16.6533 - val_loss: 19.2343\n",
      "Epoch 288/1000\n",
      "272/272 [==============================] - 0s 30us/step - loss: 16.6331 - val_loss: 19.2147\n",
      "Epoch 289/1000\n",
      "272/272 [==============================] - 0s 21us/step - loss: 16.6130 - val_loss: 19.1950\n",
      "Epoch 290/1000\n",
      "272/272 [==============================] - 0s 22us/step - loss: 16.5930 - val_loss: 19.1750\n",
      "Epoch 291/1000\n",
      "272/272 [==============================] - 0s 20us/step - loss: 16.5728 - val_loss: 19.1541\n",
      "Epoch 292/1000\n",
      "272/272 [==============================] - 0s 23us/step - loss: 16.5528 - val_loss: 19.1337\n",
      "Epoch 293/1000\n",
      "272/272 [==============================] - 0s 23us/step - loss: 16.5323 - val_loss: 19.1144\n",
      "Epoch 294/1000\n",
      "272/272 [==============================] - 0s 29us/step - loss: 16.5122 - val_loss: 19.0952\n",
      "Epoch 295/1000\n",
      "272/272 [==============================] - 0s 24us/step - loss: 16.4928 - val_loss: 19.0753\n",
      "Epoch 296/1000\n",
      "272/272 [==============================] - 0s 32us/step - loss: 16.4721 - val_loss: 19.0570\n",
      "Epoch 297/1000\n",
      "272/272 [==============================] - 0s 27us/step - loss: 16.4528 - val_loss: 19.0390\n",
      "Epoch 298/1000\n",
      "272/272 [==============================] - 0s 23us/step - loss: 16.4319 - val_loss: 19.0223\n",
      "Epoch 299/1000\n",
      "272/272 [==============================] - 0s 26us/step - loss: 16.4117 - val_loss: 19.0065\n",
      "Epoch 300/1000\n",
      "272/272 [==============================] - 0s 20us/step - loss: 16.3915 - val_loss: 18.9912\n",
      "Epoch 301/1000\n",
      "272/272 [==============================] - 0s 23us/step - loss: 16.3720 - val_loss: 18.9770\n",
      "Epoch 302/1000\n",
      "272/272 [==============================] - 0s 23us/step - loss: 16.3520 - val_loss: 18.9625\n",
      "Epoch 303/1000\n",
      "272/272 [==============================] - 0s 22us/step - loss: 16.3320 - val_loss: 18.9481\n",
      "Epoch 304/1000\n",
      "272/272 [==============================] - 0s 21us/step - loss: 16.3124 - val_loss: 18.9349\n",
      "Epoch 305/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "272/272 [==============================] - 0s 23us/step - loss: 16.2920 - val_loss: 18.9215\n",
      "Epoch 306/1000\n",
      "272/272 [==============================] - 0s 27us/step - loss: 16.2719 - val_loss: 18.9081\n",
      "Epoch 307/1000\n",
      "272/272 [==============================] - 0s 32us/step - loss: 16.2522 - val_loss: 18.8942\n",
      "Epoch 308/1000\n",
      "272/272 [==============================] - 0s 34us/step - loss: 16.2326 - val_loss: 18.8794\n",
      "Epoch 309/1000\n",
      "272/272 [==============================] - 0s 32us/step - loss: 16.2125 - val_loss: 18.8639\n",
      "Epoch 310/1000\n",
      "272/272 [==============================] - 0s 29us/step - loss: 16.1926 - val_loss: 18.8487\n",
      "Epoch 311/1000\n",
      "272/272 [==============================] - 0s 32us/step - loss: 16.1727 - val_loss: 18.8328\n",
      "Epoch 312/1000\n",
      "272/272 [==============================] - 0s 29us/step - loss: 16.1529 - val_loss: 18.8174\n",
      "Epoch 313/1000\n",
      "272/272 [==============================] - 0s 26us/step - loss: 16.1326 - val_loss: 18.8030\n",
      "Epoch 314/1000\n",
      "272/272 [==============================] - 0s 31us/step - loss: 16.1132 - val_loss: 18.7879\n",
      "Epoch 315/1000\n",
      "272/272 [==============================] - 0s 24us/step - loss: 16.0927 - val_loss: 18.7719\n",
      "Epoch 316/1000\n",
      "272/272 [==============================] - 0s 21us/step - loss: 16.0728 - val_loss: 18.7552\n",
      "Epoch 317/1000\n",
      "272/272 [==============================] - 0s 21us/step - loss: 16.0523 - val_loss: 18.7382\n",
      "Epoch 318/1000\n",
      "272/272 [==============================] - 0s 28us/step - loss: 16.0329 - val_loss: 18.7215\n",
      "Epoch 319/1000\n",
      "272/272 [==============================] - 0s 23us/step - loss: 16.0123 - val_loss: 18.7057\n",
      "Epoch 320/1000\n",
      "272/272 [==============================] - 0s 22us/step - loss: 15.9922 - val_loss: 18.6898\n",
      "Epoch 321/1000\n",
      "272/272 [==============================] - 0s 21us/step - loss: 15.9726 - val_loss: 18.6738\n",
      "Epoch 322/1000\n",
      "272/272 [==============================] - 0s 24us/step - loss: 15.9522 - val_loss: 18.6581\n",
      "Epoch 323/1000\n",
      "272/272 [==============================] - 0s 23us/step - loss: 15.9327 - val_loss: 18.6419\n",
      "Epoch 324/1000\n",
      "272/272 [==============================] - 0s 22us/step - loss: 15.9120 - val_loss: 18.6252\n",
      "Epoch 325/1000\n",
      "272/272 [==============================] - 0s 23us/step - loss: 15.8924 - val_loss: 18.6074\n",
      "Epoch 326/1000\n",
      "272/272 [==============================] - 0s 17us/step - loss: 15.8724 - val_loss: 18.5874\n",
      "Epoch 327/1000\n",
      "272/272 [==============================] - 0s 21us/step - loss: 15.8525 - val_loss: 18.5682\n",
      "Epoch 328/1000\n",
      "272/272 [==============================] - 0s 19us/step - loss: 15.8315 - val_loss: 18.5497\n",
      "Epoch 329/1000\n",
      "272/272 [==============================] - 0s 22us/step - loss: 15.8117 - val_loss: 18.5318\n",
      "Epoch 330/1000\n",
      "272/272 [==============================] - 0s 20us/step - loss: 15.7917 - val_loss: 18.5141\n",
      "Epoch 331/1000\n",
      "272/272 [==============================] - 0s 23us/step - loss: 15.7714 - val_loss: 18.4956\n",
      "Epoch 332/1000\n",
      "272/272 [==============================] - 0s 21us/step - loss: 15.7519 - val_loss: 18.4764\n",
      "Epoch 333/1000\n",
      "272/272 [==============================] - 0s 24us/step - loss: 15.7315 - val_loss: 18.4575\n",
      "Epoch 334/1000\n",
      "272/272 [==============================] - 0s 21us/step - loss: 15.7116 - val_loss: 18.4380\n",
      "Epoch 335/1000\n",
      "272/272 [==============================] - 0s 23us/step - loss: 15.6914 - val_loss: 18.4173\n",
      "Epoch 336/1000\n",
      "272/272 [==============================] - 0s 16us/step - loss: 15.6722 - val_loss: 18.3963\n",
      "Epoch 337/1000\n",
      "272/272 [==============================] - 0s 20us/step - loss: 15.6511 - val_loss: 18.3771\n",
      "Epoch 338/1000\n",
      "272/272 [==============================] - 0s 17us/step - loss: 15.6314 - val_loss: 18.3575\n",
      "Epoch 339/1000\n",
      "272/272 [==============================] - 0s 27us/step - loss: 15.6117 - val_loss: 18.3388\n",
      "Epoch 340/1000\n",
      "272/272 [==============================] - 0s 22us/step - loss: 15.5912 - val_loss: 18.3211\n",
      "Epoch 341/1000\n",
      "272/272 [==============================] - 0s 20us/step - loss: 15.5707 - val_loss: 18.3018\n",
      "Epoch 342/1000\n",
      "272/272 [==============================] - 0s 18us/step - loss: 15.5506 - val_loss: 18.2824\n",
      "Epoch 343/1000\n",
      "272/272 [==============================] - 0s 17us/step - loss: 15.5308 - val_loss: 18.2625\n",
      "Epoch 344/1000\n",
      "272/272 [==============================] - 0s 28us/step - loss: 15.5106 - val_loss: 18.2430\n",
      "Epoch 345/1000\n",
      "272/272 [==============================] - 0s 16us/step - loss: 15.4905 - val_loss: 18.2248\n",
      "Epoch 346/1000\n",
      "272/272 [==============================] - 0s 21us/step - loss: 15.4705 - val_loss: 18.2090\n",
      "Epoch 347/1000\n",
      "272/272 [==============================] - 0s 21us/step - loss: 15.4507 - val_loss: 18.1932\n",
      "Epoch 348/1000\n",
      "272/272 [==============================] - 0s 17us/step - loss: 15.4302 - val_loss: 18.1776\n",
      "Epoch 349/1000\n",
      "272/272 [==============================] - 0s 22us/step - loss: 15.4103 - val_loss: 18.1605\n",
      "Epoch 350/1000\n",
      "272/272 [==============================] - 0s 31us/step - loss: 15.3907 - val_loss: 18.1424\n",
      "Epoch 351/1000\n",
      "272/272 [==============================] - 0s 25us/step - loss: 15.3704 - val_loss: 18.1249\n",
      "Epoch 352/1000\n",
      "272/272 [==============================] - 0s 16us/step - loss: 15.3501 - val_loss: 18.1055\n",
      "Epoch 353/1000\n",
      "272/272 [==============================] - ETA: 0s - loss: 14.96 - 0s 17us/step - loss: 15.3306 - val_loss: 18.0858\n",
      "Epoch 354/1000\n",
      "272/272 [==============================] - 0s 24us/step - loss: 15.3106 - val_loss: 18.0664\n",
      "Epoch 355/1000\n",
      "272/272 [==============================] - 0s 24us/step - loss: 15.2904 - val_loss: 18.0500\n",
      "Epoch 356/1000\n",
      "272/272 [==============================] - 0s 16us/step - loss: 15.2700 - val_loss: 18.0365\n",
      "Epoch 357/1000\n",
      "272/272 [==============================] - 0s 19us/step - loss: 15.2514 - val_loss: 18.0234\n",
      "Epoch 358/1000\n",
      "272/272 [==============================] - 0s 26us/step - loss: 15.2309 - val_loss: 18.0071\n",
      "Epoch 359/1000\n",
      "272/272 [==============================] - 0s 19us/step - loss: 15.2111 - val_loss: 17.9897\n",
      "Epoch 360/1000\n",
      "272/272 [==============================] - 0s 27us/step - loss: 15.1915 - val_loss: 17.9726\n",
      "Epoch 361/1000\n",
      "272/272 [==============================] - 0s 22us/step - loss: 15.1716 - val_loss: 17.9550\n",
      "Epoch 362/1000\n",
      "272/272 [==============================] - 0s 17us/step - loss: 15.1515 - val_loss: 17.9357\n",
      "Epoch 363/1000\n",
      "272/272 [==============================] - 0s 23us/step - loss: 15.1327 - val_loss: 17.9152\n",
      "Epoch 364/1000\n",
      "272/272 [==============================] - 0s 18us/step - loss: 15.1118 - val_loss: 17.8963\n",
      "Epoch 365/1000\n",
      "272/272 [==============================] - 0s 22us/step - loss: 15.0921 - val_loss: 17.8775\n",
      "Epoch 366/1000\n",
      "272/272 [==============================] - 0s 26us/step - loss: 15.0719 - val_loss: 17.8606\n",
      "Epoch 367/1000\n",
      "272/272 [==============================] - 0s 22us/step - loss: 15.0523 - val_loss: 17.8435\n",
      "Epoch 368/1000\n",
      "272/272 [==============================] - 0s 26us/step - loss: 15.0322 - val_loss: 17.8258\n",
      "Epoch 369/1000\n",
      "272/272 [==============================] - 0s 26us/step - loss: 15.0122 - val_loss: 17.8076\n",
      "Epoch 370/1000\n",
      "272/272 [==============================] - 0s 23us/step - loss: 14.9926 - val_loss: 17.7893\n",
      "Epoch 371/1000\n",
      "272/272 [==============================] - 0s 24us/step - loss: 14.9727 - val_loss: 17.7707\n",
      "Epoch 372/1000\n",
      "272/272 [==============================] - 0s 19us/step - loss: 14.9525 - val_loss: 17.7529\n",
      "Epoch 373/1000\n",
      "272/272 [==============================] - 0s 19us/step - loss: 14.9334 - val_loss: 17.7353\n",
      "Epoch 374/1000\n",
      "272/272 [==============================] - 0s 19us/step - loss: 14.9129 - val_loss: 17.7193\n",
      "Epoch 375/1000\n",
      "272/272 [==============================] - 0s 24us/step - loss: 14.8932 - val_loss: 17.7030\n",
      "Epoch 376/1000\n",
      "272/272 [==============================] - 0s 22us/step - loss: 14.8733 - val_loss: 17.6864\n",
      "Epoch 377/1000\n",
      "272/272 [==============================] - 0s 28us/step - loss: 14.8538 - val_loss: 17.6689\n",
      "Epoch 378/1000\n",
      "272/272 [==============================] - 0s 22us/step - loss: 14.8342 - val_loss: 17.6512\n",
      "Epoch 379/1000\n",
      "272/272 [==============================] - 0s 18us/step - loss: 14.8140 - val_loss: 17.6315\n",
      "Epoch 380/1000\n",
      "272/272 [==============================] - 0s 24us/step - loss: 14.7944 - val_loss: 17.6122\n",
      "Epoch 381/1000\n",
      "272/272 [==============================] - 0s 19us/step - loss: 14.7745 - val_loss: 17.5939\n",
      "Epoch 382/1000\n",
      "272/272 [==============================] - 0s 19us/step - loss: 14.7545 - val_loss: 17.5747\n",
      "Epoch 383/1000\n",
      "272/272 [==============================] - 0s 23us/step - loss: 14.7343 - val_loss: 17.5545\n",
      "Epoch 384/1000\n",
      "272/272 [==============================] - 0s 18us/step - loss: 14.7139 - val_loss: 17.5331\n",
      "Epoch 385/1000\n",
      "272/272 [==============================] - 0s 16us/step - loss: 14.6947 - val_loss: 17.5119\n",
      "Epoch 386/1000\n",
      "272/272 [==============================] - 0s 22us/step - loss: 14.6742 - val_loss: 17.4933\n",
      "Epoch 387/1000\n",
      "272/272 [==============================] - 0s 19us/step - loss: 14.6533 - val_loss: 17.4762\n",
      "Epoch 388/1000\n",
      "272/272 [==============================] - 0s 17us/step - loss: 14.6332 - val_loss: 17.4572\n",
      "Epoch 389/1000\n",
      "272/272 [==============================] - 0s 22us/step - loss: 14.6131 - val_loss: 17.4387\n",
      "Epoch 390/1000\n",
      "272/272 [==============================] - 0s 25us/step - loss: 14.5936 - val_loss: 17.4198\n",
      "Epoch 391/1000\n",
      "272/272 [==============================] - 0s 14us/step - loss: 14.5730 - val_loss: 17.4028\n",
      "Epoch 392/1000\n",
      "272/272 [==============================] - 0s 21us/step - loss: 14.5525 - val_loss: 17.3864\n",
      "Epoch 393/1000\n",
      "272/272 [==============================] - 0s 19us/step - loss: 14.5324 - val_loss: 17.3701\n",
      "Epoch 394/1000\n",
      "272/272 [==============================] - 0s 24us/step - loss: 14.5120 - val_loss: 17.3541\n",
      "Epoch 395/1000\n",
      "272/272 [==============================] - 0s 16us/step - loss: 14.4920 - val_loss: 17.3381\n",
      "Epoch 396/1000\n",
      "272/272 [==============================] - 0s 17us/step - loss: 14.4722 - val_loss: 17.3215\n",
      "Epoch 397/1000\n",
      "272/272 [==============================] - 0s 22us/step - loss: 14.4516 - val_loss: 17.3050\n",
      "Epoch 398/1000\n",
      "272/272 [==============================] - 0s 20us/step - loss: 14.4316 - val_loss: 17.2877\n",
      "Epoch 399/1000\n",
      "272/272 [==============================] - 0s 22us/step - loss: 14.4110 - val_loss: 17.2691\n",
      "Epoch 400/1000\n",
      "272/272 [==============================] - 0s 16us/step - loss: 14.3917 - val_loss: 17.2513\n",
      "Epoch 401/1000\n",
      "272/272 [==============================] - 0s 19us/step - loss: 14.3716 - val_loss: 17.2341\n",
      "Epoch 402/1000\n",
      "272/272 [==============================] - 0s 21us/step - loss: 14.3508 - val_loss: 17.2185\n",
      "Epoch 403/1000\n",
      "272/272 [==============================] - 0s 17us/step - loss: 14.3306 - val_loss: 17.2026\n",
      "Epoch 404/1000\n",
      "272/272 [==============================] - ETA: 0s - loss: 13.93 - 0s 22us/step - loss: 14.3110 - val_loss: 17.1872\n",
      "Epoch 405/1000\n",
      "272/272 [==============================] - 0s 20us/step - loss: 14.2908 - val_loss: 17.1698\n",
      "Epoch 406/1000\n",
      "272/272 [==============================] - 0s 20us/step - loss: 14.2709 - val_loss: 17.1509\n",
      "Epoch 407/1000\n",
      "272/272 [==============================] - 0s 16us/step - loss: 14.2509 - val_loss: 17.1326\n",
      "Epoch 408/1000\n",
      "272/272 [==============================] - 0s 25us/step - loss: 14.2309 - val_loss: 17.1151\n",
      "Epoch 409/1000\n",
      "272/272 [==============================] - 0s 19us/step - loss: 14.2112 - val_loss: 17.0969\n",
      "Epoch 410/1000\n",
      "272/272 [==============================] - 0s 27us/step - loss: 14.1912 - val_loss: 17.0794\n",
      "Epoch 411/1000\n",
      "272/272 [==============================] - 0s 22us/step - loss: 14.1716 - val_loss: 17.0617\n",
      "Epoch 412/1000\n",
      "272/272 [==============================] - 0s 35us/step - loss: 14.1514 - val_loss: 17.0441\n",
      "Epoch 413/1000\n",
      "272/272 [==============================] - 0s 27us/step - loss: 14.1316 - val_loss: 17.0273\n",
      "Epoch 414/1000\n",
      "272/272 [==============================] - 0s 25us/step - loss: 14.1117 - val_loss: 17.0118\n",
      "Epoch 415/1000\n",
      "272/272 [==============================] - 0s 30us/step - loss: 14.0914 - val_loss: 16.9956\n",
      "Epoch 416/1000\n",
      "272/272 [==============================] - 0s 26us/step - loss: 14.0713 - val_loss: 16.9814\n",
      "Epoch 417/1000\n",
      "272/272 [==============================] - 0s 29us/step - loss: 14.0518 - val_loss: 16.9676\n",
      "Epoch 418/1000\n",
      "272/272 [==============================] - 0s 20us/step - loss: 14.0314 - val_loss: 16.9519\n",
      "Epoch 419/1000\n",
      "272/272 [==============================] - 0s 22us/step - loss: 14.0109 - val_loss: 16.9345\n",
      "Epoch 420/1000\n",
      "272/272 [==============================] - 0s 24us/step - loss: 13.9916 - val_loss: 16.9169\n",
      "Epoch 421/1000\n",
      "272/272 [==============================] - 0s 24us/step - loss: 13.9714 - val_loss: 16.9007\n",
      "Epoch 422/1000\n",
      "272/272 [==============================] - 0s 23us/step - loss: 13.9504 - val_loss: 16.8845\n",
      "Epoch 423/1000\n",
      "272/272 [==============================] - 0s 26us/step - loss: 13.9307 - val_loss: 16.8687\n",
      "Epoch 424/1000\n",
      "272/272 [==============================] - 0s 23us/step - loss: 13.9104 - val_loss: 16.8509\n",
      "Epoch 425/1000\n",
      "272/272 [==============================] - 0s 20us/step - loss: 13.8899 - val_loss: 16.8301\n",
      "Epoch 426/1000\n",
      "272/272 [==============================] - 0s 24us/step - loss: 13.8696 - val_loss: 16.8074\n",
      "Epoch 427/1000\n",
      "272/272 [==============================] - 0s 26us/step - loss: 13.8485 - val_loss: 16.7845\n",
      "Epoch 428/1000\n",
      "272/272 [==============================] - 0s 21us/step - loss: 13.8283 - val_loss: 16.7605\n",
      "Epoch 429/1000\n",
      "272/272 [==============================] - 0s 19us/step - loss: 13.8097 - val_loss: 16.7374\n",
      "Epoch 430/1000\n",
      "272/272 [==============================] - 0s 15us/step - loss: 13.7888 - val_loss: 16.7177\n",
      "Epoch 431/1000\n",
      "272/272 [==============================] - 0s 19us/step - loss: 13.7687 - val_loss: 16.6985\n",
      "Epoch 432/1000\n",
      "272/272 [==============================] - 0s 27us/step - loss: 13.7486 - val_loss: 16.6804\n",
      "Epoch 433/1000\n",
      "272/272 [==============================] - 0s 28us/step - loss: 13.7283 - val_loss: 16.6634\n",
      "Epoch 434/1000\n",
      "272/272 [==============================] - 0s 17us/step - loss: 13.7083 - val_loss: 16.6466\n",
      "Epoch 435/1000\n",
      "272/272 [==============================] - 0s 16us/step - loss: 13.6882 - val_loss: 16.6307\n",
      "Epoch 436/1000\n",
      "272/272 [==============================] - 0s 21us/step - loss: 13.6679 - val_loss: 16.6147\n",
      "Epoch 437/1000\n",
      "272/272 [==============================] - 0s 27us/step - loss: 13.6481 - val_loss: 16.5983\n",
      "Epoch 438/1000\n",
      "272/272 [==============================] - 0s 21us/step - loss: 13.6278 - val_loss: 16.5824\n",
      "Epoch 439/1000\n",
      "272/272 [==============================] - 0s 18us/step - loss: 13.6078 - val_loss: 16.5663\n",
      "Epoch 440/1000\n",
      "272/272 [==============================] - 0s 24us/step - loss: 13.5880 - val_loss: 16.5506\n",
      "Epoch 441/1000\n",
      "272/272 [==============================] - 0s 22us/step - loss: 13.5683 - val_loss: 16.5343\n",
      "Epoch 442/1000\n",
      "272/272 [==============================] - 0s 23us/step - loss: 13.5483 - val_loss: 16.5158\n",
      "Epoch 443/1000\n",
      "272/272 [==============================] - 0s 24us/step - loss: 13.5287 - val_loss: 16.4971\n",
      "Epoch 444/1000\n",
      "272/272 [==============================] - 0s 22us/step - loss: 13.5092 - val_loss: 16.4793\n",
      "Epoch 445/1000\n",
      "272/272 [==============================] - 0s 19us/step - loss: 13.4897 - val_loss: 16.4615\n",
      "Epoch 446/1000\n",
      "272/272 [==============================] - 0s 17us/step - loss: 13.4707 - val_loss: 16.4439\n",
      "Epoch 447/1000\n",
      "272/272 [==============================] - 0s 17us/step - loss: 13.4510 - val_loss: 16.4256\n",
      "Epoch 448/1000\n",
      "272/272 [==============================] - 0s 22us/step - loss: 13.4316 - val_loss: 16.4069\n",
      "Epoch 449/1000\n",
      "272/272 [==============================] - 0s 21us/step - loss: 13.4113 - val_loss: 16.3892\n",
      "Epoch 450/1000\n",
      "272/272 [==============================] - 0s 16us/step - loss: 13.3917 - val_loss: 16.3717\n",
      "Epoch 451/1000\n",
      "272/272 [==============================] - 0s 25us/step - loss: 13.3717 - val_loss: 16.3537\n",
      "Epoch 452/1000\n",
      "272/272 [==============================] - 0s 18us/step - loss: 13.3523 - val_loss: 16.3364\n",
      "Epoch 453/1000\n",
      "272/272 [==============================] - 0s 21us/step - loss: 13.3318 - val_loss: 16.3185\n",
      "Epoch 454/1000\n",
      "272/272 [==============================] - 0s 18us/step - loss: 13.3123 - val_loss: 16.2993\n",
      "Epoch 455/1000\n",
      "272/272 [==============================] - 0s 24us/step - loss: 13.2922 - val_loss: 16.2816\n",
      "Epoch 456/1000\n",
      "272/272 [==============================] - 0s 27us/step - loss: 13.2726 - val_loss: 16.2646\n",
      "Epoch 457/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "272/272 [==============================] - 0s 20us/step - loss: 13.2525 - val_loss: 16.2481\n",
      "Epoch 458/1000\n",
      "272/272 [==============================] - 0s 23us/step - loss: 13.2331 - val_loss: 16.2326\n",
      "Epoch 459/1000\n",
      "272/272 [==============================] - 0s 28us/step - loss: 13.2133 - val_loss: 16.2168\n",
      "Epoch 460/1000\n",
      "272/272 [==============================] - 0s 24us/step - loss: 13.1941 - val_loss: 16.1996\n",
      "Epoch 461/1000\n",
      "272/272 [==============================] - 0s 18us/step - loss: 13.1750 - val_loss: 16.1825\n",
      "Epoch 462/1000\n",
      "272/272 [==============================] - 0s 23us/step - loss: 13.1551 - val_loss: 16.1657\n",
      "Epoch 463/1000\n",
      "272/272 [==============================] - 0s 21us/step - loss: 13.1362 - val_loss: 16.1495\n",
      "Epoch 464/1000\n",
      "272/272 [==============================] - 0s 25us/step - loss: 13.1166 - val_loss: 16.1342\n",
      "Epoch 465/1000\n",
      "272/272 [==============================] - 0s 16us/step - loss: 13.0967 - val_loss: 16.1194\n",
      "Epoch 466/1000\n",
      "272/272 [==============================] - 0s 18us/step - loss: 13.0772 - val_loss: 16.1035\n",
      "Epoch 467/1000\n",
      "272/272 [==============================] - 0s 21us/step - loss: 13.0568 - val_loss: 16.0874\n",
      "Epoch 468/1000\n",
      "272/272 [==============================] - 0s 25us/step - loss: 13.0372 - val_loss: 16.0700\n",
      "Epoch 469/1000\n",
      "272/272 [==============================] - 0s 33us/step - loss: 13.0165 - val_loss: 16.0526\n",
      "Epoch 470/1000\n",
      "272/272 [==============================] - 0s 19us/step - loss: 12.9962 - val_loss: 16.0338\n",
      "Epoch 471/1000\n",
      "272/272 [==============================] - 0s 22us/step - loss: 12.9765 - val_loss: 16.0133\n",
      "Epoch 472/1000\n",
      "272/272 [==============================] - 0s 16us/step - loss: 12.9564 - val_loss: 15.9925\n",
      "Epoch 473/1000\n",
      "272/272 [==============================] - 0s 21us/step - loss: 12.9360 - val_loss: 15.9734\n",
      "Epoch 474/1000\n",
      "272/272 [==============================] - 0s 18us/step - loss: 12.9157 - val_loss: 15.9545\n",
      "Epoch 475/1000\n",
      "272/272 [==============================] - 0s 19us/step - loss: 12.8955 - val_loss: 15.9356\n",
      "Epoch 476/1000\n",
      "272/272 [==============================] - 0s 26us/step - loss: 12.8754 - val_loss: 15.9184\n",
      "Epoch 477/1000\n",
      "272/272 [==============================] - 0s 22us/step - loss: 12.8572 - val_loss: 15.9024\n",
      "Epoch 478/1000\n",
      "272/272 [==============================] - 0s 23us/step - loss: 12.8352 - val_loss: 15.8875\n",
      "Epoch 479/1000\n",
      "272/272 [==============================] - 0s 21us/step - loss: 12.8154 - val_loss: 15.8712\n",
      "Epoch 480/1000\n",
      "272/272 [==============================] - 0s 20us/step - loss: 12.7952 - val_loss: 15.8570\n",
      "Epoch 481/1000\n",
      "272/272 [==============================] - 0s 27us/step - loss: 12.7752 - val_loss: 15.8438\n",
      "Epoch 482/1000\n",
      "272/272 [==============================] - 0s 21us/step - loss: 12.7554 - val_loss: 15.8299\n",
      "Epoch 483/1000\n",
      "272/272 [==============================] - 0s 21us/step - loss: 12.7357 - val_loss: 15.8149\n",
      "Epoch 484/1000\n",
      "272/272 [==============================] - 0s 19us/step - loss: 12.7153 - val_loss: 15.7994\n",
      "Epoch 485/1000\n",
      "272/272 [==============================] - 0s 23us/step - loss: 12.6952 - val_loss: 15.7835\n",
      "Epoch 486/1000\n",
      "272/272 [==============================] - 0s 20us/step - loss: 12.6756 - val_loss: 15.7675\n",
      "Epoch 487/1000\n",
      "272/272 [==============================] - 0s 21us/step - loss: 12.6555 - val_loss: 15.7523\n",
      "Epoch 488/1000\n",
      "272/272 [==============================] - 0s 18us/step - loss: 12.6351 - val_loss: 15.7379\n",
      "Epoch 489/1000\n",
      "272/272 [==============================] - 0s 20us/step - loss: 12.6149 - val_loss: 15.7229\n",
      "Epoch 490/1000\n",
      "272/272 [==============================] - 0s 21us/step - loss: 12.5957 - val_loss: 15.7072\n",
      "Epoch 491/1000\n",
      "272/272 [==============================] - 0s 17us/step - loss: 12.5748 - val_loss: 15.6906\n",
      "Epoch 492/1000\n",
      "272/272 [==============================] - 0s 16us/step - loss: 12.5551 - val_loss: 15.6746\n",
      "Epoch 493/1000\n",
      "272/272 [==============================] - 0s 18us/step - loss: 12.5347 - val_loss: 15.6580\n",
      "Epoch 494/1000\n",
      "272/272 [==============================] - 0s 20us/step - loss: 12.5149 - val_loss: 15.6415\n",
      "Epoch 495/1000\n",
      "272/272 [==============================] - 0s 18us/step - loss: 12.4950 - val_loss: 15.6239\n",
      "Epoch 496/1000\n",
      "272/272 [==============================] - 0s 19us/step - loss: 12.4751 - val_loss: 15.6069\n",
      "Epoch 497/1000\n",
      "272/272 [==============================] - 0s 23us/step - loss: 12.4557 - val_loss: 15.5908\n",
      "Epoch 498/1000\n",
      "272/272 [==============================] - 0s 21us/step - loss: 12.4354 - val_loss: 15.5764\n",
      "Epoch 499/1000\n",
      "272/272 [==============================] - 0s 28us/step - loss: 12.4161 - val_loss: 15.5615\n",
      "Epoch 500/1000\n",
      "272/272 [==============================] - 0s 23us/step - loss: 12.3959 - val_loss: 15.5450\n",
      "Epoch 501/1000\n",
      "272/272 [==============================] - 0s 18us/step - loss: 12.3761 - val_loss: 15.5295\n",
      "Epoch 502/1000\n",
      "272/272 [==============================] - ETA: 0s - loss: 11.57 - 0s 18us/step - loss: 12.3566 - val_loss: 15.5140\n",
      "Epoch 503/1000\n",
      "272/272 [==============================] - 0s 24us/step - loss: 12.3369 - val_loss: 15.4975\n",
      "Epoch 504/1000\n",
      "272/272 [==============================] - 0s 27us/step - loss: 12.3172 - val_loss: 15.4808\n",
      "Epoch 505/1000\n",
      "272/272 [==============================] - 0s 21us/step - loss: 12.2976 - val_loss: 15.4632\n",
      "Epoch 506/1000\n",
      "272/272 [==============================] - 0s 26us/step - loss: 12.2780 - val_loss: 15.4455\n",
      "Epoch 507/1000\n",
      "272/272 [==============================] - 0s 26us/step - loss: 12.2583 - val_loss: 15.4274\n",
      "Epoch 508/1000\n",
      "272/272 [==============================] - 0s 29us/step - loss: 12.2386 - val_loss: 15.4087\n",
      "Epoch 509/1000\n",
      "272/272 [==============================] - ETA: 0s - loss: 12.05 - 0s 30us/step - loss: 12.2192 - val_loss: 15.3887\n",
      "Epoch 510/1000\n",
      "272/272 [==============================] - 0s 27us/step - loss: 12.1994 - val_loss: 15.3691\n",
      "Epoch 511/1000\n",
      "272/272 [==============================] - 0s 40us/step - loss: 12.1802 - val_loss: 15.3517\n",
      "Epoch 512/1000\n",
      "272/272 [==============================] - 0s 57us/step - loss: 12.1609 - val_loss: 15.3353\n",
      "Epoch 513/1000\n",
      "272/272 [==============================] - 0s 37us/step - loss: 12.1406 - val_loss: 15.3196\n",
      "Epoch 514/1000\n",
      "272/272 [==============================] - 0s 42us/step - loss: 12.1211 - val_loss: 15.3030\n",
      "Epoch 515/1000\n",
      "272/272 [==============================] - 0s 35us/step - loss: 12.1021 - val_loss: 15.2855\n",
      "Epoch 516/1000\n",
      "272/272 [==============================] - 0s 25us/step - loss: 12.0823 - val_loss: 15.2691\n",
      "Epoch 517/1000\n",
      "272/272 [==============================] - 0s 24us/step - loss: 12.0627 - val_loss: 15.2539\n",
      "Epoch 518/1000\n",
      "272/272 [==============================] - 0s 21us/step - loss: 12.0439 - val_loss: 15.2383\n",
      "Epoch 519/1000\n",
      "272/272 [==============================] - 0s 33us/step - loss: 12.0254 - val_loss: 15.2218\n",
      "Epoch 520/1000\n",
      "272/272 [==============================] - 0s 41us/step - loss: 12.0054 - val_loss: 15.2041\n",
      "Epoch 521/1000\n",
      "272/272 [==============================] - 0s 35us/step - loss: 11.9854 - val_loss: 15.1866\n",
      "Epoch 522/1000\n",
      "272/272 [==============================] - 0s 19us/step - loss: 11.9657 - val_loss: 15.1685\n",
      "Epoch 523/1000\n",
      "272/272 [==============================] - 0s 18us/step - loss: 11.9463 - val_loss: 15.1505\n",
      "Epoch 524/1000\n",
      "272/272 [==============================] - 0s 23us/step - loss: 11.9266 - val_loss: 15.1331\n",
      "Epoch 525/1000\n",
      "272/272 [==============================] - 0s 20us/step - loss: 11.9073 - val_loss: 15.1165\n",
      "Epoch 526/1000\n",
      "272/272 [==============================] - 0s 33us/step - loss: 11.8883 - val_loss: 15.0981\n",
      "Epoch 527/1000\n",
      "272/272 [==============================] - 0s 32us/step - loss: 11.8675 - val_loss: 15.0814\n",
      "Epoch 528/1000\n",
      "272/272 [==============================] - 0s 34us/step - loss: 11.8480 - val_loss: 15.0646\n",
      "Epoch 529/1000\n",
      "272/272 [==============================] - 0s 21us/step - loss: 11.8287 - val_loss: 15.0478\n",
      "Epoch 530/1000\n",
      "272/272 [==============================] - 0s 18us/step - loss: 11.8089 - val_loss: 15.0328\n",
      "Epoch 531/1000\n",
      "272/272 [==============================] - 0s 25us/step - loss: 11.7893 - val_loss: 15.0185\n",
      "Epoch 532/1000\n",
      "272/272 [==============================] - 0s 19us/step - loss: 11.7699 - val_loss: 15.0045\n",
      "Epoch 533/1000\n",
      "272/272 [==============================] - 0s 19us/step - loss: 11.7509 - val_loss: 14.9894\n",
      "Epoch 534/1000\n",
      "272/272 [==============================] - 0s 18us/step - loss: 11.7314 - val_loss: 14.9728\n",
      "Epoch 535/1000\n",
      "272/272 [==============================] - 0s 19us/step - loss: 11.7118 - val_loss: 14.9553\n",
      "Epoch 536/1000\n",
      "272/272 [==============================] - 0s 22us/step - loss: 11.6924 - val_loss: 14.9372\n",
      "Epoch 537/1000\n",
      "272/272 [==============================] - 0s 22us/step - loss: 11.6733 - val_loss: 14.9190\n",
      "Epoch 538/1000\n",
      "272/272 [==============================] - 0s 26us/step - loss: 11.6538 - val_loss: 14.9009\n",
      "Epoch 539/1000\n",
      "272/272 [==============================] - 0s 20us/step - loss: 11.6341 - val_loss: 14.8831\n",
      "Epoch 540/1000\n",
      "272/272 [==============================] - 0s 22us/step - loss: 11.6145 - val_loss: 14.8636\n",
      "Epoch 541/1000\n",
      "272/272 [==============================] - 0s 20us/step - loss: 11.5952 - val_loss: 14.8437\n",
      "Epoch 542/1000\n",
      "272/272 [==============================] - 0s 20us/step - loss: 11.5750 - val_loss: 14.8247\n",
      "Epoch 543/1000\n",
      "272/272 [==============================] - 0s 26us/step - loss: 11.5553 - val_loss: 14.8053\n",
      "Epoch 544/1000\n",
      "272/272 [==============================] - 0s 19us/step - loss: 11.5355 - val_loss: 14.7856\n",
      "Epoch 545/1000\n",
      "272/272 [==============================] - 0s 21us/step - loss: 11.5170 - val_loss: 14.7662\n",
      "Epoch 546/1000\n",
      "272/272 [==============================] - 0s 20us/step - loss: 11.4963 - val_loss: 14.7480\n",
      "Epoch 547/1000\n",
      "272/272 [==============================] - 0s 22us/step - loss: 11.4768 - val_loss: 14.7279\n",
      "Epoch 548/1000\n",
      "272/272 [==============================] - 0s 20us/step - loss: 11.4573 - val_loss: 14.7068\n",
      "Epoch 549/1000\n",
      "272/272 [==============================] - 0s 21us/step - loss: 11.4378 - val_loss: 14.6874\n",
      "Epoch 550/1000\n",
      "272/272 [==============================] - 0s 27us/step - loss: 11.4181 - val_loss: 14.6683\n",
      "Epoch 551/1000\n",
      "272/272 [==============================] - 0s 16us/step - loss: 11.3984 - val_loss: 14.6491\n",
      "Epoch 552/1000\n",
      "272/272 [==============================] - 0s 24us/step - loss: 11.3787 - val_loss: 14.6296\n",
      "Epoch 553/1000\n",
      "272/272 [==============================] - 0s 22us/step - loss: 11.3594 - val_loss: 14.6104\n",
      "Epoch 554/1000\n",
      "272/272 [==============================] - 0s 27us/step - loss: 11.3399 - val_loss: 14.5924\n",
      "Epoch 555/1000\n",
      "272/272 [==============================] - 0s 30us/step - loss: 11.3209 - val_loss: 14.5755\n",
      "Epoch 556/1000\n",
      "272/272 [==============================] - 0s 39us/step - loss: 11.3009 - val_loss: 14.5597\n",
      "Epoch 557/1000\n",
      "272/272 [==============================] - 0s 23us/step - loss: 11.2814 - val_loss: 14.5445\n",
      "Epoch 558/1000\n",
      "272/272 [==============================] - 0s 32us/step - loss: 11.2622 - val_loss: 14.5289\n",
      "Epoch 559/1000\n",
      "272/272 [==============================] - 0s 26us/step - loss: 11.2431 - val_loss: 14.5129\n",
      "Epoch 560/1000\n",
      "272/272 [==============================] - 0s 20us/step - loss: 11.2235 - val_loss: 14.4961\n",
      "Epoch 561/1000\n",
      "272/272 [==============================] - 0s 23us/step - loss: 11.2054 - val_loss: 14.4800\n",
      "Epoch 562/1000\n",
      "272/272 [==============================] - 0s 30us/step - loss: 11.1853 - val_loss: 14.4649\n",
      "Epoch 563/1000\n",
      "272/272 [==============================] - 0s 25us/step - loss: 11.1656 - val_loss: 14.4477\n",
      "Epoch 564/1000\n",
      "272/272 [==============================] - 0s 21us/step - loss: 11.1468 - val_loss: 14.4307\n",
      "Epoch 565/1000\n",
      "272/272 [==============================] - 0s 23us/step - loss: 11.1274 - val_loss: 14.4137\n",
      "Epoch 566/1000\n",
      "272/272 [==============================] - 0s 22us/step - loss: 11.1081 - val_loss: 14.3971\n",
      "Epoch 567/1000\n",
      "272/272 [==============================] - 0s 20us/step - loss: 11.0898 - val_loss: 14.3816\n",
      "Epoch 568/1000\n",
      "272/272 [==============================] - 0s 17us/step - loss: 11.0702 - val_loss: 14.3644\n",
      "Epoch 569/1000\n",
      "272/272 [==============================] - 0s 24us/step - loss: 11.0511 - val_loss: 14.3461\n",
      "Epoch 570/1000\n",
      "272/272 [==============================] - 0s 21us/step - loss: 11.0320 - val_loss: 14.3280\n",
      "Epoch 571/1000\n",
      "272/272 [==============================] - 0s 28us/step - loss: 11.0127 - val_loss: 14.3094\n",
      "Epoch 572/1000\n",
      "272/272 [==============================] - 0s 21us/step - loss: 10.9940 - val_loss: 14.2905\n",
      "Epoch 573/1000\n",
      "272/272 [==============================] - 0s 20us/step - loss: 10.9748 - val_loss: 14.2706\n",
      "Epoch 574/1000\n",
      "272/272 [==============================] - 0s 22us/step - loss: 10.9573 - val_loss: 14.2510\n",
      "Epoch 575/1000\n",
      "272/272 [==============================] - 0s 28us/step - loss: 10.9369 - val_loss: 14.2340\n",
      "Epoch 576/1000\n",
      "272/272 [==============================] - 0s 24us/step - loss: 10.9178 - val_loss: 14.2166\n",
      "Epoch 577/1000\n",
      "272/272 [==============================] - 0s 22us/step - loss: 10.8995 - val_loss: 14.1988\n",
      "Epoch 578/1000\n",
      "272/272 [==============================] - 0s 22us/step - loss: 10.8798 - val_loss: 14.1831\n",
      "Epoch 579/1000\n",
      "272/272 [==============================] - 0s 27us/step - loss: 10.8608 - val_loss: 14.1667\n",
      "Epoch 580/1000\n",
      "272/272 [==============================] - 0s 21us/step - loss: 10.8419 - val_loss: 14.1507\n",
      "Epoch 581/1000\n",
      "272/272 [==============================] - 0s 25us/step - loss: 10.8232 - val_loss: 14.1335\n",
      "Epoch 582/1000\n",
      "272/272 [==============================] - 0s 24us/step - loss: 10.8039 - val_loss: 14.1173\n",
      "Epoch 583/1000\n",
      "272/272 [==============================] - 0s 22us/step - loss: 10.7857 - val_loss: 14.0997\n",
      "Epoch 584/1000\n",
      "272/272 [==============================] - 0s 19us/step - loss: 10.7670 - val_loss: 14.0811\n",
      "Epoch 585/1000\n",
      "272/272 [==============================] - 0s 20us/step - loss: 10.7479 - val_loss: 14.0617\n",
      "Epoch 586/1000\n",
      "272/272 [==============================] - 0s 31us/step - loss: 10.7297 - val_loss: 14.0431\n",
      "Epoch 587/1000\n",
      "272/272 [==============================] - 0s 23us/step - loss: 10.7103 - val_loss: 14.0260\n",
      "Epoch 588/1000\n",
      "272/272 [==============================] - 0s 26us/step - loss: 10.6917 - val_loss: 14.0090\n",
      "Epoch 589/1000\n",
      "272/272 [==============================] - 0s 20us/step - loss: 10.6730 - val_loss: 13.9923\n",
      "Epoch 590/1000\n",
      "272/272 [==============================] - 0s 25us/step - loss: 10.6548 - val_loss: 13.9754\n",
      "Epoch 591/1000\n",
      "272/272 [==============================] - 0s 25us/step - loss: 10.6357 - val_loss: 13.9588\n",
      "Epoch 592/1000\n",
      "272/272 [==============================] - 0s 22us/step - loss: 10.6173 - val_loss: 13.9424\n",
      "Epoch 593/1000\n",
      "272/272 [==============================] - 0s 22us/step - loss: 10.5987 - val_loss: 13.9257\n",
      "Epoch 594/1000\n",
      "272/272 [==============================] - 0s 19us/step - loss: 10.5806 - val_loss: 13.9083\n",
      "Epoch 595/1000\n",
      "272/272 [==============================] - 0s 34us/step - loss: 10.5619 - val_loss: 13.8898\n",
      "Epoch 596/1000\n",
      "272/272 [==============================] - 0s 19us/step - loss: 10.5430 - val_loss: 13.8698\n",
      "Epoch 597/1000\n",
      "272/272 [==============================] - 0s 23us/step - loss: 10.5236 - val_loss: 13.8479\n",
      "Epoch 598/1000\n",
      "272/272 [==============================] - ETA: 0s - loss: 10.77 - 0s 20us/step - loss: 10.5047 - val_loss: 13.8239\n",
      "Epoch 599/1000\n",
      "272/272 [==============================] - 0s 28us/step - loss: 10.4852 - val_loss: 13.7998\n",
      "Epoch 600/1000\n",
      "272/272 [==============================] - 0s 24us/step - loss: 10.4658 - val_loss: 13.7750\n",
      "Epoch 601/1000\n",
      "272/272 [==============================] - 0s 20us/step - loss: 10.4483 - val_loss: 13.7495\n",
      "Epoch 602/1000\n",
      "272/272 [==============================] - 0s 26us/step - loss: 10.4284 - val_loss: 13.7263\n",
      "Epoch 603/1000\n",
      "272/272 [==============================] - 0s 24us/step - loss: 10.4098 - val_loss: 13.7044\n",
      "Epoch 604/1000\n",
      "272/272 [==============================] - 0s 30us/step - loss: 10.3910 - val_loss: 13.6841\n",
      "Epoch 605/1000\n",
      "272/272 [==============================] - 0s 29us/step - loss: 10.3728 - val_loss: 13.6647\n",
      "Epoch 606/1000\n",
      "272/272 [==============================] - 0s 21us/step - loss: 10.3541 - val_loss: 13.6461\n",
      "Epoch 607/1000\n",
      "272/272 [==============================] - 0s 35us/step - loss: 10.3356 - val_loss: 13.6276\n",
      "Epoch 608/1000\n",
      "272/272 [==============================] - 0s 32us/step - loss: 10.3174 - val_loss: 13.6085\n",
      "Epoch 609/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "272/272 [==============================] - 0s 26us/step - loss: 10.2991 - val_loss: 13.5884\n",
      "Epoch 610/1000\n",
      "272/272 [==============================] - 0s 20us/step - loss: 10.2809 - val_loss: 13.5671\n",
      "Epoch 611/1000\n",
      "272/272 [==============================] - 0s 30us/step - loss: 10.2622 - val_loss: 13.5465\n",
      "Epoch 612/1000\n",
      "272/272 [==============================] - 0s 24us/step - loss: 10.2437 - val_loss: 13.5247\n",
      "Epoch 613/1000\n",
      "272/272 [==============================] - 0s 28us/step - loss: 10.2252 - val_loss: 13.5019\n",
      "Epoch 614/1000\n",
      "272/272 [==============================] - 0s 31us/step - loss: 10.2074 - val_loss: 13.4782\n",
      "Epoch 615/1000\n",
      "272/272 [==============================] - 0s 22us/step - loss: 10.1898 - val_loss: 13.4555\n",
      "Epoch 616/1000\n",
      "272/272 [==============================] - 0s 23us/step - loss: 10.1712 - val_loss: 13.4339\n",
      "Epoch 617/1000\n",
      "272/272 [==============================] - 0s 21us/step - loss: 10.1525 - val_loss: 13.4113\n",
      "Epoch 618/1000\n",
      "272/272 [==============================] - 0s 19us/step - loss: 10.1345 - val_loss: 13.3882\n",
      "Epoch 619/1000\n",
      "272/272 [==============================] - 0s 33us/step - loss: 10.1161 - val_loss: 13.3656\n",
      "Epoch 620/1000\n",
      "272/272 [==============================] - 0s 21us/step - loss: 10.0979 - val_loss: 13.3434\n",
      "Epoch 621/1000\n",
      "272/272 [==============================] - 0s 32us/step - loss: 10.0792 - val_loss: 13.3218\n",
      "Epoch 622/1000\n",
      "272/272 [==============================] - 0s 23us/step - loss: 10.0608 - val_loss: 13.3009\n",
      "Epoch 623/1000\n",
      "272/272 [==============================] - ETA: 0s - loss: 9.924 - 0s 35us/step - loss: 10.0417 - val_loss: 13.2795\n",
      "Epoch 624/1000\n",
      "272/272 [==============================] - 0s 27us/step - loss: 10.0228 - val_loss: 13.2568\n",
      "Epoch 625/1000\n",
      "272/272 [==============================] - 0s 31us/step - loss: 10.0049 - val_loss: 13.2349\n",
      "Epoch 626/1000\n",
      "272/272 [==============================] - 0s 31us/step - loss: 9.9861 - val_loss: 13.2150\n",
      "Epoch 627/1000\n",
      "272/272 [==============================] - 0s 32us/step - loss: 9.9674 - val_loss: 13.1943\n",
      "Epoch 628/1000\n",
      "272/272 [==============================] - 0s 24us/step - loss: 9.9494 - val_loss: 13.1741\n",
      "Epoch 629/1000\n",
      "272/272 [==============================] - 0s 28us/step - loss: 9.9304 - val_loss: 13.1529\n",
      "Epoch 630/1000\n",
      "272/272 [==============================] - 0s 48us/step - loss: 9.9116 - val_loss: 13.1324\n",
      "Epoch 631/1000\n",
      "272/272 [==============================] - 0s 30us/step - loss: 9.8935 - val_loss: 13.1119\n",
      "Epoch 632/1000\n",
      "272/272 [==============================] - 0s 27us/step - loss: 9.8756 - val_loss: 13.0912\n",
      "Epoch 633/1000\n",
      "272/272 [==============================] - 0s 24us/step - loss: 9.8566 - val_loss: 13.0695\n",
      "Epoch 634/1000\n",
      "272/272 [==============================] - 0s 21us/step - loss: 9.8384 - val_loss: 13.0469\n",
      "Epoch 635/1000\n",
      "272/272 [==============================] - 0s 27us/step - loss: 9.8199 - val_loss: 13.0252\n",
      "Epoch 636/1000\n",
      "272/272 [==============================] - 0s 23us/step - loss: 9.8014 - val_loss: 13.0033\n",
      "Epoch 637/1000\n",
      "272/272 [==============================] - 0s 26us/step - loss: 9.7836 - val_loss: 12.9809\n",
      "Epoch 638/1000\n",
      "272/272 [==============================] - 0s 25us/step - loss: 9.7653 - val_loss: 12.9603\n",
      "Epoch 639/1000\n",
      "272/272 [==============================] - 0s 24us/step - loss: 9.7462 - val_loss: 12.9405\n",
      "Epoch 640/1000\n",
      "272/272 [==============================] - 0s 36us/step - loss: 9.7281 - val_loss: 12.9192\n",
      "Epoch 641/1000\n",
      "272/272 [==============================] - 0s 29us/step - loss: 9.7089 - val_loss: 12.8987\n",
      "Epoch 642/1000\n",
      "272/272 [==============================] - 0s 28us/step - loss: 9.6905 - val_loss: 12.8756\n",
      "Epoch 643/1000\n",
      "272/272 [==============================] - 0s 30us/step - loss: 9.6722 - val_loss: 12.8519\n",
      "Epoch 644/1000\n",
      "272/272 [==============================] - 0s 20us/step - loss: 9.6535 - val_loss: 12.8276\n",
      "Epoch 645/1000\n",
      "272/272 [==============================] - 0s 24us/step - loss: 9.6353 - val_loss: 12.8031\n",
      "Epoch 646/1000\n",
      "272/272 [==============================] - 0s 25us/step - loss: 9.6161 - val_loss: 12.7793\n",
      "Epoch 647/1000\n",
      "272/272 [==============================] - 0s 20us/step - loss: 9.5978 - val_loss: 12.7549\n",
      "Epoch 648/1000\n",
      "272/272 [==============================] - 0s 27us/step - loss: 9.5795 - val_loss: 12.7303\n",
      "Epoch 649/1000\n",
      "272/272 [==============================] - 0s 29us/step - loss: 9.5619 - val_loss: 12.7062\n",
      "Epoch 650/1000\n",
      "272/272 [==============================] - 0s 28us/step - loss: 9.5427 - val_loss: 12.6836\n",
      "Epoch 651/1000\n",
      "272/272 [==============================] - 0s 25us/step - loss: 9.5247 - val_loss: 12.6602\n",
      "Epoch 652/1000\n",
      "272/272 [==============================] - 0s 29us/step - loss: 9.5062 - val_loss: 12.6379\n",
      "Epoch 653/1000\n",
      "272/272 [==============================] - 0s 26us/step - loss: 9.4874 - val_loss: 12.6143\n",
      "Epoch 654/1000\n",
      "272/272 [==============================] - 0s 23us/step - loss: 9.4691 - val_loss: 12.5893\n",
      "Epoch 655/1000\n",
      "272/272 [==============================] - 0s 27us/step - loss: 9.4507 - val_loss: 12.5643\n",
      "Epoch 656/1000\n",
      "272/272 [==============================] - 0s 24us/step - loss: 9.4333 - val_loss: 12.5392\n",
      "Epoch 657/1000\n",
      "272/272 [==============================] - 0s 35us/step - loss: 9.4143 - val_loss: 12.5156\n",
      "Epoch 658/1000\n",
      "272/272 [==============================] - 0s 29us/step - loss: 9.3960 - val_loss: 12.4923\n",
      "Epoch 659/1000\n",
      "272/272 [==============================] - 0s 26us/step - loss: 9.3774 - val_loss: 12.4696\n",
      "Epoch 660/1000\n",
      "272/272 [==============================] - 0s 29us/step - loss: 9.3594 - val_loss: 12.4481\n",
      "Epoch 661/1000\n",
      "272/272 [==============================] - 0s 33us/step - loss: 9.3403 - val_loss: 12.4275\n",
      "Epoch 662/1000\n",
      "272/272 [==============================] - 0s 23us/step - loss: 9.3218 - val_loss: 12.4067\n",
      "Epoch 663/1000\n",
      "272/272 [==============================] - 0s 35us/step - loss: 9.3029 - val_loss: 12.3866\n",
      "Epoch 664/1000\n",
      "272/272 [==============================] - 0s 26us/step - loss: 9.2845 - val_loss: 12.3658\n",
      "Epoch 665/1000\n",
      "272/272 [==============================] - 0s 24us/step - loss: 9.2659 - val_loss: 12.3449\n",
      "Epoch 666/1000\n",
      "272/272 [==============================] - 0s 25us/step - loss: 9.2482 - val_loss: 12.3238\n",
      "Epoch 667/1000\n",
      "272/272 [==============================] - 0s 22us/step - loss: 9.2296 - val_loss: 12.3023\n",
      "Epoch 668/1000\n",
      "272/272 [==============================] - 0s 24us/step - loss: 9.2107 - val_loss: 12.2805\n",
      "Epoch 669/1000\n",
      "272/272 [==============================] - 0s 26us/step - loss: 9.1923 - val_loss: 12.2587\n",
      "Epoch 670/1000\n",
      "272/272 [==============================] - 0s 22us/step - loss: 9.1742 - val_loss: 12.2363\n",
      "Epoch 671/1000\n",
      "272/272 [==============================] - 0s 28us/step - loss: 9.1558 - val_loss: 12.2139\n",
      "Epoch 672/1000\n",
      "272/272 [==============================] - 0s 25us/step - loss: 9.1374 - val_loss: 12.1927\n",
      "Epoch 673/1000\n",
      "272/272 [==============================] - 0s 23us/step - loss: 9.1185 - val_loss: 12.1721\n",
      "Epoch 674/1000\n",
      "272/272 [==============================] - 0s 22us/step - loss: 9.1014 - val_loss: 12.1517\n",
      "Epoch 675/1000\n",
      "272/272 [==============================] - 0s 30us/step - loss: 9.0836 - val_loss: 12.1305\n",
      "Epoch 676/1000\n",
      "272/272 [==============================] - 0s 30us/step - loss: 9.0646 - val_loss: 12.1078\n",
      "Epoch 677/1000\n",
      "272/272 [==============================] - 0s 30us/step - loss: 9.0461 - val_loss: 12.0848\n",
      "Epoch 678/1000\n",
      "272/272 [==============================] - 0s 23us/step - loss: 9.0274 - val_loss: 12.0617\n",
      "Epoch 679/1000\n",
      "272/272 [==============================] - 0s 29us/step - loss: 9.0092 - val_loss: 12.0393\n",
      "Epoch 680/1000\n",
      "272/272 [==============================] - 0s 23us/step - loss: 8.9902 - val_loss: 12.0180\n",
      "Epoch 681/1000\n",
      "272/272 [==============================] - 0s 26us/step - loss: 8.9723 - val_loss: 11.9957\n",
      "Epoch 682/1000\n",
      "272/272 [==============================] - 0s 20us/step - loss: 8.9541 - val_loss: 11.9733\n",
      "Epoch 683/1000\n",
      "272/272 [==============================] - 0s 21us/step - loss: 8.9358 - val_loss: 11.9514\n",
      "Epoch 684/1000\n",
      "272/272 [==============================] - 0s 18us/step - loss: 8.9188 - val_loss: 11.9295\n",
      "Epoch 685/1000\n",
      "272/272 [==============================] - 0s 22us/step - loss: 8.8995 - val_loss: 11.9082\n",
      "Epoch 686/1000\n",
      "272/272 [==============================] - 0s 29us/step - loss: 8.8817 - val_loss: 11.8864\n",
      "Epoch 687/1000\n",
      "272/272 [==============================] - 0s 24us/step - loss: 8.8631 - val_loss: 11.8639\n",
      "Epoch 688/1000\n",
      "272/272 [==============================] - 0s 19us/step - loss: 8.8449 - val_loss: 11.8415\n",
      "Epoch 689/1000\n",
      "272/272 [==============================] - 0s 34us/step - loss: 8.8265 - val_loss: 11.8190\n",
      "Epoch 690/1000\n",
      "272/272 [==============================] - 0s 26us/step - loss: 8.8079 - val_loss: 11.7968\n",
      "Epoch 691/1000\n",
      "272/272 [==============================] - 0s 38us/step - loss: 8.7901 - val_loss: 11.7753\n",
      "Epoch 692/1000\n",
      "272/272 [==============================] - 0s 32us/step - loss: 8.7713 - val_loss: 11.7529\n",
      "Epoch 693/1000\n",
      "272/272 [==============================] - 0s 23us/step - loss: 8.7541 - val_loss: 11.7308\n",
      "Epoch 694/1000\n",
      "272/272 [==============================] - 0s 26us/step - loss: 8.7364 - val_loss: 11.7085\n",
      "Epoch 695/1000\n",
      "272/272 [==============================] - 0s 30us/step - loss: 8.7175 - val_loss: 11.6844\n",
      "Epoch 696/1000\n",
      "272/272 [==============================] - 0s 40us/step - loss: 8.6994 - val_loss: 11.6607\n",
      "Epoch 697/1000\n",
      "272/272 [==============================] - 0s 26us/step - loss: 8.6808 - val_loss: 11.6382\n",
      "Epoch 698/1000\n",
      "272/272 [==============================] - 0s 20us/step - loss: 8.6629 - val_loss: 11.6149\n",
      "Epoch 699/1000\n",
      "272/272 [==============================] - 0s 27us/step - loss: 8.6445 - val_loss: 11.5905\n",
      "Epoch 700/1000\n",
      "272/272 [==============================] - ETA: 0s - loss: 9.193 - 0s 23us/step - loss: 8.6266 - val_loss: 11.5664\n",
      "Epoch 701/1000\n",
      "272/272 [==============================] - 0s 26us/step - loss: 8.6080 - val_loss: 11.5434\n",
      "Epoch 702/1000\n",
      "272/272 [==============================] - 0s 31us/step - loss: 8.5897 - val_loss: 11.5189\n",
      "Epoch 703/1000\n",
      "272/272 [==============================] - 0s 35us/step - loss: 8.5715 - val_loss: 11.4938\n",
      "Epoch 704/1000\n",
      "272/272 [==============================] - 0s 35us/step - loss: 8.5532 - val_loss: 11.4686\n",
      "Epoch 705/1000\n",
      "272/272 [==============================] - 0s 28us/step - loss: 8.5357 - val_loss: 11.4433\n",
      "Epoch 706/1000\n",
      "272/272 [==============================] - 0s 33us/step - loss: 8.5170 - val_loss: 11.4200\n",
      "Epoch 707/1000\n",
      "272/272 [==============================] - 0s 36us/step - loss: 8.4988 - val_loss: 11.3962\n",
      "Epoch 708/1000\n",
      "272/272 [==============================] - 0s 39us/step - loss: 8.4802 - val_loss: 11.3725\n",
      "Epoch 709/1000\n",
      "272/272 [==============================] - ETA: 0s - loss: 8.341 - 0s 19us/step - loss: 8.4624 - val_loss: 11.3487\n",
      "Epoch 710/1000\n",
      "272/272 [==============================] - 0s 27us/step - loss: 8.4448 - val_loss: 11.3247\n",
      "Epoch 711/1000\n",
      "272/272 [==============================] - 0s 24us/step - loss: 8.4273 - val_loss: 11.3014\n",
      "Epoch 712/1000\n",
      "272/272 [==============================] - 0s 21us/step - loss: 8.4087 - val_loss: 11.2803\n",
      "Epoch 713/1000\n",
      "272/272 [==============================] - 0s 26us/step - loss: 8.3898 - val_loss: 11.2604\n",
      "Epoch 714/1000\n",
      "272/272 [==============================] - 0s 24us/step - loss: 8.3709 - val_loss: 11.2404\n",
      "Epoch 715/1000\n",
      "272/272 [==============================] - 0s 28us/step - loss: 8.3518 - val_loss: 11.2208\n",
      "Epoch 716/1000\n",
      "272/272 [==============================] - 0s 25us/step - loss: 8.3332 - val_loss: 11.2018\n",
      "Epoch 717/1000\n",
      "272/272 [==============================] - 0s 22us/step - loss: 8.3150 - val_loss: 11.1838\n",
      "Epoch 718/1000\n",
      "272/272 [==============================] - 0s 21us/step - loss: 8.2970 - val_loss: 11.1649\n",
      "Epoch 719/1000\n",
      "272/272 [==============================] - 0s 36us/step - loss: 8.2795 - val_loss: 11.1446\n",
      "Epoch 720/1000\n",
      "272/272 [==============================] - 0s 33us/step - loss: 8.2608 - val_loss: 11.1223\n",
      "Epoch 721/1000\n",
      "272/272 [==============================] - 0s 32us/step - loss: 8.2430 - val_loss: 11.0998\n",
      "Epoch 722/1000\n",
      "272/272 [==============================] - 0s 28us/step - loss: 8.2251 - val_loss: 11.0782\n",
      "Epoch 723/1000\n",
      "272/272 [==============================] - 0s 26us/step - loss: 8.2082 - val_loss: 11.0571\n",
      "Epoch 724/1000\n",
      "272/272 [==============================] - 0s 32us/step - loss: 8.1893 - val_loss: 11.0344\n",
      "Epoch 725/1000\n",
      "272/272 [==============================] - 0s 23us/step - loss: 8.1710 - val_loss: 11.0116\n",
      "Epoch 726/1000\n",
      "272/272 [==============================] - 0s 36us/step - loss: 8.1526 - val_loss: 10.9876\n",
      "Epoch 727/1000\n",
      "272/272 [==============================] - 0s 27us/step - loss: 8.1349 - val_loss: 10.9636\n",
      "Epoch 728/1000\n",
      "272/272 [==============================] - 0s 25us/step - loss: 8.1159 - val_loss: 10.9398\n",
      "Epoch 729/1000\n",
      "272/272 [==============================] - 0s 19us/step - loss: 8.0980 - val_loss: 10.9140\n",
      "Epoch 730/1000\n",
      "272/272 [==============================] - 0s 23us/step - loss: 8.0801 - val_loss: 10.8884\n",
      "Epoch 731/1000\n",
      "272/272 [==============================] - 0s 20us/step - loss: 8.0617 - val_loss: 10.8638\n",
      "Epoch 732/1000\n",
      "272/272 [==============================] - 0s 34us/step - loss: 8.0431 - val_loss: 10.8392\n",
      "Epoch 733/1000\n",
      "272/272 [==============================] - 0s 19us/step - loss: 8.0256 - val_loss: 10.8151\n",
      "Epoch 734/1000\n",
      "272/272 [==============================] - 0s 22us/step - loss: 8.0070 - val_loss: 10.7922\n",
      "Epoch 735/1000\n",
      "272/272 [==============================] - 0s 22us/step - loss: 7.9893 - val_loss: 10.7711\n",
      "Epoch 736/1000\n",
      "272/272 [==============================] - 0s 23us/step - loss: 7.9712 - val_loss: 10.7510\n",
      "Epoch 737/1000\n",
      "272/272 [==============================] - 0s 21us/step - loss: 7.9538 - val_loss: 10.7306\n",
      "Epoch 738/1000\n",
      "272/272 [==============================] - 0s 27us/step - loss: 7.9364 - val_loss: 10.7105\n",
      "Epoch 739/1000\n",
      "272/272 [==============================] - 0s 25us/step - loss: 7.9186 - val_loss: 10.6908\n",
      "Epoch 740/1000\n",
      "272/272 [==============================] - 0s 30us/step - loss: 7.9014 - val_loss: 10.6707\n",
      "Epoch 741/1000\n",
      "272/272 [==============================] - 0s 28us/step - loss: 7.8838 - val_loss: 10.6495\n",
      "Epoch 742/1000\n",
      "272/272 [==============================] - 0s 26us/step - loss: 7.8664 - val_loss: 10.6280\n",
      "Epoch 743/1000\n",
      "272/272 [==============================] - 0s 32us/step - loss: 7.8489 - val_loss: 10.6063\n",
      "Epoch 744/1000\n",
      "272/272 [==============================] - 0s 29us/step - loss: 7.8312 - val_loss: 10.5839\n",
      "Epoch 745/1000\n",
      "272/272 [==============================] - 0s 25us/step - loss: 7.8138 - val_loss: 10.5609\n",
      "Epoch 746/1000\n",
      "272/272 [==============================] - 0s 27us/step - loss: 7.7956 - val_loss: 10.5382\n",
      "Epoch 747/1000\n",
      "272/272 [==============================] - 0s 23us/step - loss: 7.7781 - val_loss: 10.5155\n",
      "Epoch 748/1000\n",
      "272/272 [==============================] - 0s 25us/step - loss: 7.7600 - val_loss: 10.4923\n",
      "Epoch 749/1000\n",
      "272/272 [==============================] - 0s 22us/step - loss: 7.7416 - val_loss: 10.4682\n",
      "Epoch 750/1000\n",
      "272/272 [==============================] - 0s 25us/step - loss: 7.7233 - val_loss: 10.4429\n",
      "Epoch 751/1000\n",
      "272/272 [==============================] - 0s 25us/step - loss: 7.7061 - val_loss: 10.4174\n",
      "Epoch 752/1000\n",
      "272/272 [==============================] - 0s 19us/step - loss: 7.6876 - val_loss: 10.3922\n",
      "Epoch 753/1000\n",
      "272/272 [==============================] - 0s 29us/step - loss: 7.6697 - val_loss: 10.3667\n",
      "Epoch 754/1000\n",
      "272/272 [==============================] - 0s 30us/step - loss: 7.6518 - val_loss: 10.3409\n",
      "Epoch 755/1000\n",
      "272/272 [==============================] - 0s 24us/step - loss: 7.6344 - val_loss: 10.3153\n",
      "Epoch 756/1000\n",
      "272/272 [==============================] - 0s 25us/step - loss: 7.6158 - val_loss: 10.2892\n",
      "Epoch 757/1000\n",
      "272/272 [==============================] - 0s 20us/step - loss: 7.5979 - val_loss: 10.2599\n",
      "Epoch 758/1000\n",
      "272/272 [==============================] - 0s 22us/step - loss: 7.5802 - val_loss: 10.2304\n",
      "Epoch 759/1000\n",
      "272/272 [==============================] - 0s 27us/step - loss: 7.5627 - val_loss: 10.2013\n",
      "Epoch 760/1000\n",
      "272/272 [==============================] - 0s 21us/step - loss: 7.5453 - val_loss: 10.1727\n",
      "Epoch 761/1000\n",
      "272/272 [==============================] - 0s 28us/step - loss: 7.5279 - val_loss: 10.1444\n",
      "Epoch 762/1000\n",
      "272/272 [==============================] - 0s 25us/step - loss: 7.5105 - val_loss: 10.1158\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 763/1000\n",
      "272/272 [==============================] - 0s 24us/step - loss: 7.4930 - val_loss: 10.0888\n",
      "Epoch 764/1000\n",
      "272/272 [==============================] - 0s 24us/step - loss: 7.4754 - val_loss: 10.0633\n",
      "Epoch 765/1000\n",
      "272/272 [==============================] - 0s 28us/step - loss: 7.4573 - val_loss: 10.0386\n",
      "Epoch 766/1000\n",
      "272/272 [==============================] - 0s 37us/step - loss: 7.4391 - val_loss: 10.0153\n",
      "Epoch 767/1000\n",
      "272/272 [==============================] - 0s 26us/step - loss: 7.4213 - val_loss: 9.9942\n",
      "Epoch 768/1000\n",
      "272/272 [==============================] - 0s 21us/step - loss: 7.4031 - val_loss: 9.9750\n",
      "Epoch 769/1000\n",
      "272/272 [==============================] - 0s 24us/step - loss: 7.3849 - val_loss: 9.9539\n",
      "Epoch 770/1000\n",
      "272/272 [==============================] - 0s 35us/step - loss: 7.3678 - val_loss: 9.9322\n",
      "Epoch 771/1000\n",
      "272/272 [==============================] - 0s 23us/step - loss: 7.3498 - val_loss: 9.9108\n",
      "Epoch 772/1000\n",
      "272/272 [==============================] - 0s 30us/step - loss: 7.3321 - val_loss: 9.8888\n",
      "Epoch 773/1000\n",
      "272/272 [==============================] - 0s 21us/step - loss: 7.3146 - val_loss: 9.8653\n",
      "Epoch 774/1000\n",
      "272/272 [==============================] - 0s 27us/step - loss: 7.2971 - val_loss: 9.8410\n",
      "Epoch 775/1000\n",
      "272/272 [==============================] - 0s 29us/step - loss: 7.2794 - val_loss: 9.8170\n",
      "Epoch 776/1000\n",
      "272/272 [==============================] - 0s 20us/step - loss: 7.2616 - val_loss: 9.7927\n",
      "Epoch 777/1000\n",
      "272/272 [==============================] - 0s 18us/step - loss: 7.2442 - val_loss: 9.7672\n",
      "Epoch 778/1000\n",
      "272/272 [==============================] - 0s 20us/step - loss: 7.2272 - val_loss: 9.7412\n",
      "Epoch 779/1000\n",
      "272/272 [==============================] - 0s 28us/step - loss: 7.2089 - val_loss: 9.7153\n",
      "Epoch 780/1000\n",
      "272/272 [==============================] - 0s 25us/step - loss: 7.1912 - val_loss: 9.6887\n",
      "Epoch 781/1000\n",
      "272/272 [==============================] - 0s 30us/step - loss: 7.1732 - val_loss: 9.6625\n",
      "Epoch 782/1000\n",
      "272/272 [==============================] - 0s 35us/step - loss: 7.1560 - val_loss: 9.6358\n",
      "Epoch 783/1000\n",
      "272/272 [==============================] - 0s 29us/step - loss: 7.1384 - val_loss: 9.6109\n",
      "Epoch 784/1000\n",
      "272/272 [==============================] - 0s 49us/step - loss: 7.1206 - val_loss: 9.5865\n",
      "Epoch 785/1000\n",
      "272/272 [==============================] - 0s 24us/step - loss: 7.1036 - val_loss: 9.5614\n",
      "Epoch 786/1000\n",
      "272/272 [==============================] - 0s 21us/step - loss: 7.0855 - val_loss: 9.5361\n",
      "Epoch 787/1000\n",
      "272/272 [==============================] - 0s 32us/step - loss: 7.0685 - val_loss: 9.5094\n",
      "Epoch 788/1000\n",
      "272/272 [==============================] - 0s 37us/step - loss: 7.0512 - val_loss: 9.4839\n",
      "Epoch 789/1000\n",
      "272/272 [==============================] - 0s 24us/step - loss: 7.0333 - val_loss: 9.4594\n",
      "Epoch 790/1000\n",
      "272/272 [==============================] - 0s 29us/step - loss: 7.0155 - val_loss: 9.4352\n",
      "Epoch 791/1000\n",
      "272/272 [==============================] - 0s 32us/step - loss: 6.9986 - val_loss: 9.4109\n",
      "Epoch 792/1000\n",
      "272/272 [==============================] - 0s 22us/step - loss: 6.9806 - val_loss: 9.3867\n",
      "Epoch 793/1000\n",
      "272/272 [==============================] - 0s 24us/step - loss: 6.9628 - val_loss: 9.3617\n",
      "Epoch 794/1000\n",
      "272/272 [==============================] - 0s 34us/step - loss: 6.9455 - val_loss: 9.3364\n",
      "Epoch 795/1000\n",
      "272/272 [==============================] - 0s 21us/step - loss: 6.9279 - val_loss: 9.3109\n",
      "Epoch 796/1000\n",
      "272/272 [==============================] - 0s 25us/step - loss: 6.9103 - val_loss: 9.2860\n",
      "Epoch 797/1000\n",
      "272/272 [==============================] - 0s 27us/step - loss: 6.8921 - val_loss: 9.2604\n",
      "Epoch 798/1000\n",
      "272/272 [==============================] - 0s 28us/step - loss: 6.8748 - val_loss: 9.2337\n",
      "Epoch 799/1000\n",
      "272/272 [==============================] - 0s 23us/step - loss: 6.8567 - val_loss: 9.2047\n",
      "Epoch 800/1000\n",
      "272/272 [==============================] - 0s 28us/step - loss: 6.8391 - val_loss: 9.1756\n",
      "Epoch 801/1000\n",
      "272/272 [==============================] - 0s 20us/step - loss: 6.8212 - val_loss: 9.1460\n",
      "Epoch 802/1000\n",
      "272/272 [==============================] - 0s 24us/step - loss: 6.8047 - val_loss: 9.1160\n",
      "Epoch 803/1000\n",
      "272/272 [==============================] - 0s 21us/step - loss: 6.7869 - val_loss: 9.0888\n",
      "Epoch 804/1000\n",
      "272/272 [==============================] - 0s 26us/step - loss: 6.7688 - val_loss: 9.0630\n",
      "Epoch 805/1000\n",
      "272/272 [==============================] - 0s 19us/step - loss: 6.7516 - val_loss: 9.0370\n",
      "Epoch 806/1000\n",
      "272/272 [==============================] - 0s 24us/step - loss: 6.7344 - val_loss: 9.0120\n",
      "Epoch 807/1000\n",
      "272/272 [==============================] - 0s 22us/step - loss: 6.7167 - val_loss: 8.9890\n",
      "Epoch 808/1000\n",
      "272/272 [==============================] - 0s 26us/step - loss: 6.6990 - val_loss: 8.9662\n",
      "Epoch 809/1000\n",
      "272/272 [==============================] - 0s 34us/step - loss: 6.6813 - val_loss: 8.9461\n",
      "Epoch 810/1000\n",
      "272/272 [==============================] - 0s 31us/step - loss: 6.6633 - val_loss: 8.9279\n",
      "Epoch 811/1000\n",
      "272/272 [==============================] - 0s 21us/step - loss: 6.6462 - val_loss: 8.9097\n",
      "Epoch 812/1000\n",
      "272/272 [==============================] - 0s 20us/step - loss: 6.6282 - val_loss: 8.8900\n",
      "Epoch 813/1000\n",
      "272/272 [==============================] - 0s 31us/step - loss: 6.6111 - val_loss: 8.8705\n",
      "Epoch 814/1000\n",
      "272/272 [==============================] - 0s 27us/step - loss: 6.5941 - val_loss: 8.8510\n",
      "Epoch 815/1000\n",
      "272/272 [==============================] - 0s 34us/step - loss: 6.5767 - val_loss: 8.8309\n",
      "Epoch 816/1000\n",
      "272/272 [==============================] - 0s 21us/step - loss: 6.5597 - val_loss: 8.8101\n",
      "Epoch 817/1000\n",
      "272/272 [==============================] - 0s 25us/step - loss: 6.5418 - val_loss: 8.7894\n",
      "Epoch 818/1000\n",
      "272/272 [==============================] - 0s 18us/step - loss: 6.5248 - val_loss: 8.7673\n",
      "Epoch 819/1000\n",
      "272/272 [==============================] - 0s 32us/step - loss: 6.5076 - val_loss: 8.7447\n",
      "Epoch 820/1000\n",
      "272/272 [==============================] - 0s 36us/step - loss: 6.4903 - val_loss: 8.7215\n",
      "Epoch 821/1000\n",
      "272/272 [==============================] - 0s 28us/step - loss: 6.4728 - val_loss: 8.6975\n",
      "Epoch 822/1000\n",
      "272/272 [==============================] - 0s 25us/step - loss: 6.4551 - val_loss: 8.6720\n",
      "Epoch 823/1000\n",
      "272/272 [==============================] - 0s 26us/step - loss: 6.4371 - val_loss: 8.6464\n",
      "Epoch 824/1000\n",
      "272/272 [==============================] - 0s 37us/step - loss: 6.4205 - val_loss: 8.6202\n",
      "Epoch 825/1000\n",
      "272/272 [==============================] - 0s 27us/step - loss: 6.4020 - val_loss: 8.5942\n",
      "Epoch 826/1000\n",
      "272/272 [==============================] - 0s 47us/step - loss: 6.3858 - val_loss: 8.5674\n",
      "Epoch 827/1000\n",
      "272/272 [==============================] - 0s 37us/step - loss: 6.3676 - val_loss: 8.5420\n",
      "Epoch 828/1000\n",
      "272/272 [==============================] - ETA: 0s - loss: 6.080 - 0s 47us/step - loss: 6.3504 - val_loss: 8.5163\n",
      "Epoch 829/1000\n",
      "272/272 [==============================] - 0s 42us/step - loss: 6.3340 - val_loss: 8.4913\n",
      "Epoch 830/1000\n",
      "272/272 [==============================] - 0s 29us/step - loss: 6.3173 - val_loss: 8.4682\n",
      "Epoch 831/1000\n",
      "272/272 [==============================] - 0s 46us/step - loss: 6.2991 - val_loss: 8.4459\n",
      "Epoch 832/1000\n",
      "272/272 [==============================] - 0s 57us/step - loss: 6.2825 - val_loss: 8.4240\n",
      "Epoch 833/1000\n",
      "272/272 [==============================] - 0s 23us/step - loss: 6.2646 - val_loss: 8.4010\n",
      "Epoch 834/1000\n",
      "272/272 [==============================] - 0s 50us/step - loss: 6.2476 - val_loss: 8.3788\n",
      "Epoch 835/1000\n",
      "272/272 [==============================] - 0s 18us/step - loss: 6.2303 - val_loss: 8.3576\n",
      "Epoch 836/1000\n",
      "272/272 [==============================] - 0s 24us/step - loss: 6.2134 - val_loss: 8.3344\n",
      "Epoch 837/1000\n",
      "272/272 [==============================] - 0s 37us/step - loss: 6.1960 - val_loss: 8.3102\n",
      "Epoch 838/1000\n",
      "272/272 [==============================] - 0s 24us/step - loss: 6.1793 - val_loss: 8.2863\n",
      "Epoch 839/1000\n",
      "272/272 [==============================] - 0s 24us/step - loss: 6.1621 - val_loss: 8.2633\n",
      "Epoch 840/1000\n",
      "272/272 [==============================] - 0s 31us/step - loss: 6.1454 - val_loss: 8.2414\n",
      "Epoch 841/1000\n",
      "272/272 [==============================] - 0s 32us/step - loss: 6.1283 - val_loss: 8.2203\n",
      "Epoch 842/1000\n",
      "272/272 [==============================] - 0s 21us/step - loss: 6.1115 - val_loss: 8.1984\n",
      "Epoch 843/1000\n",
      "272/272 [==============================] - 0s 29us/step - loss: 6.0944 - val_loss: 8.1762\n",
      "Epoch 844/1000\n",
      "272/272 [==============================] - 0s 22us/step - loss: 6.0779 - val_loss: 8.1535\n",
      "Epoch 845/1000\n",
      "272/272 [==============================] - 0s 25us/step - loss: 6.0617 - val_loss: 8.1308\n",
      "Epoch 846/1000\n",
      "272/272 [==============================] - 0s 29us/step - loss: 6.0445 - val_loss: 8.1095\n",
      "Epoch 847/1000\n",
      "272/272 [==============================] - 0s 25us/step - loss: 6.0279 - val_loss: 8.0891\n",
      "Epoch 848/1000\n",
      "272/272 [==============================] - 0s 21us/step - loss: 6.0113 - val_loss: 8.0699\n",
      "Epoch 849/1000\n",
      "272/272 [==============================] - 0s 26us/step - loss: 5.9946 - val_loss: 8.0488\n",
      "Epoch 850/1000\n",
      "272/272 [==============================] - 0s 30us/step - loss: 5.9784 - val_loss: 8.0254\n",
      "Epoch 851/1000\n",
      "272/272 [==============================] - 0s 18us/step - loss: 5.9619 - val_loss: 8.0030\n",
      "Epoch 852/1000\n",
      "272/272 [==============================] - 0s 27us/step - loss: 5.9452 - val_loss: 7.9820\n",
      "Epoch 853/1000\n",
      "272/272 [==============================] - 0s 19us/step - loss: 5.9286 - val_loss: 7.9619\n",
      "Epoch 854/1000\n",
      "272/272 [==============================] - 0s 27us/step - loss: 5.9129 - val_loss: 7.9418\n",
      "Epoch 855/1000\n",
      "272/272 [==============================] - 0s 33us/step - loss: 5.8965 - val_loss: 7.9213\n",
      "Epoch 856/1000\n",
      "272/272 [==============================] - 0s 28us/step - loss: 5.8816 - val_loss: 7.9005\n",
      "Epoch 857/1000\n",
      "272/272 [==============================] - 0s 23us/step - loss: 5.8657 - val_loss: 7.8805\n",
      "Epoch 858/1000\n",
      "272/272 [==============================] - ETA: 0s - loss: 5.500 - 0s 38us/step - loss: 5.8494 - val_loss: 7.8613\n",
      "Epoch 859/1000\n",
      "272/272 [==============================] - 0s 36us/step - loss: 5.8331 - val_loss: 7.8419\n",
      "Epoch 860/1000\n",
      "272/272 [==============================] - 0s 27us/step - loss: 5.8170 - val_loss: 7.8222\n",
      "Epoch 861/1000\n",
      "272/272 [==============================] - 0s 30us/step - loss: 5.8009 - val_loss: 7.8018\n",
      "Epoch 862/1000\n",
      "272/272 [==============================] - 0s 22us/step - loss: 5.7845 - val_loss: 7.7816\n",
      "Epoch 863/1000\n",
      "272/272 [==============================] - 0s 25us/step - loss: 5.7697 - val_loss: 7.7609\n",
      "Epoch 864/1000\n",
      "272/272 [==============================] - 0s 22us/step - loss: 5.7533 - val_loss: 7.7423\n",
      "Epoch 865/1000\n",
      "272/272 [==============================] - 0s 29us/step - loss: 5.7370 - val_loss: 7.7236\n",
      "Epoch 866/1000\n",
      "272/272 [==============================] - 0s 30us/step - loss: 5.7214 - val_loss: 7.7063\n",
      "Epoch 867/1000\n",
      "272/272 [==============================] - 0s 21us/step - loss: 5.7066 - val_loss: 7.6888\n",
      "Epoch 868/1000\n",
      "272/272 [==============================] - 0s 25us/step - loss: 5.6909 - val_loss: 7.6701\n",
      "Epoch 869/1000\n",
      "272/272 [==============================] - 0s 23us/step - loss: 5.6764 - val_loss: 7.6514\n",
      "Epoch 870/1000\n",
      "272/272 [==============================] - 0s 33us/step - loss: 5.6610 - val_loss: 7.6315\n",
      "Epoch 871/1000\n",
      "272/272 [==============================] - 0s 25us/step - loss: 5.6460 - val_loss: 7.6104\n",
      "Epoch 872/1000\n",
      "272/272 [==============================] - 0s 36us/step - loss: 5.6316 - val_loss: 7.5896\n",
      "Epoch 873/1000\n",
      "272/272 [==============================] - 0s 23us/step - loss: 5.6156 - val_loss: 7.5702\n",
      "Epoch 874/1000\n",
      "272/272 [==============================] - 0s 38us/step - loss: 5.6003 - val_loss: 7.5511\n",
      "Epoch 875/1000\n",
      "272/272 [==============================] - 0s 32us/step - loss: 5.5852 - val_loss: 7.5324\n",
      "Epoch 876/1000\n",
      "272/272 [==============================] - 0s 25us/step - loss: 5.5701 - val_loss: 7.5127\n",
      "Epoch 877/1000\n",
      "272/272 [==============================] - 0s 29us/step - loss: 5.5547 - val_loss: 7.4907\n",
      "Epoch 878/1000\n",
      "272/272 [==============================] - 0s 26us/step - loss: 5.5405 - val_loss: 7.4667\n",
      "Epoch 879/1000\n",
      "272/272 [==============================] - 0s 21us/step - loss: 5.5254 - val_loss: 7.4448\n",
      "Epoch 880/1000\n",
      "272/272 [==============================] - 0s 28us/step - loss: 5.5109 - val_loss: 7.4240\n",
      "Epoch 881/1000\n",
      "272/272 [==============================] - 0s 31us/step - loss: 5.4957 - val_loss: 7.4037\n",
      "Epoch 882/1000\n",
      "272/272 [==============================] - 0s 24us/step - loss: 5.4814 - val_loss: 7.3838\n",
      "Epoch 883/1000\n",
      "272/272 [==============================] - 0s 31us/step - loss: 5.4662 - val_loss: 7.3645\n",
      "Epoch 884/1000\n",
      "272/272 [==============================] - 0s 26us/step - loss: 5.4519 - val_loss: 7.3453\n",
      "Epoch 885/1000\n",
      "272/272 [==============================] - 0s 33us/step - loss: 5.4373 - val_loss: 7.3248\n",
      "Epoch 886/1000\n",
      "272/272 [==============================] - 0s 34us/step - loss: 5.4232 - val_loss: 7.3035\n",
      "Epoch 887/1000\n",
      "272/272 [==============================] - 0s 34us/step - loss: 5.4096 - val_loss: 7.2815\n",
      "Epoch 888/1000\n",
      "272/272 [==============================] - 0s 31us/step - loss: 5.3956 - val_loss: 7.2592\n",
      "Epoch 889/1000\n",
      "272/272 [==============================] - 0s 28us/step - loss: 5.3813 - val_loss: 7.2378\n",
      "Epoch 890/1000\n",
      "272/272 [==============================] - 0s 43us/step - loss: 5.3666 - val_loss: 7.2168\n",
      "Epoch 891/1000\n",
      "272/272 [==============================] - 0s 34us/step - loss: 5.3520 - val_loss: 7.1956\n",
      "Epoch 892/1000\n",
      "272/272 [==============================] - 0s 23us/step - loss: 5.3382 - val_loss: 7.1750\n",
      "Epoch 893/1000\n",
      "272/272 [==============================] - ETA: 0s - loss: 5.190 - 0s 30us/step - loss: 5.3239 - val_loss: 7.1557\n",
      "Epoch 894/1000\n",
      "272/272 [==============================] - 0s 24us/step - loss: 5.3102 - val_loss: 7.1363\n",
      "Epoch 895/1000\n",
      "272/272 [==============================] - 0s 24us/step - loss: 5.2965 - val_loss: 7.1165\n",
      "Epoch 896/1000\n",
      "272/272 [==============================] - 0s 25us/step - loss: 5.2852 - val_loss: 7.0969\n",
      "Epoch 897/1000\n",
      "272/272 [==============================] - 0s 35us/step - loss: 5.2698 - val_loss: 7.0773\n",
      "Epoch 898/1000\n",
      "272/272 [==============================] - 0s 26us/step - loss: 5.2561 - val_loss: 7.0577\n",
      "Epoch 899/1000\n",
      "272/272 [==============================] - 0s 30us/step - loss: 5.2423 - val_loss: 7.0378\n",
      "Epoch 900/1000\n",
      "272/272 [==============================] - 0s 27us/step - loss: 5.2297 - val_loss: 7.0172\n",
      "Epoch 901/1000\n",
      "272/272 [==============================] - 0s 26us/step - loss: 5.2159 - val_loss: 6.9983\n",
      "Epoch 902/1000\n",
      "272/272 [==============================] - 0s 27us/step - loss: 5.2033 - val_loss: 6.9800\n",
      "Epoch 903/1000\n",
      "272/272 [==============================] - 0s 27us/step - loss: 5.1899 - val_loss: 6.9625\n",
      "Epoch 904/1000\n",
      "272/272 [==============================] - 0s 28us/step - loss: 5.1774 - val_loss: 6.9453\n",
      "Epoch 905/1000\n",
      "272/272 [==============================] - 0s 26us/step - loss: 5.1643 - val_loss: 6.9294\n",
      "Epoch 906/1000\n",
      "272/272 [==============================] - 0s 19us/step - loss: 5.1520 - val_loss: 6.9138\n",
      "Epoch 907/1000\n",
      "272/272 [==============================] - 0s 23us/step - loss: 5.1389 - val_loss: 6.8984\n",
      "Epoch 908/1000\n",
      "272/272 [==============================] - 0s 21us/step - loss: 5.1265 - val_loss: 6.8827\n",
      "Epoch 909/1000\n",
      "272/272 [==============================] - 0s 25us/step - loss: 5.1138 - val_loss: 6.8669\n",
      "Epoch 910/1000\n",
      "272/272 [==============================] - 0s 25us/step - loss: 5.1015 - val_loss: 6.8519\n",
      "Epoch 911/1000\n",
      "272/272 [==============================] - 0s 26us/step - loss: 5.0897 - val_loss: 6.8360\n",
      "Epoch 912/1000\n",
      "272/272 [==============================] - 0s 20us/step - loss: 5.0781 - val_loss: 6.8214\n",
      "Epoch 913/1000\n",
      "272/272 [==============================] - 0s 27us/step - loss: 5.0659 - val_loss: 6.8079\n",
      "Epoch 914/1000\n",
      "272/272 [==============================] - 0s 18us/step - loss: 5.0540 - val_loss: 6.7939\n",
      "Epoch 915/1000\n",
      "272/272 [==============================] - 0s 33us/step - loss: 5.0421 - val_loss: 6.7797\n",
      "Epoch 916/1000\n",
      "272/272 [==============================] - 0s 26us/step - loss: 5.0301 - val_loss: 6.7647\n",
      "Epoch 917/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "272/272 [==============================] - 0s 29us/step - loss: 5.0180 - val_loss: 6.7480\n",
      "Epoch 918/1000\n",
      "272/272 [==============================] - 0s 23us/step - loss: 5.0065 - val_loss: 6.7318\n",
      "Epoch 919/1000\n",
      "272/272 [==============================] - 0s 30us/step - loss: 4.9949 - val_loss: 6.7162\n",
      "Epoch 920/1000\n",
      "272/272 [==============================] - 0s 22us/step - loss: 4.9834 - val_loss: 6.7006\n",
      "Epoch 921/1000\n",
      "272/272 [==============================] - 0s 29us/step - loss: 4.9722 - val_loss: 6.6842\n",
      "Epoch 922/1000\n",
      "272/272 [==============================] - 0s 34us/step - loss: 4.9607 - val_loss: 6.6660\n",
      "Epoch 923/1000\n",
      "272/272 [==============================] - 0s 28us/step - loss: 4.9503 - val_loss: 6.6475\n",
      "Epoch 924/1000\n",
      "272/272 [==============================] - 0s 41us/step - loss: 4.9379 - val_loss: 6.6303\n",
      "Epoch 925/1000\n",
      "272/272 [==============================] - 0s 29us/step - loss: 4.9272 - val_loss: 6.6122\n",
      "Epoch 926/1000\n",
      "272/272 [==============================] - 0s 21us/step - loss: 4.9159 - val_loss: 6.5945\n",
      "Epoch 927/1000\n",
      "272/272 [==============================] - 0s 27us/step - loss: 4.9054 - val_loss: 6.5773\n",
      "Epoch 928/1000\n",
      "272/272 [==============================] - 0s 19us/step - loss: 4.8942 - val_loss: 6.5607\n",
      "Epoch 929/1000\n",
      "272/272 [==============================] - ETA: 0s - loss: 4.626 - 0s 24us/step - loss: 4.8823 - val_loss: 6.5425\n",
      "Epoch 930/1000\n",
      "272/272 [==============================] - 0s 23us/step - loss: 4.8717 - val_loss: 6.5243\n",
      "Epoch 931/1000\n",
      "272/272 [==============================] - 0s 28us/step - loss: 4.8599 - val_loss: 6.5072\n",
      "Epoch 932/1000\n",
      "272/272 [==============================] - 0s 37us/step - loss: 4.8488 - val_loss: 6.4901\n",
      "Epoch 933/1000\n",
      "272/272 [==============================] - 0s 36us/step - loss: 4.8391 - val_loss: 6.4717\n",
      "Epoch 934/1000\n",
      "272/272 [==============================] - 0s 21us/step - loss: 4.8268 - val_loss: 6.4510\n",
      "Epoch 935/1000\n",
      "272/272 [==============================] - 0s 27us/step - loss: 4.8154 - val_loss: 6.4311\n",
      "Epoch 936/1000\n",
      "272/272 [==============================] - 0s 22us/step - loss: 4.8043 - val_loss: 6.4115\n",
      "Epoch 937/1000\n",
      "272/272 [==============================] - 0s 30us/step - loss: 4.7930 - val_loss: 6.3915\n",
      "Epoch 938/1000\n",
      "272/272 [==============================] - 0s 19us/step - loss: 4.7817 - val_loss: 6.3726\n",
      "Epoch 939/1000\n",
      "272/272 [==============================] - 0s 29us/step - loss: 4.7704 - val_loss: 6.3543\n",
      "Epoch 940/1000\n",
      "272/272 [==============================] - 0s 27us/step - loss: 4.7588 - val_loss: 6.3370\n",
      "Epoch 941/1000\n",
      "272/272 [==============================] - 0s 33us/step - loss: 4.7482 - val_loss: 6.3190\n",
      "Epoch 942/1000\n",
      "272/272 [==============================] - 0s 22us/step - loss: 4.7366 - val_loss: 6.3006\n",
      "Epoch 943/1000\n",
      "272/272 [==============================] - 0s 25us/step - loss: 4.7252 - val_loss: 6.2833\n",
      "Epoch 944/1000\n",
      "272/272 [==============================] - 0s 24us/step - loss: 4.7139 - val_loss: 6.2673\n",
      "Epoch 945/1000\n",
      "272/272 [==============================] - 0s 24us/step - loss: 4.7026 - val_loss: 6.2513\n",
      "Epoch 946/1000\n",
      "272/272 [==============================] - 0s 34us/step - loss: 4.6921 - val_loss: 6.2338\n",
      "Epoch 947/1000\n",
      "272/272 [==============================] - 0s 21us/step - loss: 4.6806 - val_loss: 6.2153\n",
      "Epoch 948/1000\n",
      "272/272 [==============================] - 0s 29us/step - loss: 4.6694 - val_loss: 6.1962\n",
      "Epoch 949/1000\n",
      "272/272 [==============================] - 0s 25us/step - loss: 4.6590 - val_loss: 6.1760\n",
      "Epoch 950/1000\n",
      "272/272 [==============================] - 0s 18us/step - loss: 4.6469 - val_loss: 6.1569\n",
      "Epoch 951/1000\n",
      "272/272 [==============================] - 0s 23us/step - loss: 4.6364 - val_loss: 6.1377\n",
      "Epoch 952/1000\n",
      "272/272 [==============================] - 0s 22us/step - loss: 4.6256 - val_loss: 6.1191\n",
      "Epoch 953/1000\n",
      "272/272 [==============================] - 0s 24us/step - loss: 4.6140 - val_loss: 6.1014\n",
      "Epoch 954/1000\n",
      "272/272 [==============================] - 0s 28us/step - loss: 4.6025 - val_loss: 6.0831\n",
      "Epoch 955/1000\n",
      "272/272 [==============================] - 0s 20us/step - loss: 4.5914 - val_loss: 6.0642\n",
      "Epoch 956/1000\n",
      "272/272 [==============================] - 0s 23us/step - loss: 4.5796 - val_loss: 6.0462\n",
      "Epoch 957/1000\n",
      "272/272 [==============================] - 0s 33us/step - loss: 4.5676 - val_loss: 6.0287\n",
      "Epoch 958/1000\n",
      "272/272 [==============================] - 0s 21us/step - loss: 4.5572 - val_loss: 6.0107\n",
      "Epoch 959/1000\n",
      "272/272 [==============================] - 0s 24us/step - loss: 4.5453 - val_loss: 5.9924\n",
      "Epoch 960/1000\n",
      "272/272 [==============================] - 0s 19us/step - loss: 4.5355 - val_loss: 5.9738\n",
      "Epoch 961/1000\n",
      "272/272 [==============================] - 0s 19us/step - loss: 4.5242 - val_loss: 5.9541\n",
      "Epoch 962/1000\n",
      "272/272 [==============================] - 0s 30us/step - loss: 4.5123 - val_loss: 5.9368\n",
      "Epoch 963/1000\n",
      "272/272 [==============================] - 0s 20us/step - loss: 4.5012 - val_loss: 5.9202\n",
      "Epoch 964/1000\n",
      "272/272 [==============================] - 0s 28us/step - loss: 4.4906 - val_loss: 5.9045\n",
      "Epoch 965/1000\n",
      "272/272 [==============================] - 0s 21us/step - loss: 4.4808 - val_loss: 5.8905\n",
      "Epoch 966/1000\n",
      "272/272 [==============================] - 0s 27us/step - loss: 4.4708 - val_loss: 5.8750\n",
      "Epoch 967/1000\n",
      "272/272 [==============================] - 0s 32us/step - loss: 4.4604 - val_loss: 5.8577\n",
      "Epoch 968/1000\n",
      "272/272 [==============================] - 0s 31us/step - loss: 4.4497 - val_loss: 5.8397\n",
      "Epoch 969/1000\n",
      "272/272 [==============================] - 0s 33us/step - loss: 4.4393 - val_loss: 5.8240\n",
      "Epoch 970/1000\n",
      "272/272 [==============================] - ETA: 0s - loss: 4.593 - 0s 22us/step - loss: 4.4295 - val_loss: 5.8092\n",
      "Epoch 971/1000\n",
      "272/272 [==============================] - 0s 23us/step - loss: 4.4195 - val_loss: 5.7941\n",
      "Epoch 972/1000\n",
      "272/272 [==============================] - 0s 24us/step - loss: 4.4101 - val_loss: 5.7783\n",
      "Epoch 973/1000\n",
      "272/272 [==============================] - 0s 25us/step - loss: 4.4014 - val_loss: 5.7621\n",
      "Epoch 974/1000\n",
      "272/272 [==============================] - 0s 28us/step - loss: 4.3918 - val_loss: 5.7455\n",
      "Epoch 975/1000\n",
      "272/272 [==============================] - 0s 27us/step - loss: 4.3824 - val_loss: 5.7286\n",
      "Epoch 976/1000\n",
      "272/272 [==============================] - 0s 28us/step - loss: 4.3729 - val_loss: 5.7121\n",
      "Epoch 977/1000\n",
      "272/272 [==============================] - 0s 22us/step - loss: 4.3635 - val_loss: 5.6948\n",
      "Epoch 978/1000\n",
      "272/272 [==============================] - 0s 33us/step - loss: 4.3538 - val_loss: 5.6773\n",
      "Epoch 979/1000\n",
      "272/272 [==============================] - 0s 28us/step - loss: 4.3445 - val_loss: 5.6595\n",
      "Epoch 980/1000\n",
      "272/272 [==============================] - 0s 29us/step - loss: 4.3361 - val_loss: 5.6424\n",
      "Epoch 981/1000\n",
      "272/272 [==============================] - 0s 21us/step - loss: 4.3269 - val_loss: 5.6253\n",
      "Epoch 982/1000\n",
      "272/272 [==============================] - 0s 29us/step - loss: 4.3175 - val_loss: 5.6087\n",
      "Epoch 983/1000\n",
      "272/272 [==============================] - 0s 28us/step - loss: 4.3075 - val_loss: 5.5933\n",
      "Epoch 984/1000\n",
      "272/272 [==============================] - 0s 36us/step - loss: 4.2966 - val_loss: 5.5784\n",
      "Epoch 985/1000\n",
      "272/272 [==============================] - 0s 26us/step - loss: 4.2862 - val_loss: 5.5628\n",
      "Epoch 986/1000\n",
      "272/272 [==============================] - 0s 23us/step - loss: 4.2772 - val_loss: 5.5466\n",
      "Epoch 987/1000\n",
      "272/272 [==============================] - 0s 25us/step - loss: 4.2678 - val_loss: 5.5312\n",
      "Epoch 988/1000\n",
      "272/272 [==============================] - 0s 33us/step - loss: 4.2589 - val_loss: 5.5147\n",
      "Epoch 989/1000\n",
      "272/272 [==============================] - 0s 24us/step - loss: 4.2500 - val_loss: 5.4978\n",
      "Epoch 990/1000\n",
      "272/272 [==============================] - 0s 21us/step - loss: 4.2413 - val_loss: 5.4790\n",
      "Epoch 991/1000\n",
      "272/272 [==============================] - 0s 26us/step - loss: 4.2320 - val_loss: 5.4603\n",
      "Epoch 992/1000\n",
      "272/272 [==============================] - 0s 21us/step - loss: 4.2229 - val_loss: 5.4414\n",
      "Epoch 993/1000\n",
      "272/272 [==============================] - 0s 26us/step - loss: 4.2133 - val_loss: 5.4233\n",
      "Epoch 994/1000\n",
      "272/272 [==============================] - 0s 26us/step - loss: 4.2041 - val_loss: 5.4044\n",
      "Epoch 995/1000\n",
      "272/272 [==============================] - 0s 29us/step - loss: 4.1955 - val_loss: 5.3852\n",
      "Epoch 996/1000\n",
      "272/272 [==============================] - 0s 20us/step - loss: 4.1852 - val_loss: 5.3663\n",
      "Epoch 997/1000\n",
      "272/272 [==============================] - 0s 27us/step - loss: 4.1763 - val_loss: 5.3462\n",
      "Epoch 998/1000\n",
      "272/272 [==============================] - 0s 22us/step - loss: 4.1677 - val_loss: 5.3258\n",
      "Epoch 999/1000\n",
      "272/272 [==============================] - ETA: 0s - loss: 4.059 - 0s 24us/step - loss: 4.1599 - val_loss: 5.3058\n",
      "Epoch 1000/1000\n",
      "272/272 [==============================] - 0s 33us/step - loss: 4.1513 - val_loss: 5.2867\n",
      "Train on 272 samples, validate on 31 samples\n",
      "Epoch 1/1000\n",
      "272/272 [==============================] - 0s 382us/step - loss: 21.9618 - val_loss: 23.3370\n",
      "Epoch 2/1000\n",
      "272/272 [==============================] - 0s 20us/step - loss: 21.9383 - val_loss: 23.3224\n",
      "Epoch 3/1000\n",
      "272/272 [==============================] - 0s 23us/step - loss: 21.9170 - val_loss: 23.3056\n",
      "Epoch 4/1000\n",
      "272/272 [==============================] - 0s 17us/step - loss: 21.8967 - val_loss: 23.2891\n",
      "Epoch 5/1000\n",
      "272/272 [==============================] - 0s 36us/step - loss: 21.8767 - val_loss: 23.2734\n",
      "Epoch 6/1000\n",
      "272/272 [==============================] - 0s 19us/step - loss: 21.8565 - val_loss: 23.2582\n",
      "Epoch 7/1000\n",
      "272/272 [==============================] - 0s 24us/step - loss: 21.8353 - val_loss: 23.2431\n",
      "Epoch 8/1000\n",
      "272/272 [==============================] - 0s 18us/step - loss: 21.8154 - val_loss: 23.2279\n",
      "Epoch 9/1000\n",
      "272/272 [==============================] - 0s 24us/step - loss: 21.7948 - val_loss: 23.2142\n",
      "Epoch 10/1000\n",
      "272/272 [==============================] - 0s 24us/step - loss: 21.7737 - val_loss: 23.1988\n",
      "Epoch 11/1000\n",
      "272/272 [==============================] - 0s 25us/step - loss: 21.7529 - val_loss: 23.1829\n",
      "Epoch 12/1000\n",
      "272/272 [==============================] - 0s 21us/step - loss: 21.7329 - val_loss: 23.1680\n",
      "Epoch 13/1000\n",
      "272/272 [==============================] - 0s 17us/step - loss: 21.7119 - val_loss: 23.1533\n",
      "Epoch 14/1000\n",
      "272/272 [==============================] - 0s 23us/step - loss: 21.6917 - val_loss: 23.1387\n",
      "Epoch 15/1000\n",
      "272/272 [==============================] - 0s 18us/step - loss: 21.6708 - val_loss: 23.1229\n",
      "Epoch 16/1000\n",
      "272/272 [==============================] - 0s 21us/step - loss: 21.6503 - val_loss: 23.1086\n",
      "Epoch 17/1000\n",
      "272/272 [==============================] - 0s 21us/step - loss: 21.6292 - val_loss: 23.0973\n",
      "Epoch 18/1000\n",
      "272/272 [==============================] - 0s 39us/step - loss: 21.6095 - val_loss: 23.0868\n",
      "Epoch 19/1000\n",
      "272/272 [==============================] - 0s 18us/step - loss: 21.5873 - val_loss: 23.0746\n",
      "Epoch 20/1000\n",
      "272/272 [==============================] - 0s 27us/step - loss: 21.5664 - val_loss: 23.0643\n",
      "Epoch 21/1000\n",
      "272/272 [==============================] - 0s 40us/step - loss: 21.5451 - val_loss: 23.0549\n",
      "Epoch 22/1000\n",
      "272/272 [==============================] - 0s 23us/step - loss: 21.5238 - val_loss: 23.0469\n",
      "Epoch 23/1000\n",
      "272/272 [==============================] - 0s 23us/step - loss: 21.5029 - val_loss: 23.0392\n",
      "Epoch 24/1000\n",
      "272/272 [==============================] - 0s 29us/step - loss: 21.4828 - val_loss: 23.0299\n",
      "Epoch 25/1000\n",
      "272/272 [==============================] - 0s 18us/step - loss: 21.4598 - val_loss: 23.0183\n",
      "Epoch 26/1000\n",
      "272/272 [==============================] - 0s 28us/step - loss: 21.4393 - val_loss: 23.0076\n",
      "Epoch 27/1000\n",
      "272/272 [==============================] - 0s 19us/step - loss: 21.4182 - val_loss: 22.9972\n",
      "Epoch 28/1000\n",
      "272/272 [==============================] - 0s 34us/step - loss: 21.3969 - val_loss: 22.9871\n",
      "Epoch 29/1000\n",
      "272/272 [==============================] - 0s 26us/step - loss: 21.3759 - val_loss: 22.9776\n",
      "Epoch 30/1000\n",
      "272/272 [==============================] - 0s 29us/step - loss: 21.3550 - val_loss: 22.9678\n",
      "Epoch 31/1000\n",
      "272/272 [==============================] - 0s 26us/step - loss: 21.3339 - val_loss: 22.9574\n",
      "Epoch 32/1000\n",
      "272/272 [==============================] - 0s 30us/step - loss: 21.3127 - val_loss: 22.9464\n",
      "Epoch 33/1000\n",
      "272/272 [==============================] - 0s 27us/step - loss: 21.2912 - val_loss: 22.9355\n",
      "Epoch 34/1000\n",
      "272/272 [==============================] - 0s 25us/step - loss: 21.2705 - val_loss: 22.9258\n",
      "Epoch 35/1000\n",
      "272/272 [==============================] - 0s 27us/step - loss: 21.2494 - val_loss: 22.9161\n",
      "Epoch 36/1000\n",
      "272/272 [==============================] - 0s 34us/step - loss: 21.2279 - val_loss: 22.9064\n",
      "Epoch 37/1000\n",
      "272/272 [==============================] - 0s 28us/step - loss: 21.2071 - val_loss: 22.8975\n",
      "Epoch 38/1000\n",
      "272/272 [==============================] - 0s 31us/step - loss: 21.1859 - val_loss: 22.8877\n",
      "Epoch 39/1000\n",
      "272/272 [==============================] - 0s 29us/step - loss: 21.1653 - val_loss: 22.8766\n",
      "Epoch 40/1000\n",
      "272/272 [==============================] - 0s 23us/step - loss: 21.1437 - val_loss: 22.8640\n",
      "Epoch 41/1000\n",
      "272/272 [==============================] - 0s 21us/step - loss: 21.1225 - val_loss: 22.8520\n",
      "Epoch 42/1000\n",
      "272/272 [==============================] - 0s 24us/step - loss: 21.1017 - val_loss: 22.8418\n",
      "Epoch 43/1000\n",
      "272/272 [==============================] - 0s 19us/step - loss: 21.0814 - val_loss: 22.8316\n",
      "Epoch 44/1000\n",
      "272/272 [==============================] - 0s 22us/step - loss: 21.0594 - val_loss: 22.8211\n",
      "Epoch 45/1000\n",
      "272/272 [==============================] - 0s 22us/step - loss: 21.0397 - val_loss: 22.8105\n",
      "Epoch 46/1000\n",
      "272/272 [==============================] - 0s 22us/step - loss: 21.0174 - val_loss: 22.7981\n",
      "Epoch 47/1000\n",
      "272/272 [==============================] - 0s 22us/step - loss: 20.9968 - val_loss: 22.7860\n",
      "Epoch 48/1000\n",
      "272/272 [==============================] - 0s 27us/step - loss: 20.9755 - val_loss: 22.7741\n",
      "Epoch 49/1000\n",
      "272/272 [==============================] - 0s 24us/step - loss: 20.9551 - val_loss: 22.7637\n",
      "Epoch 50/1000\n",
      "272/272 [==============================] - 0s 24us/step - loss: 20.9337 - val_loss: 22.7527\n",
      "Epoch 51/1000\n",
      "272/272 [==============================] - 0s 24us/step - loss: 20.9125 - val_loss: 22.7428\n",
      "Epoch 52/1000\n",
      "272/272 [==============================] - 0s 21us/step - loss: 20.8924 - val_loss: 22.7331\n",
      "Epoch 53/1000\n",
      "272/272 [==============================] - 0s 21us/step - loss: 20.8705 - val_loss: 22.7218\n",
      "Epoch 54/1000\n",
      "272/272 [==============================] - 0s 27us/step - loss: 20.8497 - val_loss: 22.7099\n",
      "Epoch 55/1000\n",
      "272/272 [==============================] - 0s 24us/step - loss: 20.8287 - val_loss: 22.6974\n",
      "Epoch 56/1000\n",
      "272/272 [==============================] - 0s 19us/step - loss: 20.8077 - val_loss: 22.6864\n",
      "Epoch 57/1000\n",
      "272/272 [==============================] - 0s 22us/step - loss: 20.7880 - val_loss: 22.6773\n",
      "Epoch 58/1000\n",
      "272/272 [==============================] - 0s 29us/step - loss: 20.7656 - val_loss: 22.6657\n",
      "Epoch 59/1000\n",
      "272/272 [==============================] - 0s 22us/step - loss: 20.7443 - val_loss: 22.6547\n",
      "Epoch 60/1000\n",
      "272/272 [==============================] - 0s 28us/step - loss: 20.7239 - val_loss: 22.6448\n",
      "Epoch 61/1000\n",
      "272/272 [==============================] - 0s 22us/step - loss: 20.7030 - val_loss: 22.6339\n",
      "Epoch 62/1000\n",
      "272/272 [==============================] - 0s 27us/step - loss: 20.6815 - val_loss: 22.6220\n",
      "Epoch 63/1000\n",
      "272/272 [==============================] - 0s 20us/step - loss: 20.6604 - val_loss: 22.6113\n",
      "Epoch 64/1000\n",
      "272/272 [==============================] - 0s 27us/step - loss: 20.6404 - val_loss: 22.6009\n",
      "Epoch 65/1000\n",
      "272/272 [==============================] - 0s 22us/step - loss: 20.6186 - val_loss: 22.5902\n",
      "Epoch 66/1000\n",
      "272/272 [==============================] - 0s 20us/step - loss: 20.5981 - val_loss: 22.5795\n",
      "Epoch 67/1000\n",
      "272/272 [==============================] - 0s 21us/step - loss: 20.5763 - val_loss: 22.5676\n",
      "Epoch 68/1000\n",
      "272/272 [==============================] - 0s 31us/step - loss: 20.5557 - val_loss: 22.5569\n",
      "Epoch 69/1000\n",
      "272/272 [==============================] - 0s 26us/step - loss: 20.5360 - val_loss: 22.5449\n",
      "Epoch 70/1000\n",
      "272/272 [==============================] - 0s 21us/step - loss: 20.5142 - val_loss: 22.5301\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 71/1000\n",
      "272/272 [==============================] - 0s 31us/step - loss: 20.4932 - val_loss: 22.5142\n",
      "Epoch 72/1000\n",
      "272/272 [==============================] - 0s 23us/step - loss: 20.4727 - val_loss: 22.4991\n",
      "Epoch 73/1000\n",
      "272/272 [==============================] - 0s 20us/step - loss: 20.4521 - val_loss: 22.4852\n",
      "Epoch 74/1000\n",
      "272/272 [==============================] - 0s 29us/step - loss: 20.4314 - val_loss: 22.4721\n",
      "Epoch 75/1000\n",
      "272/272 [==============================] - 0s 34us/step - loss: 20.4108 - val_loss: 22.4604\n",
      "Epoch 76/1000\n",
      "272/272 [==============================] - 0s 21us/step - loss: 20.3908 - val_loss: 22.4479\n",
      "Epoch 77/1000\n",
      "272/272 [==============================] - 0s 28us/step - loss: 20.3697 - val_loss: 22.4323\n",
      "Epoch 78/1000\n",
      "272/272 [==============================] - ETA: 0s - loss: 20.15 - 0s 25us/step - loss: 20.3486 - val_loss: 22.4154\n",
      "Epoch 79/1000\n",
      "272/272 [==============================] - 0s 30us/step - loss: 20.3282 - val_loss: 22.3999\n",
      "Epoch 80/1000\n",
      "272/272 [==============================] - 0s 30us/step - loss: 20.3077 - val_loss: 22.3861\n",
      "Epoch 81/1000\n",
      "272/272 [==============================] - 0s 31us/step - loss: 20.2870 - val_loss: 22.3732\n",
      "Epoch 82/1000\n",
      "272/272 [==============================] - 0s 28us/step - loss: 20.2663 - val_loss: 22.3606\n",
      "Epoch 83/1000\n",
      "272/272 [==============================] - 0s 23us/step - loss: 20.2458 - val_loss: 22.3476\n",
      "Epoch 84/1000\n",
      "272/272 [==============================] - 0s 27us/step - loss: 20.2248 - val_loss: 22.3336\n",
      "Epoch 85/1000\n",
      "272/272 [==============================] - ETA: 0s - loss: 20.45 - 0s 28us/step - loss: 20.2039 - val_loss: 22.3208\n",
      "Epoch 86/1000\n",
      "272/272 [==============================] - 0s 29us/step - loss: 20.1840 - val_loss: 22.3078\n",
      "Epoch 87/1000\n",
      "272/272 [==============================] - 0s 23us/step - loss: 20.1624 - val_loss: 22.2939\n",
      "Epoch 88/1000\n",
      "272/272 [==============================] - 0s 26us/step - loss: 20.1417 - val_loss: 22.2816\n",
      "Epoch 89/1000\n",
      "272/272 [==============================] - 0s 26us/step - loss: 20.1209 - val_loss: 22.2692\n",
      "Epoch 90/1000\n",
      "272/272 [==============================] - 0s 25us/step - loss: 20.0998 - val_loss: 22.2568\n",
      "Epoch 91/1000\n",
      "272/272 [==============================] - 0s 21us/step - loss: 20.0793 - val_loss: 22.2454\n",
      "Epoch 92/1000\n",
      "272/272 [==============================] - 0s 27us/step - loss: 20.0582 - val_loss: 22.2342\n",
      "Epoch 93/1000\n",
      "272/272 [==============================] - 0s 22us/step - loss: 20.0372 - val_loss: 22.2241\n",
      "Epoch 94/1000\n",
      "272/272 [==============================] - 0s 34us/step - loss: 20.0156 - val_loss: 22.2146\n",
      "Epoch 95/1000\n",
      "272/272 [==============================] - 0s 22us/step - loss: 19.9948 - val_loss: 22.2062\n",
      "Epoch 96/1000\n",
      "272/272 [==============================] - 0s 31us/step - loss: 19.9744 - val_loss: 22.1971\n",
      "Epoch 97/1000\n",
      "272/272 [==============================] - 0s 22us/step - loss: 19.9520 - val_loss: 22.1876\n",
      "Epoch 98/1000\n",
      "272/272 [==============================] - 0s 30us/step - loss: 19.9311 - val_loss: 22.1797\n",
      "Epoch 99/1000\n",
      "272/272 [==============================] - 0s 26us/step - loss: 19.9108 - val_loss: 22.1715\n",
      "Epoch 100/1000\n",
      "272/272 [==============================] - 0s 29us/step - loss: 19.8885 - val_loss: 22.1623\n",
      "Epoch 101/1000\n",
      "272/272 [==============================] - 0s 34us/step - loss: 19.8675 - val_loss: 22.1534\n",
      "Epoch 102/1000\n",
      "272/272 [==============================] - 0s 31us/step - loss: 19.8461 - val_loss: 22.1450\n",
      "Epoch 103/1000\n",
      "272/272 [==============================] - 0s 23us/step - loss: 19.8255 - val_loss: 22.1363\n",
      "Epoch 104/1000\n",
      "272/272 [==============================] - 0s 28us/step - loss: 19.8039 - val_loss: 22.1261\n",
      "Epoch 105/1000\n",
      "272/272 [==============================] - 0s 23us/step - loss: 19.7833 - val_loss: 22.1154\n",
      "Epoch 106/1000\n",
      "272/272 [==============================] - 0s 22us/step - loss: 19.7618 - val_loss: 22.1040\n",
      "Epoch 107/1000\n",
      "272/272 [==============================] - 0s 27us/step - loss: 19.7408 - val_loss: 22.0929\n",
      "Epoch 108/1000\n",
      "272/272 [==============================] - 0s 34us/step - loss: 19.7198 - val_loss: 22.0820\n",
      "Epoch 109/1000\n",
      "272/272 [==============================] - 0s 27us/step - loss: 19.6993 - val_loss: 22.0703\n",
      "Epoch 110/1000\n",
      "272/272 [==============================] - 0s 27us/step - loss: 19.6786 - val_loss: 22.0569\n",
      "Epoch 111/1000\n",
      "272/272 [==============================] - 0s 27us/step - loss: 19.6578 - val_loss: 22.0431\n",
      "Epoch 112/1000\n",
      "272/272 [==============================] - 0s 38us/step - loss: 19.6371 - val_loss: 22.0285\n",
      "Epoch 113/1000\n",
      "272/272 [==============================] - 0s 25us/step - loss: 19.6160 - val_loss: 22.0135\n",
      "Epoch 114/1000\n",
      "272/272 [==============================] - 0s 21us/step - loss: 19.5957 - val_loss: 22.0002\n",
      "Epoch 115/1000\n",
      "272/272 [==============================] - 0s 28us/step - loss: 19.5745 - val_loss: 21.9888\n",
      "Epoch 116/1000\n",
      "272/272 [==============================] - 0s 26us/step - loss: 19.5542 - val_loss: 21.9786\n",
      "Epoch 117/1000\n",
      "272/272 [==============================] - 0s 28us/step - loss: 19.5326 - val_loss: 21.9675\n",
      "Epoch 118/1000\n",
      "272/272 [==============================] - 0s 27us/step - loss: 19.5119 - val_loss: 21.9564\n",
      "Epoch 119/1000\n",
      "272/272 [==============================] - 0s 23us/step - loss: 19.4907 - val_loss: 21.9449\n",
      "Epoch 120/1000\n",
      "272/272 [==============================] - 0s 35us/step - loss: 19.4709 - val_loss: 21.9328\n",
      "Epoch 121/1000\n",
      "272/272 [==============================] - 0s 29us/step - loss: 19.4493 - val_loss: 21.9188\n",
      "Epoch 122/1000\n",
      "272/272 [==============================] - 0s 25us/step - loss: 19.4285 - val_loss: 21.9062\n",
      "Epoch 123/1000\n",
      "272/272 [==============================] - 0s 30us/step - loss: 19.4073 - val_loss: 21.8960\n",
      "Epoch 124/1000\n",
      "272/272 [==============================] - 0s 21us/step - loss: 19.3872 - val_loss: 21.8859\n",
      "Epoch 125/1000\n",
      "272/272 [==============================] - 0s 24us/step - loss: 19.3655 - val_loss: 21.8744\n",
      "Epoch 126/1000\n",
      "272/272 [==============================] - 0s 25us/step - loss: 19.3448 - val_loss: 21.8635\n",
      "Epoch 127/1000\n",
      "272/272 [==============================] - 0s 45us/step - loss: 19.3237 - val_loss: 21.8519\n",
      "Epoch 128/1000\n",
      "272/272 [==============================] - 0s 30us/step - loss: 19.3029 - val_loss: 21.8404\n",
      "Epoch 129/1000\n",
      "272/272 [==============================] - 0s 17us/step - loss: 19.2820 - val_loss: 21.8286\n",
      "Epoch 130/1000\n",
      "272/272 [==============================] - 0s 21us/step - loss: 19.2617 - val_loss: 21.8164\n",
      "Epoch 131/1000\n",
      "272/272 [==============================] - 0s 18us/step - loss: 19.2406 - val_loss: 21.8035\n",
      "Epoch 132/1000\n",
      "272/272 [==============================] - 0s 23us/step - loss: 19.2202 - val_loss: 21.7917\n",
      "Epoch 133/1000\n",
      "272/272 [==============================] - 0s 27us/step - loss: 19.1991 - val_loss: 21.7810\n",
      "Epoch 134/1000\n",
      "272/272 [==============================] - 0s 34us/step - loss: 19.1794 - val_loss: 21.7728\n",
      "Epoch 135/1000\n",
      "272/272 [==============================] - 0s 35us/step - loss: 19.1582 - val_loss: 21.7627\n",
      "Epoch 136/1000\n",
      "272/272 [==============================] - 0s 29us/step - loss: 19.1374 - val_loss: 21.7515\n",
      "Epoch 137/1000\n",
      "272/272 [==============================] - 0s 29us/step - loss: 19.1171 - val_loss: 21.7404\n",
      "Epoch 138/1000\n",
      "272/272 [==============================] - 0s 23us/step - loss: 19.0976 - val_loss: 21.7283\n",
      "Epoch 139/1000\n",
      "272/272 [==============================] - 0s 20us/step - loss: 19.0762 - val_loss: 21.7139\n",
      "Epoch 140/1000\n",
      "272/272 [==============================] - ETA: 0s - loss: 18.18 - 0s 30us/step - loss: 19.0562 - val_loss: 21.6989\n",
      "Epoch 141/1000\n",
      "272/272 [==============================] - 0s 36us/step - loss: 19.0360 - val_loss: 21.6825\n",
      "Epoch 142/1000\n",
      "272/272 [==============================] - 0s 33us/step - loss: 19.0158 - val_loss: 21.6669\n",
      "Epoch 143/1000\n",
      "272/272 [==============================] - 0s 28us/step - loss: 18.9956 - val_loss: 21.6514\n",
      "Epoch 144/1000\n",
      "272/272 [==============================] - 0s 30us/step - loss: 18.9757 - val_loss: 21.6357\n",
      "Epoch 145/1000\n",
      "272/272 [==============================] - 0s 27us/step - loss: 18.9559 - val_loss: 21.6199\n",
      "Epoch 146/1000\n",
      "272/272 [==============================] - 0s 27us/step - loss: 18.9357 - val_loss: 21.6032\n",
      "Epoch 147/1000\n",
      "272/272 [==============================] - 0s 23us/step - loss: 18.9156 - val_loss: 21.5870\n",
      "Epoch 148/1000\n",
      "272/272 [==============================] - 0s 25us/step - loss: 18.8968 - val_loss: 21.5704\n",
      "Epoch 149/1000\n",
      "272/272 [==============================] - 0s 29us/step - loss: 18.8762 - val_loss: 21.5513\n",
      "Epoch 150/1000\n",
      "272/272 [==============================] - 0s 39us/step - loss: 18.8563 - val_loss: 21.5337\n",
      "Epoch 151/1000\n",
      "272/272 [==============================] - 0s 20us/step - loss: 18.8363 - val_loss: 21.5169\n",
      "Epoch 152/1000\n",
      "272/272 [==============================] - 0s 26us/step - loss: 18.8168 - val_loss: 21.4998\n",
      "Epoch 153/1000\n",
      "272/272 [==============================] - 0s 24us/step - loss: 18.7967 - val_loss: 21.4818\n",
      "Epoch 154/1000\n",
      "272/272 [==============================] - 0s 21us/step - loss: 18.7769 - val_loss: 21.4636\n",
      "Epoch 155/1000\n",
      "272/272 [==============================] - ETA: 0s - loss: 17.87 - 0s 24us/step - loss: 18.7568 - val_loss: 21.4444\n",
      "Epoch 156/1000\n",
      "272/272 [==============================] - 0s 29us/step - loss: 18.7373 - val_loss: 21.4231\n",
      "Epoch 157/1000\n",
      "272/272 [==============================] - ETA: 0s - loss: 18.64 - 0s 24us/step - loss: 18.7171 - val_loss: 21.4040\n",
      "Epoch 158/1000\n",
      "272/272 [==============================] - 0s 29us/step - loss: 18.6969 - val_loss: 21.3848\n",
      "Epoch 159/1000\n",
      "272/272 [==============================] - 0s 27us/step - loss: 18.6770 - val_loss: 21.3651\n",
      "Epoch 160/1000\n",
      "272/272 [==============================] - 0s 29us/step - loss: 18.6578 - val_loss: 21.3463\n",
      "Epoch 161/1000\n",
      "272/272 [==============================] - 0s 29us/step - loss: 18.6375 - val_loss: 21.3305\n",
      "Epoch 162/1000\n",
      "272/272 [==============================] - 0s 26us/step - loss: 18.6176 - val_loss: 21.3142\n",
      "Epoch 163/1000\n",
      "272/272 [==============================] - 0s 31us/step - loss: 18.5978 - val_loss: 21.2977\n",
      "Epoch 164/1000\n",
      "272/272 [==============================] - ETA: 0s - loss: 17.73 - 0s 20us/step - loss: 18.5781 - val_loss: 21.2800\n",
      "Epoch 165/1000\n",
      "272/272 [==============================] - 0s 23us/step - loss: 18.5584 - val_loss: 21.2609\n",
      "Epoch 166/1000\n",
      "272/272 [==============================] - 0s 22us/step - loss: 18.5383 - val_loss: 21.2423\n",
      "Epoch 167/1000\n",
      "272/272 [==============================] - 0s 27us/step - loss: 18.5185 - val_loss: 21.2237\n",
      "Epoch 168/1000\n",
      "272/272 [==============================] - 0s 27us/step - loss: 18.4985 - val_loss: 21.2046\n",
      "Epoch 169/1000\n",
      "272/272 [==============================] - 0s 46us/step - loss: 18.4790 - val_loss: 21.1866\n",
      "Epoch 170/1000\n",
      "272/272 [==============================] - 0s 35us/step - loss: 18.4588 - val_loss: 21.1704\n",
      "Epoch 171/1000\n",
      "272/272 [==============================] - 0s 49us/step - loss: 18.4393 - val_loss: 21.1539\n",
      "Epoch 172/1000\n",
      "272/272 [==============================] - 0s 37us/step - loss: 18.4189 - val_loss: 21.1362\n",
      "Epoch 173/1000\n",
      "272/272 [==============================] - 0s 27us/step - loss: 18.3989 - val_loss: 21.1196\n",
      "Epoch 174/1000\n",
      "272/272 [==============================] - 0s 33us/step - loss: 18.3793 - val_loss: 21.1026\n",
      "Epoch 175/1000\n",
      "272/272 [==============================] - 0s 26us/step - loss: 18.3590 - val_loss: 21.0852\n",
      "Epoch 176/1000\n",
      "272/272 [==============================] - 0s 30us/step - loss: 18.3390 - val_loss: 21.0682\n",
      "Epoch 177/1000\n",
      "272/272 [==============================] - 0s 22us/step - loss: 18.3191 - val_loss: 21.0523\n",
      "Epoch 178/1000\n",
      "272/272 [==============================] - 0s 26us/step - loss: 18.2993 - val_loss: 21.0359\n",
      "Epoch 179/1000\n",
      "272/272 [==============================] - 0s 22us/step - loss: 18.2792 - val_loss: 21.0191\n",
      "Epoch 180/1000\n",
      "272/272 [==============================] - 0s 31us/step - loss: 18.2591 - val_loss: 21.0013\n",
      "Epoch 181/1000\n",
      "272/272 [==============================] - 0s 27us/step - loss: 18.2392 - val_loss: 20.9835\n",
      "Epoch 182/1000\n",
      "272/272 [==============================] - 0s 20us/step - loss: 18.2197 - val_loss: 20.9664\n",
      "Epoch 183/1000\n",
      "272/272 [==============================] - 0s 36us/step - loss: 18.1992 - val_loss: 20.9508\n",
      "Epoch 184/1000\n",
      "272/272 [==============================] - 0s 22us/step - loss: 18.1796 - val_loss: 20.9356\n",
      "Epoch 185/1000\n",
      "272/272 [==============================] - 0s 25us/step - loss: 18.1601 - val_loss: 20.9205\n",
      "Epoch 186/1000\n",
      "272/272 [==============================] - 0s 23us/step - loss: 18.1404 - val_loss: 20.9057\n",
      "Epoch 187/1000\n",
      "272/272 [==============================] - 0s 29us/step - loss: 18.1209 - val_loss: 20.8911\n",
      "Epoch 188/1000\n",
      "272/272 [==============================] - 0s 22us/step - loss: 18.1012 - val_loss: 20.8774\n",
      "Epoch 189/1000\n",
      "272/272 [==============================] - 0s 31us/step - loss: 18.0814 - val_loss: 20.8630\n",
      "Epoch 190/1000\n",
      "272/272 [==============================] - 0s 22us/step - loss: 18.0618 - val_loss: 20.8475\n",
      "Epoch 191/1000\n",
      "272/272 [==============================] - 0s 26us/step - loss: 18.0419 - val_loss: 20.8318\n",
      "Epoch 192/1000\n",
      "272/272 [==============================] - 0s 33us/step - loss: 18.0224 - val_loss: 20.8178\n",
      "Epoch 193/1000\n",
      "272/272 [==============================] - 0s 24us/step - loss: 18.0025 - val_loss: 20.8020\n",
      "Epoch 194/1000\n",
      "272/272 [==============================] - 0s 23us/step - loss: 17.9828 - val_loss: 20.7855\n",
      "Epoch 195/1000\n",
      "272/272 [==============================] - 0s 24us/step - loss: 17.9630 - val_loss: 20.7669\n",
      "Epoch 196/1000\n",
      "272/272 [==============================] - 0s 25us/step - loss: 17.9430 - val_loss: 20.7465\n",
      "Epoch 197/1000\n",
      "272/272 [==============================] - 0s 21us/step - loss: 17.9226 - val_loss: 20.7265\n",
      "Epoch 198/1000\n",
      "272/272 [==============================] - 0s 31us/step - loss: 17.9030 - val_loss: 20.7080\n",
      "Epoch 199/1000\n",
      "272/272 [==============================] - 0s 25us/step - loss: 17.8829 - val_loss: 20.6909\n",
      "Epoch 200/1000\n",
      "272/272 [==============================] - 0s 31us/step - loss: 17.8629 - val_loss: 20.6720\n",
      "Epoch 201/1000\n",
      "272/272 [==============================] - 0s 26us/step - loss: 17.8434 - val_loss: 20.6543\n",
      "Epoch 202/1000\n",
      "272/272 [==============================] - 0s 24us/step - loss: 17.8233 - val_loss: 20.6397\n",
      "Epoch 203/1000\n",
      "272/272 [==============================] - 0s 20us/step - loss: 17.8035 - val_loss: 20.6252\n",
      "Epoch 204/1000\n",
      "272/272 [==============================] - 0s 35us/step - loss: 17.7841 - val_loss: 20.6083\n",
      "Epoch 205/1000\n",
      "272/272 [==============================] - 0s 17us/step - loss: 17.7642 - val_loss: 20.5876\n",
      "Epoch 206/1000\n",
      "272/272 [==============================] - 0s 30us/step - loss: 17.7441 - val_loss: 20.5655\n",
      "Epoch 207/1000\n",
      "272/272 [==============================] - 0s 19us/step - loss: 17.7241 - val_loss: 20.5422\n",
      "Epoch 208/1000\n",
      "272/272 [==============================] - 0s 36us/step - loss: 17.7050 - val_loss: 20.5225\n",
      "Epoch 209/1000\n",
      "272/272 [==============================] - 0s 22us/step - loss: 17.6840 - val_loss: 20.5058\n",
      "Epoch 210/1000\n",
      "272/272 [==============================] - 0s 34us/step - loss: 17.6637 - val_loss: 20.4868\n",
      "Epoch 211/1000\n",
      "272/272 [==============================] - 0s 24us/step - loss: 17.6437 - val_loss: 20.4652\n",
      "Epoch 212/1000\n",
      "272/272 [==============================] - 0s 27us/step - loss: 17.6235 - val_loss: 20.4428\n",
      "Epoch 213/1000\n",
      "272/272 [==============================] - 0s 26us/step - loss: 17.6031 - val_loss: 20.4208\n",
      "Epoch 214/1000\n",
      "272/272 [==============================] - 0s 24us/step - loss: 17.5836 - val_loss: 20.4001\n",
      "Epoch 215/1000\n",
      "272/272 [==============================] - 0s 28us/step - loss: 17.5627 - val_loss: 20.3804\n",
      "Epoch 216/1000\n",
      "272/272 [==============================] - 0s 25us/step - loss: 17.5443 - val_loss: 20.3608\n",
      "Epoch 217/1000\n",
      "272/272 [==============================] - 0s 40us/step - loss: 17.5228 - val_loss: 20.3428\n",
      "Epoch 218/1000\n",
      "272/272 [==============================] - 0s 53us/step - loss: 17.5033 - val_loss: 20.3236\n",
      "Epoch 219/1000\n",
      "272/272 [==============================] - 0s 23us/step - loss: 17.4831 - val_loss: 20.3073\n",
      "Epoch 220/1000\n",
      "272/272 [==============================] - 0s 31us/step - loss: 17.4623 - val_loss: 20.2922\n",
      "Epoch 221/1000\n",
      "272/272 [==============================] - 0s 26us/step - loss: 17.4423 - val_loss: 20.2788\n",
      "Epoch 222/1000\n",
      "272/272 [==============================] - 0s 27us/step - loss: 17.4235 - val_loss: 20.2667\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 223/1000\n",
      "272/272 [==============================] - 0s 28us/step - loss: 17.4027 - val_loss: 20.2521\n",
      "Epoch 224/1000\n",
      "272/272 [==============================] - 0s 22us/step - loss: 17.3825 - val_loss: 20.2380\n",
      "Epoch 225/1000\n",
      "272/272 [==============================] - 0s 26us/step - loss: 17.3623 - val_loss: 20.2234\n",
      "Epoch 226/1000\n",
      "272/272 [==============================] - 0s 20us/step - loss: 17.3427 - val_loss: 20.2078\n",
      "Epoch 227/1000\n",
      "272/272 [==============================] - 0s 18us/step - loss: 17.3227 - val_loss: 20.1922\n",
      "Epoch 228/1000\n",
      "272/272 [==============================] - 0s 26us/step - loss: 17.3026 - val_loss: 20.1744\n",
      "Epoch 229/1000\n",
      "272/272 [==============================] - 0s 22us/step - loss: 17.2825 - val_loss: 20.1563\n",
      "Epoch 230/1000\n",
      "272/272 [==============================] - 0s 23us/step - loss: 17.2623 - val_loss: 20.1376\n",
      "Epoch 231/1000\n",
      "272/272 [==============================] - 0s 20us/step - loss: 17.2425 - val_loss: 20.1187\n",
      "Epoch 232/1000\n",
      "272/272 [==============================] - 0s 22us/step - loss: 17.2220 - val_loss: 20.0998\n",
      "Epoch 233/1000\n",
      "272/272 [==============================] - 0s 29us/step - loss: 17.2023 - val_loss: 20.0806\n",
      "Epoch 234/1000\n",
      "272/272 [==============================] - 0s 30us/step - loss: 17.1824 - val_loss: 20.0602\n",
      "Epoch 235/1000\n",
      "272/272 [==============================] - 0s 29us/step - loss: 17.1618 - val_loss: 20.0405\n",
      "Epoch 236/1000\n",
      "272/272 [==============================] - 0s 24us/step - loss: 17.1418 - val_loss: 20.0201\n",
      "Epoch 237/1000\n",
      "272/272 [==============================] - 0s 27us/step - loss: 17.1217 - val_loss: 19.9998\n",
      "Epoch 238/1000\n",
      "272/272 [==============================] - 0s 29us/step - loss: 17.1030 - val_loss: 19.9808\n",
      "Epoch 239/1000\n",
      "272/272 [==============================] - 0s 20us/step - loss: 17.0819 - val_loss: 19.9639\n",
      "Epoch 240/1000\n",
      "272/272 [==============================] - 0s 24us/step - loss: 17.0617 - val_loss: 19.9460\n",
      "Epoch 241/1000\n",
      "272/272 [==============================] - 0s 20us/step - loss: 17.0417 - val_loss: 19.9273\n",
      "Epoch 242/1000\n",
      "272/272 [==============================] - 0s 20us/step - loss: 17.0214 - val_loss: 19.9086\n",
      "Epoch 243/1000\n",
      "272/272 [==============================] - 0s 28us/step - loss: 17.0016 - val_loss: 19.8911\n",
      "Epoch 244/1000\n",
      "272/272 [==============================] - 0s 25us/step - loss: 16.9811 - val_loss: 19.8745\n",
      "Epoch 245/1000\n",
      "272/272 [==============================] - 0s 30us/step - loss: 16.9607 - val_loss: 19.8569\n",
      "Epoch 246/1000\n",
      "272/272 [==============================] - 0s 20us/step - loss: 16.9408 - val_loss: 19.8393\n",
      "Epoch 247/1000\n",
      "272/272 [==============================] - 0s 30us/step - loss: 16.9205 - val_loss: 19.8223\n",
      "Epoch 248/1000\n",
      "272/272 [==============================] - 0s 22us/step - loss: 16.9000 - val_loss: 19.8053\n",
      "Epoch 249/1000\n",
      "272/272 [==============================] - 0s 33us/step - loss: 16.8799 - val_loss: 19.7885\n",
      "Epoch 250/1000\n",
      "272/272 [==============================] - 0s 20us/step - loss: 16.8600 - val_loss: 19.7721\n",
      "Epoch 251/1000\n",
      "272/272 [==============================] - 0s 27us/step - loss: 16.8392 - val_loss: 19.7559\n",
      "Epoch 252/1000\n",
      "272/272 [==============================] - 0s 27us/step - loss: 16.8193 - val_loss: 19.7396\n",
      "Epoch 253/1000\n",
      "272/272 [==============================] - 0s 25us/step - loss: 16.7990 - val_loss: 19.7228\n",
      "Epoch 254/1000\n",
      "272/272 [==============================] - 0s 22us/step - loss: 16.7788 - val_loss: 19.7055\n",
      "Epoch 255/1000\n",
      "272/272 [==============================] - 0s 33us/step - loss: 16.7587 - val_loss: 19.6872\n",
      "Epoch 256/1000\n",
      "272/272 [==============================] - 0s 27us/step - loss: 16.7384 - val_loss: 19.6685\n",
      "Epoch 257/1000\n",
      "272/272 [==============================] - 0s 22us/step - loss: 16.7183 - val_loss: 19.6504\n",
      "Epoch 258/1000\n",
      "272/272 [==============================] - 0s 21us/step - loss: 16.6984 - val_loss: 19.6342\n",
      "Epoch 259/1000\n",
      "272/272 [==============================] - 0s 32us/step - loss: 16.6779 - val_loss: 19.6180\n",
      "Epoch 260/1000\n",
      "272/272 [==============================] - 0s 27us/step - loss: 16.6578 - val_loss: 19.6028\n",
      "Epoch 261/1000\n",
      "272/272 [==============================] - 0s 22us/step - loss: 16.6379 - val_loss: 19.5876\n",
      "Epoch 262/1000\n",
      "272/272 [==============================] - 0s 24us/step - loss: 16.6173 - val_loss: 19.5705\n",
      "Epoch 263/1000\n",
      "272/272 [==============================] - 0s 27us/step - loss: 16.5973 - val_loss: 19.5531\n",
      "Epoch 264/1000\n",
      "272/272 [==============================] - 0s 26us/step - loss: 16.5771 - val_loss: 19.5372\n",
      "Epoch 265/1000\n",
      "272/272 [==============================] - 0s 27us/step - loss: 16.5572 - val_loss: 19.5215\n",
      "Epoch 266/1000\n",
      "272/272 [==============================] - 0s 25us/step - loss: 16.5370 - val_loss: 19.5045\n",
      "Epoch 267/1000\n",
      "272/272 [==============================] - 0s 22us/step - loss: 16.5170 - val_loss: 19.4854\n",
      "Epoch 268/1000\n",
      "272/272 [==============================] - 0s 27us/step - loss: 16.4968 - val_loss: 19.4665\n",
      "Epoch 269/1000\n",
      "272/272 [==============================] - 0s 22us/step - loss: 16.4768 - val_loss: 19.4481\n",
      "Epoch 270/1000\n",
      "272/272 [==============================] - 0s 33us/step - loss: 16.4567 - val_loss: 19.4295\n",
      "Epoch 271/1000\n",
      "272/272 [==============================] - 0s 23us/step - loss: 16.4367 - val_loss: 19.4126\n",
      "Epoch 272/1000\n",
      "272/272 [==============================] - 0s 34us/step - loss: 16.4168 - val_loss: 19.3962\n",
      "Epoch 273/1000\n",
      "272/272 [==============================] - 0s 19us/step - loss: 16.3965 - val_loss: 19.3803\n",
      "Epoch 274/1000\n",
      "272/272 [==============================] - 0s 26us/step - loss: 16.3763 - val_loss: 19.3638\n",
      "Epoch 275/1000\n",
      "272/272 [==============================] - 0s 23us/step - loss: 16.3563 - val_loss: 19.3465\n",
      "Epoch 276/1000\n",
      "272/272 [==============================] - 0s 31us/step - loss: 16.3359 - val_loss: 19.3289\n",
      "Epoch 277/1000\n",
      "272/272 [==============================] - 0s 24us/step - loss: 16.3160 - val_loss: 19.3119\n",
      "Epoch 278/1000\n",
      "272/272 [==============================] - 0s 33us/step - loss: 16.2954 - val_loss: 19.2934\n",
      "Epoch 279/1000\n",
      "272/272 [==============================] - 0s 28us/step - loss: 16.2757 - val_loss: 19.2735\n",
      "Epoch 280/1000\n",
      "272/272 [==============================] - 0s 40us/step - loss: 16.2553 - val_loss: 19.2539\n",
      "Epoch 281/1000\n",
      "272/272 [==============================] - 0s 35us/step - loss: 16.2358 - val_loss: 19.2345\n",
      "Epoch 282/1000\n",
      "272/272 [==============================] - 0s 30us/step - loss: 16.2156 - val_loss: 19.2165\n",
      "Epoch 283/1000\n",
      "272/272 [==============================] - 0s 24us/step - loss: 16.1955 - val_loss: 19.1998\n",
      "Epoch 284/1000\n",
      "272/272 [==============================] - 0s 28us/step - loss: 16.1754 - val_loss: 19.1844\n",
      "Epoch 285/1000\n",
      "272/272 [==============================] - 0s 27us/step - loss: 16.1549 - val_loss: 19.1716\n",
      "Epoch 286/1000\n",
      "272/272 [==============================] - 0s 20us/step - loss: 16.1348 - val_loss: 19.1594\n",
      "Epoch 287/1000\n",
      "272/272 [==============================] - 0s 28us/step - loss: 16.1157 - val_loss: 19.1466\n",
      "Epoch 288/1000\n",
      "272/272 [==============================] - 0s 24us/step - loss: 16.0953 - val_loss: 19.1314\n",
      "Epoch 289/1000\n",
      "272/272 [==============================] - 0s 25us/step - loss: 16.0748 - val_loss: 19.1169\n",
      "Epoch 290/1000\n",
      "272/272 [==============================] - 0s 26us/step - loss: 16.0551 - val_loss: 19.1021\n",
      "Epoch 291/1000\n",
      "272/272 [==============================] - 0s 23us/step - loss: 16.0351 - val_loss: 19.0862\n",
      "Epoch 292/1000\n",
      "272/272 [==============================] - 0s 30us/step - loss: 16.0150 - val_loss: 19.0704\n",
      "Epoch 293/1000\n",
      "272/272 [==============================] - 0s 27us/step - loss: 15.9951 - val_loss: 19.0541\n",
      "Epoch 294/1000\n",
      "272/272 [==============================] - 0s 27us/step - loss: 15.9753 - val_loss: 19.0374\n",
      "Epoch 295/1000\n",
      "272/272 [==============================] - 0s 27us/step - loss: 15.9550 - val_loss: 19.0195\n",
      "Epoch 296/1000\n",
      "272/272 [==============================] - 0s 23us/step - loss: 15.9350 - val_loss: 19.0020\n",
      "Epoch 297/1000\n",
      "272/272 [==============================] - 0s 26us/step - loss: 15.9152 - val_loss: 18.9860\n",
      "Epoch 298/1000\n",
      "272/272 [==============================] - 0s 22us/step - loss: 15.8949 - val_loss: 18.9693\n",
      "Epoch 299/1000\n",
      "272/272 [==============================] - 0s 23us/step - loss: 15.8754 - val_loss: 18.9523\n",
      "Epoch 300/1000\n",
      "272/272 [==============================] - 0s 29us/step - loss: 15.8550 - val_loss: 18.9353\n",
      "Epoch 301/1000\n",
      "272/272 [==============================] - 0s 27us/step - loss: 15.8349 - val_loss: 18.9171\n",
      "Epoch 302/1000\n",
      "272/272 [==============================] - 0s 28us/step - loss: 15.8152 - val_loss: 18.8990\n",
      "Epoch 303/1000\n",
      "272/272 [==============================] - 0s 31us/step - loss: 15.7952 - val_loss: 18.8816\n",
      "Epoch 304/1000\n",
      "272/272 [==============================] - 0s 19us/step - loss: 15.7752 - val_loss: 18.8637\n",
      "Epoch 305/1000\n",
      "272/272 [==============================] - 0s 20us/step - loss: 15.7552 - val_loss: 18.8457\n",
      "Epoch 306/1000\n",
      "272/272 [==============================] - 0s 28us/step - loss: 15.7351 - val_loss: 18.8272\n",
      "Epoch 307/1000\n",
      "272/272 [==============================] - 0s 27us/step - loss: 15.7150 - val_loss: 18.8078\n",
      "Epoch 308/1000\n",
      "272/272 [==============================] - 0s 25us/step - loss: 15.6954 - val_loss: 18.7892\n",
      "Epoch 309/1000\n",
      "272/272 [==============================] - 0s 24us/step - loss: 15.6750 - val_loss: 18.7715\n",
      "Epoch 310/1000\n",
      "272/272 [==============================] - 0s 25us/step - loss: 15.6548 - val_loss: 18.7548\n",
      "Epoch 311/1000\n",
      "272/272 [==============================] - 0s 26us/step - loss: 15.6347 - val_loss: 18.7379\n",
      "Epoch 312/1000\n",
      "272/272 [==============================] - 0s 25us/step - loss: 15.6147 - val_loss: 18.7198\n",
      "Epoch 313/1000\n",
      "272/272 [==============================] - 0s 22us/step - loss: 15.5947 - val_loss: 18.7013\n",
      "Epoch 314/1000\n",
      "272/272 [==============================] - 0s 25us/step - loss: 15.5743 - val_loss: 18.6831\n",
      "Epoch 315/1000\n",
      "272/272 [==============================] - 0s 25us/step - loss: 15.5545 - val_loss: 18.6648\n",
      "Epoch 316/1000\n",
      "272/272 [==============================] - 0s 21us/step - loss: 15.5344 - val_loss: 18.6473\n",
      "Epoch 317/1000\n",
      "272/272 [==============================] - 0s 21us/step - loss: 15.5143 - val_loss: 18.6296\n",
      "Epoch 318/1000\n",
      "272/272 [==============================] - 0s 23us/step - loss: 15.4936 - val_loss: 18.6112\n",
      "Epoch 319/1000\n",
      "272/272 [==============================] - 0s 48us/step - loss: 15.4744 - val_loss: 18.5938\n",
      "Epoch 320/1000\n",
      "272/272 [==============================] - 0s 25us/step - loss: 15.4537 - val_loss: 18.5778\n",
      "Epoch 321/1000\n",
      "272/272 [==============================] - 0s 24us/step - loss: 15.4340 - val_loss: 18.5632\n",
      "Epoch 322/1000\n",
      "272/272 [==============================] - 0s 23us/step - loss: 15.4136 - val_loss: 18.5489\n",
      "Epoch 323/1000\n",
      "272/272 [==============================] - 0s 27us/step - loss: 15.3939 - val_loss: 18.5338\n",
      "Epoch 324/1000\n",
      "272/272 [==============================] - 0s 22us/step - loss: 15.3734 - val_loss: 18.5164\n",
      "Epoch 325/1000\n",
      "272/272 [==============================] - 0s 23us/step - loss: 15.3539 - val_loss: 18.4973\n",
      "Epoch 326/1000\n",
      "272/272 [==============================] - 0s 24us/step - loss: 15.3328 - val_loss: 18.4773\n",
      "Epoch 327/1000\n",
      "272/272 [==============================] - 0s 22us/step - loss: 15.3149 - val_loss: 18.4570\n",
      "Epoch 328/1000\n",
      "272/272 [==============================] - 0s 26us/step - loss: 15.2933 - val_loss: 18.4405\n",
      "Epoch 329/1000\n",
      "272/272 [==============================] - 0s 28us/step - loss: 15.2728 - val_loss: 18.4264\n",
      "Epoch 330/1000\n",
      "272/272 [==============================] - 0s 23us/step - loss: 15.2534 - val_loss: 18.4116\n",
      "Epoch 331/1000\n",
      "272/272 [==============================] - 0s 26us/step - loss: 15.2335 - val_loss: 18.3951\n",
      "Epoch 332/1000\n",
      "272/272 [==============================] - ETA: 0s - loss: 14.86 - 0s 30us/step - loss: 15.2139 - val_loss: 18.3790\n",
      "Epoch 333/1000\n",
      "272/272 [==============================] - 0s 29us/step - loss: 15.1942 - val_loss: 18.3637\n",
      "Epoch 334/1000\n",
      "272/272 [==============================] - 0s 26us/step - loss: 15.1741 - val_loss: 18.3505\n",
      "Epoch 335/1000\n",
      "272/272 [==============================] - 0s 22us/step - loss: 15.1549 - val_loss: 18.3378\n",
      "Epoch 336/1000\n",
      "272/272 [==============================] - 0s 24us/step - loss: 15.1365 - val_loss: 18.3242\n",
      "Epoch 337/1000\n",
      "272/272 [==============================] - 0s 23us/step - loss: 15.1172 - val_loss: 18.3078\n",
      "Epoch 338/1000\n",
      "272/272 [==============================] - 0s 28us/step - loss: 15.0977 - val_loss: 18.2901\n",
      "Epoch 339/1000\n",
      "272/272 [==============================] - 0s 27us/step - loss: 15.0780 - val_loss: 18.2719\n",
      "Epoch 340/1000\n",
      "272/272 [==============================] - 0s 29us/step - loss: 15.0578 - val_loss: 18.2527\n",
      "Epoch 341/1000\n",
      "272/272 [==============================] - 0s 26us/step - loss: 15.0378 - val_loss: 18.2316\n",
      "Epoch 342/1000\n",
      "272/272 [==============================] - 0s 30us/step - loss: 15.0173 - val_loss: 18.2109\n",
      "Epoch 343/1000\n",
      "272/272 [==============================] - 0s 53us/step - loss: 14.9963 - val_loss: 18.1907\n",
      "Epoch 344/1000\n",
      "272/272 [==============================] - 0s 28us/step - loss: 14.9761 - val_loss: 18.1714\n",
      "Epoch 345/1000\n",
      "272/272 [==============================] - 0s 28us/step - loss: 14.9560 - val_loss: 18.1521\n",
      "Epoch 346/1000\n",
      "272/272 [==============================] - 0s 22us/step - loss: 14.9354 - val_loss: 18.1322\n",
      "Epoch 347/1000\n",
      "272/272 [==============================] - 0s 31us/step - loss: 14.9156 - val_loss: 18.1125\n",
      "Epoch 348/1000\n",
      "272/272 [==============================] - 0s 38us/step - loss: 14.8947 - val_loss: 18.0940\n",
      "Epoch 349/1000\n",
      "272/272 [==============================] - 0s 24us/step - loss: 14.8750 - val_loss: 18.0755\n",
      "Epoch 350/1000\n",
      "272/272 [==============================] - 0s 21us/step - loss: 14.8545 - val_loss: 18.0571\n",
      "Epoch 351/1000\n",
      "272/272 [==============================] - 0s 39us/step - loss: 14.8342 - val_loss: 18.0394\n",
      "Epoch 352/1000\n",
      "272/272 [==============================] - 0s 32us/step - loss: 14.8141 - val_loss: 18.0216\n",
      "Epoch 353/1000\n",
      "272/272 [==============================] - 0s 40us/step - loss: 14.7938 - val_loss: 18.0044\n",
      "Epoch 354/1000\n",
      "272/272 [==============================] - 0s 29us/step - loss: 14.7740 - val_loss: 17.9876\n",
      "Epoch 355/1000\n",
      "272/272 [==============================] - 0s 30us/step - loss: 14.7540 - val_loss: 17.9702\n",
      "Epoch 356/1000\n",
      "272/272 [==============================] - 0s 36us/step - loss: 14.7342 - val_loss: 17.9529\n",
      "Epoch 357/1000\n",
      "272/272 [==============================] - 0s 27us/step - loss: 14.7138 - val_loss: 17.9326\n",
      "Epoch 358/1000\n",
      "272/272 [==============================] - 0s 30us/step - loss: 14.6940 - val_loss: 17.9109\n",
      "Epoch 359/1000\n",
      "272/272 [==============================] - 0s 19us/step - loss: 14.6735 - val_loss: 17.8904\n",
      "Epoch 360/1000\n",
      "272/272 [==============================] - 0s 34us/step - loss: 14.6538 - val_loss: 17.8698\n",
      "Epoch 361/1000\n",
      "272/272 [==============================] - 0s 25us/step - loss: 14.6336 - val_loss: 17.8494\n",
      "Epoch 362/1000\n",
      "272/272 [==============================] - 0s 25us/step - loss: 14.6138 - val_loss: 17.8304\n",
      "Epoch 363/1000\n",
      "272/272 [==============================] - 0s 30us/step - loss: 14.5938 - val_loss: 17.8131\n",
      "Epoch 364/1000\n",
      "272/272 [==============================] - 0s 21us/step - loss: 14.5737 - val_loss: 17.7980\n",
      "Epoch 365/1000\n",
      "272/272 [==============================] - 0s 27us/step - loss: 14.5536 - val_loss: 17.7848\n",
      "Epoch 366/1000\n",
      "272/272 [==============================] - 0s 27us/step - loss: 14.5336 - val_loss: 17.7709\n",
      "Epoch 367/1000\n",
      "272/272 [==============================] - 0s 31us/step - loss: 14.5134 - val_loss: 17.7564\n",
      "Epoch 368/1000\n",
      "272/272 [==============================] - 0s 27us/step - loss: 14.4934 - val_loss: 17.7422\n",
      "Epoch 369/1000\n",
      "272/272 [==============================] - 0s 30us/step - loss: 14.4737 - val_loss: 17.7274\n",
      "Epoch 370/1000\n",
      "272/272 [==============================] - 0s 30us/step - loss: 14.4534 - val_loss: 17.7137\n",
      "Epoch 371/1000\n",
      "272/272 [==============================] - 0s 27us/step - loss: 14.4341 - val_loss: 17.7001\n",
      "Epoch 372/1000\n",
      "272/272 [==============================] - 0s 25us/step - loss: 14.4140 - val_loss: 17.6842\n",
      "Epoch 373/1000\n",
      "272/272 [==============================] - 0s 22us/step - loss: 14.3938 - val_loss: 17.6690\n",
      "Epoch 374/1000\n",
      "272/272 [==============================] - 0s 31us/step - loss: 14.3739 - val_loss: 17.6549\n",
      "Epoch 375/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "272/272 [==============================] - 0s 23us/step - loss: 14.3544 - val_loss: 17.6406\n",
      "Epoch 376/1000\n",
      "272/272 [==============================] - 0s 18us/step - loss: 14.3345 - val_loss: 17.6256\n",
      "Epoch 377/1000\n",
      "272/272 [==============================] - 0s 19us/step - loss: 14.3148 - val_loss: 17.6119\n",
      "Epoch 378/1000\n",
      "272/272 [==============================] - 0s 24us/step - loss: 14.2952 - val_loss: 17.5971\n",
      "Epoch 379/1000\n",
      "272/272 [==============================] - 0s 23us/step - loss: 14.2762 - val_loss: 17.5823\n",
      "Epoch 380/1000\n",
      "272/272 [==============================] - 0s 23us/step - loss: 14.2562 - val_loss: 17.5646\n",
      "Epoch 381/1000\n",
      "272/272 [==============================] - 0s 21us/step - loss: 14.2361 - val_loss: 17.5451\n",
      "Epoch 382/1000\n",
      "272/272 [==============================] - 0s 31us/step - loss: 14.2171 - val_loss: 17.5262\n",
      "Epoch 383/1000\n",
      "272/272 [==============================] - 0s 30us/step - loss: 14.1960 - val_loss: 17.5094\n",
      "Epoch 384/1000\n",
      "272/272 [==============================] - 0s 38us/step - loss: 14.1762 - val_loss: 17.4915\n",
      "Epoch 385/1000\n",
      "272/272 [==============================] - 0s 35us/step - loss: 14.1562 - val_loss: 17.4752\n",
      "Epoch 386/1000\n",
      "272/272 [==============================] - 0s 19us/step - loss: 14.1358 - val_loss: 17.4614\n",
      "Epoch 387/1000\n",
      "272/272 [==============================] - 0s 29us/step - loss: 14.1164 - val_loss: 17.4471\n",
      "Epoch 388/1000\n",
      "272/272 [==============================] - 0s 27us/step - loss: 14.0964 - val_loss: 17.4307\n",
      "Epoch 389/1000\n",
      "272/272 [==============================] - 0s 31us/step - loss: 14.0764 - val_loss: 17.4142\n",
      "Epoch 390/1000\n",
      "272/272 [==============================] - 0s 29us/step - loss: 14.0564 - val_loss: 17.3963\n",
      "Epoch 391/1000\n",
      "272/272 [==============================] - 0s 33us/step - loss: 14.0365 - val_loss: 17.3778\n",
      "Epoch 392/1000\n",
      "272/272 [==============================] - 0s 28us/step - loss: 14.0169 - val_loss: 17.3593\n",
      "Epoch 393/1000\n",
      "272/272 [==============================] - 0s 22us/step - loss: 13.9971 - val_loss: 17.3413\n",
      "Epoch 394/1000\n",
      "272/272 [==============================] - 0s 31us/step - loss: 13.9774 - val_loss: 17.3221\n",
      "Epoch 395/1000\n",
      "272/272 [==============================] - 0s 34us/step - loss: 13.9569 - val_loss: 17.3002\n",
      "Epoch 396/1000\n",
      "272/272 [==============================] - 0s 31us/step - loss: 13.9369 - val_loss: 17.2771\n",
      "Epoch 397/1000\n",
      "272/272 [==============================] - 0s 28us/step - loss: 13.9165 - val_loss: 17.2546\n",
      "Epoch 398/1000\n",
      "272/272 [==============================] - 0s 22us/step - loss: 13.8957 - val_loss: 17.2319\n",
      "Epoch 399/1000\n",
      "272/272 [==============================] - 0s 28us/step - loss: 13.8764 - val_loss: 17.2097\n",
      "Epoch 400/1000\n",
      "272/272 [==============================] - ETA: 0s - loss: 14.25 - 0s 21us/step - loss: 13.8563 - val_loss: 17.1895\n",
      "Epoch 401/1000\n",
      "272/272 [==============================] - 0s 24us/step - loss: 13.8362 - val_loss: 17.1705\n",
      "Epoch 402/1000\n",
      "272/272 [==============================] - 0s 24us/step - loss: 13.8157 - val_loss: 17.1525\n",
      "Epoch 403/1000\n",
      "272/272 [==============================] - 0s 24us/step - loss: 13.7960 - val_loss: 17.1346\n",
      "Epoch 404/1000\n",
      "272/272 [==============================] - 0s 21us/step - loss: 13.7760 - val_loss: 17.1160\n",
      "Epoch 405/1000\n",
      "272/272 [==============================] - 0s 24us/step - loss: 13.7556 - val_loss: 17.0982\n",
      "Epoch 406/1000\n",
      "272/272 [==============================] - 0s 26us/step - loss: 13.7354 - val_loss: 17.0801\n",
      "Epoch 407/1000\n",
      "272/272 [==============================] - 0s 25us/step - loss: 13.7155 - val_loss: 17.0635\n",
      "Epoch 408/1000\n",
      "272/272 [==============================] - 0s 27us/step - loss: 13.6953 - val_loss: 17.0468\n",
      "Epoch 409/1000\n",
      "272/272 [==============================] - 0s 28us/step - loss: 13.6747 - val_loss: 17.0310\n",
      "Epoch 410/1000\n",
      "272/272 [==============================] - 0s 27us/step - loss: 13.6545 - val_loss: 17.0159\n",
      "Epoch 411/1000\n",
      "272/272 [==============================] - 0s 32us/step - loss: 13.6346 - val_loss: 17.0013\n",
      "Epoch 412/1000\n",
      "272/272 [==============================] - 0s 28us/step - loss: 13.6145 - val_loss: 16.9872\n",
      "Epoch 413/1000\n",
      "272/272 [==============================] - 0s 29us/step - loss: 13.5937 - val_loss: 16.9735\n",
      "Epoch 414/1000\n",
      "272/272 [==============================] - 0s 24us/step - loss: 13.5739 - val_loss: 16.9597\n",
      "Epoch 415/1000\n",
      "272/272 [==============================] - 0s 23us/step - loss: 13.5534 - val_loss: 16.9451\n",
      "Epoch 416/1000\n",
      "272/272 [==============================] - 0s 20us/step - loss: 13.5331 - val_loss: 16.9303\n",
      "Epoch 417/1000\n",
      "272/272 [==============================] - 0s 26us/step - loss: 13.5131 - val_loss: 16.9138\n",
      "Epoch 418/1000\n",
      "272/272 [==============================] - 0s 24us/step - loss: 13.4929 - val_loss: 16.8958\n",
      "Epoch 419/1000\n",
      "272/272 [==============================] - 0s 27us/step - loss: 13.4727 - val_loss: 16.8783\n",
      "Epoch 420/1000\n",
      "272/272 [==============================] - 0s 24us/step - loss: 13.4529 - val_loss: 16.8623\n",
      "Epoch 421/1000\n",
      "272/272 [==============================] - 0s 37us/step - loss: 13.4327 - val_loss: 16.8464\n",
      "Epoch 422/1000\n",
      "272/272 [==============================] - 0s 31us/step - loss: 13.4123 - val_loss: 16.8286\n",
      "Epoch 423/1000\n",
      "272/272 [==============================] - 0s 33us/step - loss: 13.3919 - val_loss: 16.8105\n",
      "Epoch 424/1000\n",
      "272/272 [==============================] - 0s 37us/step - loss: 13.3719 - val_loss: 16.7929\n",
      "Epoch 425/1000\n",
      "272/272 [==============================] - 0s 23us/step - loss: 13.3520 - val_loss: 16.7751\n",
      "Epoch 426/1000\n",
      "272/272 [==============================] - 0s 20us/step - loss: 13.3319 - val_loss: 16.7572\n",
      "Epoch 427/1000\n",
      "272/272 [==============================] - 0s 31us/step - loss: 13.3115 - val_loss: 16.7409\n",
      "Epoch 428/1000\n",
      "272/272 [==============================] - 0s 26us/step - loss: 13.2911 - val_loss: 16.7250\n",
      "Epoch 429/1000\n",
      "272/272 [==============================] - 0s 41us/step - loss: 13.2709 - val_loss: 16.7090\n",
      "Epoch 430/1000\n",
      "272/272 [==============================] - 0s 26us/step - loss: 13.2510 - val_loss: 16.6936\n",
      "Epoch 431/1000\n",
      "272/272 [==============================] - 0s 40us/step - loss: 13.2317 - val_loss: 16.6791\n",
      "Epoch 432/1000\n",
      "272/272 [==============================] - 0s 27us/step - loss: 13.2106 - val_loss: 16.6667\n",
      "Epoch 433/1000\n",
      "272/272 [==============================] - 0s 24us/step - loss: 13.1905 - val_loss: 16.6556\n",
      "Epoch 434/1000\n",
      "272/272 [==============================] - 0s 21us/step - loss: 13.1704 - val_loss: 16.6453\n",
      "Epoch 435/1000\n",
      "272/272 [==============================] - 0s 28us/step - loss: 13.1505 - val_loss: 16.6344\n",
      "Epoch 436/1000\n",
      "272/272 [==============================] - ETA: 0s - loss: 13.20 - 0s 23us/step - loss: 13.1305 - val_loss: 16.6225\n",
      "Epoch 437/1000\n",
      "272/272 [==============================] - 0s 28us/step - loss: 13.1105 - val_loss: 16.6101\n",
      "Epoch 438/1000\n",
      "272/272 [==============================] - 0s 28us/step - loss: 13.0909 - val_loss: 16.5989\n",
      "Epoch 439/1000\n",
      "272/272 [==============================] - 0s 28us/step - loss: 13.0714 - val_loss: 16.5863\n",
      "Epoch 440/1000\n",
      "272/272 [==============================] - 0s 33us/step - loss: 13.0517 - val_loss: 16.5719\n",
      "Epoch 441/1000\n",
      "272/272 [==============================] - 0s 27us/step - loss: 13.0319 - val_loss: 16.5557\n",
      "Epoch 442/1000\n",
      "272/272 [==============================] - 0s 35us/step - loss: 13.0121 - val_loss: 16.5375\n",
      "Epoch 443/1000\n",
      "272/272 [==============================] - 0s 27us/step - loss: 12.9926 - val_loss: 16.5183\n",
      "Epoch 444/1000\n",
      "272/272 [==============================] - 0s 32us/step - loss: 12.9722 - val_loss: 16.5017\n",
      "Epoch 445/1000\n",
      "272/272 [==============================] - 0s 29us/step - loss: 12.9520 - val_loss: 16.4872\n",
      "Epoch 446/1000\n",
      "272/272 [==============================] - 0s 24us/step - loss: 12.9320 - val_loss: 16.4729\n",
      "Epoch 447/1000\n",
      "272/272 [==============================] - 0s 33us/step - loss: 12.9123 - val_loss: 16.4581\n",
      "Epoch 448/1000\n",
      "272/272 [==============================] - 0s 25us/step - loss: 12.8923 - val_loss: 16.4405\n",
      "Epoch 449/1000\n",
      "272/272 [==============================] - 0s 35us/step - loss: 12.8721 - val_loss: 16.4206\n",
      "Epoch 450/1000\n",
      "272/272 [==============================] - 0s 44us/step - loss: 12.8531 - val_loss: 16.4007\n",
      "Epoch 451/1000\n",
      "272/272 [==============================] - 0s 24us/step - loss: 12.8322 - val_loss: 16.3828\n",
      "Epoch 452/1000\n",
      "272/272 [==============================] - 0s 37us/step - loss: 12.8125 - val_loss: 16.3659\n",
      "Epoch 453/1000\n",
      "272/272 [==============================] - 0s 33us/step - loss: 12.7926 - val_loss: 16.3498\n",
      "Epoch 454/1000\n",
      "272/272 [==============================] - 0s 26us/step - loss: 12.7727 - val_loss: 16.3328\n",
      "Epoch 455/1000\n",
      "272/272 [==============================] - 0s 35us/step - loss: 12.7533 - val_loss: 16.3156\n",
      "Epoch 456/1000\n",
      "272/272 [==============================] - 0s 36us/step - loss: 12.7330 - val_loss: 16.3005\n",
      "Epoch 457/1000\n",
      "272/272 [==============================] - 0s 27us/step - loss: 12.7131 - val_loss: 16.2878\n",
      "Epoch 458/1000\n",
      "272/272 [==============================] - 0s 26us/step - loss: 12.6935 - val_loss: 16.2766\n",
      "Epoch 459/1000\n",
      "272/272 [==============================] - 0s 36us/step - loss: 12.6737 - val_loss: 16.2637\n",
      "Epoch 460/1000\n",
      "272/272 [==============================] - 0s 31us/step - loss: 12.6537 - val_loss: 16.2513\n",
      "Epoch 461/1000\n",
      "272/272 [==============================] - 0s 31us/step - loss: 12.6349 - val_loss: 16.2379\n",
      "Epoch 462/1000\n",
      "272/272 [==============================] - 0s 27us/step - loss: 12.6148 - val_loss: 16.2222\n",
      "Epoch 463/1000\n",
      "272/272 [==============================] - 0s 37us/step - loss: 12.5949 - val_loss: 16.2067\n",
      "Epoch 464/1000\n",
      "272/272 [==============================] - 0s 37us/step - loss: 12.5751 - val_loss: 16.1924\n",
      "Epoch 465/1000\n",
      "272/272 [==============================] - 0s 31us/step - loss: 12.5550 - val_loss: 16.1781\n",
      "Epoch 466/1000\n",
      "272/272 [==============================] - 0s 29us/step - loss: 12.5353 - val_loss: 16.1646\n",
      "Epoch 467/1000\n",
      "272/272 [==============================] - 0s 32us/step - loss: 12.5155 - val_loss: 16.1519\n",
      "Epoch 468/1000\n",
      "272/272 [==============================] - 0s 27us/step - loss: 12.4955 - val_loss: 16.1365\n",
      "Epoch 469/1000\n",
      "272/272 [==============================] - 0s 40us/step - loss: 12.4753 - val_loss: 16.1187\n",
      "Epoch 470/1000\n",
      "272/272 [==============================] - 0s 26us/step - loss: 12.4542 - val_loss: 16.0998\n",
      "Epoch 471/1000\n",
      "272/272 [==============================] - 0s 31us/step - loss: 12.4339 - val_loss: 16.0806\n",
      "Epoch 472/1000\n",
      "272/272 [==============================] - 0s 30us/step - loss: 12.4143 - val_loss: 16.0600\n",
      "Epoch 473/1000\n",
      "272/272 [==============================] - 0s 28us/step - loss: 12.3937 - val_loss: 16.0412\n",
      "Epoch 474/1000\n",
      "272/272 [==============================] - 0s 24us/step - loss: 12.3741 - val_loss: 16.0237\n",
      "Epoch 475/1000\n",
      "272/272 [==============================] - 0s 21us/step - loss: 12.3529 - val_loss: 16.0082\n",
      "Epoch 476/1000\n",
      "272/272 [==============================] - 0s 43us/step - loss: 12.3334 - val_loss: 15.9937\n",
      "Epoch 477/1000\n",
      "272/272 [==============================] - 0s 28us/step - loss: 12.3134 - val_loss: 15.9795\n",
      "Epoch 478/1000\n",
      "272/272 [==============================] - 0s 22us/step - loss: 12.2935 - val_loss: 15.9642\n",
      "Epoch 479/1000\n",
      "272/272 [==============================] - 0s 29us/step - loss: 12.2734 - val_loss: 15.9477\n",
      "Epoch 480/1000\n",
      "272/272 [==============================] - 0s 26us/step - loss: 12.2538 - val_loss: 15.9308\n",
      "Epoch 481/1000\n",
      "272/272 [==============================] - 0s 27us/step - loss: 12.2337 - val_loss: 15.9152\n",
      "Epoch 482/1000\n",
      "272/272 [==============================] - 0s 25us/step - loss: 12.2137 - val_loss: 15.8986\n",
      "Epoch 483/1000\n",
      "272/272 [==============================] - 0s 26us/step - loss: 12.1947 - val_loss: 15.8811\n",
      "Epoch 484/1000\n",
      "272/272 [==============================] - 0s 29us/step - loss: 12.1742 - val_loss: 15.8639\n",
      "Epoch 485/1000\n",
      "272/272 [==============================] - 0s 29us/step - loss: 12.1542 - val_loss: 15.8465\n",
      "Epoch 486/1000\n",
      "272/272 [==============================] - ETA: 0s - loss: 11.94 - 0s 31us/step - loss: 12.1349 - val_loss: 15.8290\n",
      "Epoch 487/1000\n",
      "272/272 [==============================] - 0s 32us/step - loss: 12.1154 - val_loss: 15.8107\n",
      "Epoch 488/1000\n",
      "272/272 [==============================] - 0s 24us/step - loss: 12.0955 - val_loss: 15.7937\n",
      "Epoch 489/1000\n",
      "272/272 [==============================] - 0s 31us/step - loss: 12.0764 - val_loss: 15.7770\n",
      "Epoch 490/1000\n",
      "272/272 [==============================] - 0s 30us/step - loss: 12.0563 - val_loss: 15.7609\n",
      "Epoch 491/1000\n",
      "272/272 [==============================] - 0s 30us/step - loss: 12.0364 - val_loss: 15.7436\n",
      "Epoch 492/1000\n",
      "272/272 [==============================] - 0s 32us/step - loss: 12.0170 - val_loss: 15.7266\n",
      "Epoch 493/1000\n",
      "272/272 [==============================] - 0s 41us/step - loss: 11.9971 - val_loss: 15.7093\n",
      "Epoch 494/1000\n",
      "272/272 [==============================] - ETA: 0s - loss: 11.48 - 0s 29us/step - loss: 11.9773 - val_loss: 15.6901\n",
      "Epoch 495/1000\n",
      "272/272 [==============================] - 0s 30us/step - loss: 11.9583 - val_loss: 15.6707\n",
      "Epoch 496/1000\n",
      "272/272 [==============================] - 0s 26us/step - loss: 11.9382 - val_loss: 15.6492\n",
      "Epoch 497/1000\n",
      "272/272 [==============================] - 0s 30us/step - loss: 11.9193 - val_loss: 15.6287\n",
      "Epoch 498/1000\n",
      "272/272 [==============================] - 0s 24us/step - loss: 11.9001 - val_loss: 15.6099\n",
      "Epoch 499/1000\n",
      "272/272 [==============================] - 0s 26us/step - loss: 11.8806 - val_loss: 15.5937\n",
      "Epoch 500/1000\n",
      "272/272 [==============================] - 0s 30us/step - loss: 11.8603 - val_loss: 15.5797\n",
      "Epoch 501/1000\n",
      "272/272 [==============================] - 0s 24us/step - loss: 11.8402 - val_loss: 15.5683\n",
      "Epoch 502/1000\n",
      "272/272 [==============================] - 0s 34us/step - loss: 11.8195 - val_loss: 15.5566\n",
      "Epoch 503/1000\n",
      "272/272 [==============================] - 0s 31us/step - loss: 11.7989 - val_loss: 15.5447\n",
      "Epoch 504/1000\n",
      "272/272 [==============================] - 0s 23us/step - loss: 11.7790 - val_loss: 15.5320\n",
      "Epoch 505/1000\n",
      "272/272 [==============================] - 0s 32us/step - loss: 11.7582 - val_loss: 15.5176\n",
      "Epoch 506/1000\n",
      "272/272 [==============================] - 0s 29us/step - loss: 11.7381 - val_loss: 15.5036\n",
      "Epoch 507/1000\n",
      "272/272 [==============================] - 0s 31us/step - loss: 11.7178 - val_loss: 15.4903\n",
      "Epoch 508/1000\n",
      "272/272 [==============================] - 0s 27us/step - loss: 11.6976 - val_loss: 15.4787\n",
      "Epoch 509/1000\n",
      "272/272 [==============================] - 0s 34us/step - loss: 11.6780 - val_loss: 15.4658\n",
      "Epoch 510/1000\n",
      "272/272 [==============================] - 0s 38us/step - loss: 11.6583 - val_loss: 15.4509\n",
      "Epoch 511/1000\n",
      "272/272 [==============================] - 0s 34us/step - loss: 11.6385 - val_loss: 15.4346\n",
      "Epoch 512/1000\n",
      "272/272 [==============================] - 0s 33us/step - loss: 11.6191 - val_loss: 15.4192\n",
      "Epoch 513/1000\n",
      "272/272 [==============================] - 0s 32us/step - loss: 11.5993 - val_loss: 15.4043\n",
      "Epoch 514/1000\n",
      "272/272 [==============================] - 0s 23us/step - loss: 11.5799 - val_loss: 15.3891\n",
      "Epoch 515/1000\n",
      "272/272 [==============================] - 0s 30us/step - loss: 11.5601 - val_loss: 15.3767\n",
      "Epoch 516/1000\n",
      "272/272 [==============================] - 0s 32us/step - loss: 11.5410 - val_loss: 15.3625\n",
      "Epoch 517/1000\n",
      "272/272 [==============================] - 0s 28us/step - loss: 11.5216 - val_loss: 15.3460\n",
      "Epoch 518/1000\n",
      "272/272 [==============================] - 0s 29us/step - loss: 11.5019 - val_loss: 15.3271\n",
      "Epoch 519/1000\n",
      "272/272 [==============================] - 0s 24us/step - loss: 11.4825 - val_loss: 15.3074\n",
      "Epoch 520/1000\n",
      "272/272 [==============================] - 0s 31us/step - loss: 11.4628 - val_loss: 15.2880\n",
      "Epoch 521/1000\n",
      "272/272 [==============================] - 0s 31us/step - loss: 11.4433 - val_loss: 15.2686\n",
      "Epoch 522/1000\n",
      "272/272 [==============================] - 0s 31us/step - loss: 11.4237 - val_loss: 15.2494\n",
      "Epoch 523/1000\n",
      "272/272 [==============================] - 0s 31us/step - loss: 11.4046 - val_loss: 15.2294\n",
      "Epoch 524/1000\n",
      "272/272 [==============================] - 0s 32us/step - loss: 11.3844 - val_loss: 15.2107\n",
      "Epoch 525/1000\n",
      "272/272 [==============================] - 0s 32us/step - loss: 11.3647 - val_loss: 15.1918\n",
      "Epoch 526/1000\n",
      "272/272 [==============================] - 0s 28us/step - loss: 11.3451 - val_loss: 15.1723\n",
      "Epoch 527/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "272/272 [==============================] - 0s 28us/step - loss: 11.3256 - val_loss: 15.1539\n",
      "Epoch 528/1000\n",
      "272/272 [==============================] - 0s 31us/step - loss: 11.3063 - val_loss: 15.1374\n",
      "Epoch 529/1000\n",
      "272/272 [==============================] - 0s 37us/step - loss: 11.2867 - val_loss: 15.1205\n",
      "Epoch 530/1000\n",
      "272/272 [==============================] - 0s 32us/step - loss: 11.2670 - val_loss: 15.1033\n",
      "Epoch 531/1000\n",
      "272/272 [==============================] - 0s 23us/step - loss: 11.2472 - val_loss: 15.0851\n",
      "Epoch 532/1000\n",
      "272/272 [==============================] - 0s 32us/step - loss: 11.2277 - val_loss: 15.0662\n",
      "Epoch 533/1000\n",
      "272/272 [==============================] - 0s 23us/step - loss: 11.2084 - val_loss: 15.0465\n",
      "Epoch 534/1000\n",
      "272/272 [==============================] - 0s 26us/step - loss: 11.1887 - val_loss: 15.0260\n",
      "Epoch 535/1000\n",
      "272/272 [==============================] - 0s 23us/step - loss: 11.1692 - val_loss: 15.0050\n",
      "Epoch 536/1000\n",
      "272/272 [==============================] - 0s 30us/step - loss: 11.1500 - val_loss: 14.9817\n",
      "Epoch 537/1000\n",
      "272/272 [==============================] - 0s 26us/step - loss: 11.1306 - val_loss: 14.9593\n",
      "Epoch 538/1000\n",
      "272/272 [==============================] - 0s 23us/step - loss: 11.1117 - val_loss: 14.9368\n",
      "Epoch 539/1000\n",
      "272/272 [==============================] - 0s 27us/step - loss: 11.0932 - val_loss: 14.9166\n",
      "Epoch 540/1000\n",
      "272/272 [==============================] - 0s 44us/step - loss: 11.0739 - val_loss: 14.8980\n",
      "Epoch 541/1000\n",
      "272/272 [==============================] - 0s 38us/step - loss: 11.0552 - val_loss: 14.8807\n",
      "Epoch 542/1000\n",
      "272/272 [==============================] - 0s 29us/step - loss: 11.0348 - val_loss: 14.8650\n",
      "Epoch 543/1000\n",
      "272/272 [==============================] - 0s 38us/step - loss: 11.0151 - val_loss: 14.8493\n",
      "Epoch 544/1000\n",
      "272/272 [==============================] - 0s 27us/step - loss: 10.9952 - val_loss: 14.8333\n",
      "Epoch 545/1000\n",
      "272/272 [==============================] - 0s 27us/step - loss: 10.9755 - val_loss: 14.8176\n",
      "Epoch 546/1000\n",
      "272/272 [==============================] - 0s 25us/step - loss: 10.9556 - val_loss: 14.8041\n",
      "Epoch 547/1000\n",
      "272/272 [==============================] - 0s 41us/step - loss: 10.9351 - val_loss: 14.7909\n",
      "Epoch 548/1000\n",
      "272/272 [==============================] - 0s 19us/step - loss: 10.9158 - val_loss: 14.7779\n",
      "Epoch 549/1000\n",
      "272/272 [==============================] - 0s 20us/step - loss: 10.8965 - val_loss: 14.7632\n",
      "Epoch 550/1000\n",
      "272/272 [==============================] - 0s 21us/step - loss: 10.8768 - val_loss: 14.7470\n",
      "Epoch 551/1000\n",
      "272/272 [==============================] - 0s 28us/step - loss: 10.8574 - val_loss: 14.7296\n",
      "Epoch 552/1000\n",
      "272/272 [==============================] - 0s 32us/step - loss: 10.8386 - val_loss: 14.7123\n",
      "Epoch 553/1000\n",
      "272/272 [==============================] - 0s 25us/step - loss: 10.8190 - val_loss: 14.6948\n",
      "Epoch 554/1000\n",
      "272/272 [==============================] - 0s 31us/step - loss: 10.7998 - val_loss: 14.6773\n",
      "Epoch 555/1000\n",
      "272/272 [==============================] - 0s 24us/step - loss: 10.7805 - val_loss: 14.6590\n",
      "Epoch 556/1000\n",
      "272/272 [==============================] - 0s 27us/step - loss: 10.7613 - val_loss: 14.6394\n",
      "Epoch 557/1000\n",
      "272/272 [==============================] - 0s 24us/step - loss: 10.7418 - val_loss: 14.6207\n",
      "Epoch 558/1000\n",
      "272/272 [==============================] - 0s 31us/step - loss: 10.7226 - val_loss: 14.6018\n",
      "Epoch 559/1000\n",
      "272/272 [==============================] - 0s 21us/step - loss: 10.7038 - val_loss: 14.5837\n",
      "Epoch 560/1000\n",
      "272/272 [==============================] - 0s 23us/step - loss: 10.6842 - val_loss: 14.5666\n",
      "Epoch 561/1000\n",
      "272/272 [==============================] - 0s 26us/step - loss: 10.6650 - val_loss: 14.5479\n",
      "Epoch 562/1000\n",
      "272/272 [==============================] - 0s 25us/step - loss: 10.6456 - val_loss: 14.5284\n",
      "Epoch 563/1000\n",
      "272/272 [==============================] - 0s 35us/step - loss: 10.6264 - val_loss: 14.5084\n",
      "Epoch 564/1000\n",
      "272/272 [==============================] - 0s 26us/step - loss: 10.6079 - val_loss: 14.4877\n",
      "Epoch 565/1000\n",
      "272/272 [==============================] - 0s 25us/step - loss: 10.5885 - val_loss: 14.4686\n",
      "Epoch 566/1000\n",
      "272/272 [==============================] - 0s 37us/step - loss: 10.5693 - val_loss: 14.4500\n",
      "Epoch 567/1000\n",
      "272/272 [==============================] - 0s 37us/step - loss: 10.5501 - val_loss: 14.4324\n",
      "Epoch 568/1000\n",
      "272/272 [==============================] - 0s 24us/step - loss: 10.5313 - val_loss: 14.4140\n",
      "Epoch 569/1000\n",
      "272/272 [==============================] - 0s 34us/step - loss: 10.5121 - val_loss: 14.3961\n",
      "Epoch 570/1000\n",
      "272/272 [==============================] - 0s 29us/step - loss: 10.4935 - val_loss: 14.3779\n",
      "Epoch 571/1000\n",
      "272/272 [==============================] - 0s 28us/step - loss: 10.4746 - val_loss: 14.3594\n",
      "Epoch 572/1000\n",
      "272/272 [==============================] - 0s 29us/step - loss: 10.4557 - val_loss: 14.3398\n",
      "Epoch 573/1000\n",
      "272/272 [==============================] - 0s 35us/step - loss: 10.4367 - val_loss: 14.3206\n",
      "Epoch 574/1000\n",
      "272/272 [==============================] - 0s 41us/step - loss: 10.4182 - val_loss: 14.3009\n",
      "Epoch 575/1000\n",
      "272/272 [==============================] - 0s 28us/step - loss: 10.3991 - val_loss: 14.2812\n",
      "Epoch 576/1000\n",
      "272/272 [==============================] - ETA: 0s - loss: 10.16 - 0s 23us/step - loss: 10.3810 - val_loss: 14.2591\n",
      "Epoch 577/1000\n",
      "272/272 [==============================] - 0s 30us/step - loss: 10.3616 - val_loss: 14.2350\n",
      "Epoch 578/1000\n",
      "272/272 [==============================] - 0s 20us/step - loss: 10.3427 - val_loss: 14.2111\n",
      "Epoch 579/1000\n",
      "272/272 [==============================] - 0s 31us/step - loss: 10.3241 - val_loss: 14.1861\n",
      "Epoch 580/1000\n",
      "272/272 [==============================] - 0s 26us/step - loss: 10.3049 - val_loss: 14.1615\n",
      "Epoch 581/1000\n",
      "272/272 [==============================] - 0s 35us/step - loss: 10.2863 - val_loss: 14.1360\n",
      "Epoch 582/1000\n",
      "272/272 [==============================] - 0s 23us/step - loss: 10.2703 - val_loss: 14.1116\n",
      "Epoch 583/1000\n",
      "272/272 [==============================] - 0s 26us/step - loss: 10.2508 - val_loss: 14.0908\n",
      "Epoch 584/1000\n",
      "272/272 [==============================] - ETA: 0s - loss: 10.10 - 0s 25us/step - loss: 10.2313 - val_loss: 14.0723\n",
      "Epoch 585/1000\n",
      "272/272 [==============================] - 0s 28us/step - loss: 10.2125 - val_loss: 14.0542\n",
      "Epoch 586/1000\n",
      "272/272 [==============================] - 0s 30us/step - loss: 10.1932 - val_loss: 14.0370\n",
      "Epoch 587/1000\n",
      "272/272 [==============================] - 0s 28us/step - loss: 10.1738 - val_loss: 14.0197\n",
      "Epoch 588/1000\n",
      "272/272 [==============================] - 0s 29us/step - loss: 10.1563 - val_loss: 14.0023\n",
      "Epoch 589/1000\n",
      "272/272 [==============================] - 0s 26us/step - loss: 10.1361 - val_loss: 13.9840\n",
      "Epoch 590/1000\n",
      "272/272 [==============================] - 0s 45us/step - loss: 10.1172 - val_loss: 13.9650\n",
      "Epoch 591/1000\n",
      "272/272 [==============================] - 0s 27us/step - loss: 10.0985 - val_loss: 13.9458\n",
      "Epoch 592/1000\n",
      "272/272 [==============================] - 0s 29us/step - loss: 10.0794 - val_loss: 13.9234\n",
      "Epoch 593/1000\n",
      "272/272 [==============================] - 0s 26us/step - loss: 10.0604 - val_loss: 13.9013\n",
      "Epoch 594/1000\n",
      "272/272 [==============================] - 0s 25us/step - loss: 10.0413 - val_loss: 13.8786\n",
      "Epoch 595/1000\n",
      "272/272 [==============================] - 0s 38us/step - loss: 10.0236 - val_loss: 13.8573\n",
      "Epoch 596/1000\n",
      "272/272 [==============================] - 0s 28us/step - loss: 10.0040 - val_loss: 13.8386\n",
      "Epoch 597/1000\n",
      "272/272 [==============================] - 0s 31us/step - loss: 9.9849 - val_loss: 13.8211\n",
      "Epoch 598/1000\n",
      "272/272 [==============================] - 0s 33us/step - loss: 9.9663 - val_loss: 13.8040\n",
      "Epoch 599/1000\n",
      "272/272 [==============================] - 0s 23us/step - loss: 9.9476 - val_loss: 13.7870\n",
      "Epoch 600/1000\n",
      "272/272 [==============================] - 0s 29us/step - loss: 9.9288 - val_loss: 13.7685\n",
      "Epoch 601/1000\n",
      "272/272 [==============================] - 0s 31us/step - loss: 9.9104 - val_loss: 13.7512\n",
      "Epoch 602/1000\n",
      "272/272 [==============================] - 0s 25us/step - loss: 9.8922 - val_loss: 13.7350\n",
      "Epoch 603/1000\n",
      "272/272 [==============================] - 0s 25us/step - loss: 9.8733 - val_loss: 13.7168\n",
      "Epoch 604/1000\n",
      "272/272 [==============================] - 0s 28us/step - loss: 9.8546 - val_loss: 13.6982\n",
      "Epoch 605/1000\n",
      "272/272 [==============================] - 0s 22us/step - loss: 9.8362 - val_loss: 13.6801\n",
      "Epoch 606/1000\n",
      "272/272 [==============================] - 0s 22us/step - loss: 9.8177 - val_loss: 13.6611\n",
      "Epoch 607/1000\n",
      "272/272 [==============================] - 0s 25us/step - loss: 9.7994 - val_loss: 13.6402\n",
      "Epoch 608/1000\n",
      "272/272 [==============================] - 0s 26us/step - loss: 9.7801 - val_loss: 13.6170\n",
      "Epoch 609/1000\n",
      "272/272 [==============================] - 0s 22us/step - loss: 9.7625 - val_loss: 13.5941\n",
      "Epoch 610/1000\n",
      "272/272 [==============================] - 0s 34us/step - loss: 9.7434 - val_loss: 13.5722\n",
      "Epoch 611/1000\n",
      "272/272 [==============================] - 0s 27us/step - loss: 9.7242 - val_loss: 13.5506\n",
      "Epoch 612/1000\n",
      "272/272 [==============================] - 0s 27us/step - loss: 9.7058 - val_loss: 13.5288\n",
      "Epoch 613/1000\n",
      "272/272 [==============================] - 0s 27us/step - loss: 9.6874 - val_loss: 13.5065\n",
      "Epoch 614/1000\n",
      "272/272 [==============================] - 0s 29us/step - loss: 9.6690 - val_loss: 13.4854\n",
      "Epoch 615/1000\n",
      "272/272 [==============================] - 0s 22us/step - loss: 9.6502 - val_loss: 13.4644\n",
      "Epoch 616/1000\n",
      "272/272 [==============================] - 0s 38us/step - loss: 9.6313 - val_loss: 13.4441\n",
      "Epoch 617/1000\n",
      "272/272 [==============================] - 0s 32us/step - loss: 9.6134 - val_loss: 13.4225\n",
      "Epoch 618/1000\n",
      "272/272 [==============================] - 0s 25us/step - loss: 9.5951 - val_loss: 13.4010\n",
      "Epoch 619/1000\n",
      "272/272 [==============================] - 0s 32us/step - loss: 9.5762 - val_loss: 13.3805\n",
      "Epoch 620/1000\n",
      "272/272 [==============================] - 0s 39us/step - loss: 9.5576 - val_loss: 13.3592\n",
      "Epoch 621/1000\n",
      "272/272 [==============================] - 0s 30us/step - loss: 9.5399 - val_loss: 13.3372\n",
      "Epoch 622/1000\n",
      "272/272 [==============================] - 0s 26us/step - loss: 9.5212 - val_loss: 13.3146\n",
      "Epoch 623/1000\n",
      "272/272 [==============================] - 0s 30us/step - loss: 9.5028 - val_loss: 13.2910\n",
      "Epoch 624/1000\n",
      "272/272 [==============================] - 0s 28us/step - loss: 9.4849 - val_loss: 13.2660\n",
      "Epoch 625/1000\n",
      "272/272 [==============================] - 0s 24us/step - loss: 9.4664 - val_loss: 13.2410\n",
      "Epoch 626/1000\n",
      "272/272 [==============================] - 0s 30us/step - loss: 9.4487 - val_loss: 13.2179\n",
      "Epoch 627/1000\n",
      "272/272 [==============================] - 0s 25us/step - loss: 9.4303 - val_loss: 13.1962\n",
      "Epoch 628/1000\n",
      "272/272 [==============================] - 0s 26us/step - loss: 9.4122 - val_loss: 13.1749\n",
      "Epoch 629/1000\n",
      "272/272 [==============================] - 0s 26us/step - loss: 9.3931 - val_loss: 13.1554\n",
      "Epoch 630/1000\n",
      "272/272 [==============================] - 0s 30us/step - loss: 9.3750 - val_loss: 13.1358\n",
      "Epoch 631/1000\n",
      "272/272 [==============================] - 0s 29us/step - loss: 9.3556 - val_loss: 13.1162\n",
      "Epoch 632/1000\n",
      "272/272 [==============================] - 0s 29us/step - loss: 9.3379 - val_loss: 13.0960\n",
      "Epoch 633/1000\n",
      "272/272 [==============================] - 0s 29us/step - loss: 9.3190 - val_loss: 13.0763\n",
      "Epoch 634/1000\n",
      "272/272 [==============================] - 0s 25us/step - loss: 9.3011 - val_loss: 13.0557\n",
      "Epoch 635/1000\n",
      "272/272 [==============================] - 0s 27us/step - loss: 9.2829 - val_loss: 13.0359\n",
      "Epoch 636/1000\n",
      "272/272 [==============================] - 0s 21us/step - loss: 9.2648 - val_loss: 13.0150\n",
      "Epoch 637/1000\n",
      "272/272 [==============================] - 0s 35us/step - loss: 9.2466 - val_loss: 12.9944\n",
      "Epoch 638/1000\n",
      "272/272 [==============================] - 0s 23us/step - loss: 9.2283 - val_loss: 12.9729\n",
      "Epoch 639/1000\n",
      "272/272 [==============================] - 0s 37us/step - loss: 9.2106 - val_loss: 12.9496\n",
      "Epoch 640/1000\n",
      "272/272 [==============================] - 0s 35us/step - loss: 9.1927 - val_loss: 12.9258\n",
      "Epoch 641/1000\n",
      "272/272 [==============================] - 0s 27us/step - loss: 9.1746 - val_loss: 12.9021\n",
      "Epoch 642/1000\n",
      "272/272 [==============================] - 0s 37us/step - loss: 9.1570 - val_loss: 12.8785\n",
      "Epoch 643/1000\n",
      "272/272 [==============================] - 0s 25us/step - loss: 9.1388 - val_loss: 12.8556\n",
      "Epoch 644/1000\n",
      "272/272 [==============================] - 0s 32us/step - loss: 9.1204 - val_loss: 12.8326\n",
      "Epoch 645/1000\n",
      "272/272 [==============================] - 0s 22us/step - loss: 9.1026 - val_loss: 12.8101\n",
      "Epoch 646/1000\n",
      "272/272 [==============================] - 0s 25us/step - loss: 9.0850 - val_loss: 12.7882\n",
      "Epoch 647/1000\n",
      "272/272 [==============================] - 0s 22us/step - loss: 9.0669 - val_loss: 12.7663\n",
      "Epoch 648/1000\n",
      "272/272 [==============================] - 0s 30us/step - loss: 9.0489 - val_loss: 12.7436\n",
      "Epoch 649/1000\n",
      "272/272 [==============================] - 0s 32us/step - loss: 9.0312 - val_loss: 12.7218\n",
      "Epoch 650/1000\n",
      "272/272 [==============================] - 0s 30us/step - loss: 9.0133 - val_loss: 12.7016\n",
      "Epoch 651/1000\n",
      "272/272 [==============================] - 0s 28us/step - loss: 8.9956 - val_loss: 12.6815\n",
      "Epoch 652/1000\n",
      "272/272 [==============================] - 0s 24us/step - loss: 8.9787 - val_loss: 12.6612\n",
      "Epoch 653/1000\n",
      "272/272 [==============================] - 0s 36us/step - loss: 8.9606 - val_loss: 12.6397\n",
      "Epoch 654/1000\n",
      "272/272 [==============================] - 0s 17us/step - loss: 8.9427 - val_loss: 12.6174\n",
      "Epoch 655/1000\n",
      "272/272 [==============================] - 0s 36us/step - loss: 8.9251 - val_loss: 12.5950\n",
      "Epoch 656/1000\n",
      "272/272 [==============================] - 0s 21us/step - loss: 8.9072 - val_loss: 12.5727\n",
      "Epoch 657/1000\n",
      "272/272 [==============================] - 0s 28us/step - loss: 8.8897 - val_loss: 12.5497\n",
      "Epoch 658/1000\n",
      "272/272 [==============================] - 0s 22us/step - loss: 8.8718 - val_loss: 12.5258\n",
      "Epoch 659/1000\n",
      "272/272 [==============================] - 0s 27us/step - loss: 8.8544 - val_loss: 12.5010\n",
      "Epoch 660/1000\n",
      "272/272 [==============================] - 0s 20us/step - loss: 8.8372 - val_loss: 12.4778\n",
      "Epoch 661/1000\n",
      "272/272 [==============================] - 0s 28us/step - loss: 8.8194 - val_loss: 12.4558\n",
      "Epoch 662/1000\n",
      "272/272 [==============================] - 0s 22us/step - loss: 8.8024 - val_loss: 12.4332\n",
      "Epoch 663/1000\n",
      "272/272 [==============================] - 0s 30us/step - loss: 8.7849 - val_loss: 12.4112\n",
      "Epoch 664/1000\n",
      "272/272 [==============================] - 0s 20us/step - loss: 8.7669 - val_loss: 12.3892\n",
      "Epoch 665/1000\n",
      "272/272 [==============================] - 0s 36us/step - loss: 8.7496 - val_loss: 12.3667\n",
      "Epoch 666/1000\n",
      "272/272 [==============================] - 0s 28us/step - loss: 8.7321 - val_loss: 12.3435\n",
      "Epoch 667/1000\n",
      "272/272 [==============================] - 0s 29us/step - loss: 8.7150 - val_loss: 12.3212\n",
      "Epoch 668/1000\n",
      "272/272 [==============================] - 0s 21us/step - loss: 8.6965 - val_loss: 12.2986\n",
      "Epoch 669/1000\n",
      "272/272 [==============================] - 0s 25us/step - loss: 8.6789 - val_loss: 12.2764\n",
      "Epoch 670/1000\n",
      "272/272 [==============================] - 0s 20us/step - loss: 8.6616 - val_loss: 12.2553\n",
      "Epoch 671/1000\n",
      "272/272 [==============================] - 0s 32us/step - loss: 8.6440 - val_loss: 12.2343\n",
      "Epoch 672/1000\n",
      "272/272 [==============================] - 0s 26us/step - loss: 8.6261 - val_loss: 12.2126\n",
      "Epoch 673/1000\n",
      "272/272 [==============================] - 0s 24us/step - loss: 8.6086 - val_loss: 12.1901\n",
      "Epoch 674/1000\n",
      "272/272 [==============================] - 0s 29us/step - loss: 8.5916 - val_loss: 12.1676\n",
      "Epoch 675/1000\n",
      "272/272 [==============================] - 0s 24us/step - loss: 8.5737 - val_loss: 12.1445\n",
      "Epoch 676/1000\n",
      "272/272 [==============================] - 0s 21us/step - loss: 8.5556 - val_loss: 12.1209\n",
      "Epoch 677/1000\n",
      "272/272 [==============================] - 0s 25us/step - loss: 8.5385 - val_loss: 12.0965\n",
      "Epoch 678/1000\n",
      "272/272 [==============================] - 0s 18us/step - loss: 8.5208 - val_loss: 12.0726\n",
      "Epoch 679/1000\n",
      "272/272 [==============================] - 0s 31us/step - loss: 8.5033 - val_loss: 12.0490\n",
      "Epoch 680/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "272/272 [==============================] - 0s 23us/step - loss: 8.4858 - val_loss: 12.0266\n",
      "Epoch 681/1000\n",
      "272/272 [==============================] - 0s 27us/step - loss: 8.4686 - val_loss: 12.0049\n",
      "Epoch 682/1000\n",
      "272/272 [==============================] - 0s 20us/step - loss: 8.4507 - val_loss: 11.9837\n",
      "Epoch 683/1000\n",
      "272/272 [==============================] - 0s 36us/step - loss: 8.4335 - val_loss: 11.9607\n",
      "Epoch 684/1000\n",
      "272/272 [==============================] - 0s 33us/step - loss: 8.4151 - val_loss: 11.9363\n",
      "Epoch 685/1000\n",
      "272/272 [==============================] - 0s 22us/step - loss: 8.3981 - val_loss: 11.9115\n",
      "Epoch 686/1000\n",
      "272/272 [==============================] - 0s 25us/step - loss: 8.3802 - val_loss: 11.8888\n",
      "Epoch 687/1000\n",
      "272/272 [==============================] - 0s 44us/step - loss: 8.3623 - val_loss: 11.8681\n",
      "Epoch 688/1000\n",
      "272/272 [==============================] - 0s 27us/step - loss: 8.3449 - val_loss: 11.8477\n",
      "Epoch 689/1000\n",
      "272/272 [==============================] - 0s 24us/step - loss: 8.3273 - val_loss: 11.8275\n",
      "Epoch 690/1000\n",
      "272/272 [==============================] - 0s 33us/step - loss: 8.3101 - val_loss: 11.8064\n",
      "Epoch 691/1000\n",
      "272/272 [==============================] - 0s 24us/step - loss: 8.2929 - val_loss: 11.7837\n",
      "Epoch 692/1000\n",
      "272/272 [==============================] - 0s 21us/step - loss: 8.2748 - val_loss: 11.7589\n",
      "Epoch 693/1000\n",
      "272/272 [==============================] - 0s 37us/step - loss: 8.2580 - val_loss: 11.7349\n",
      "Epoch 694/1000\n",
      "272/272 [==============================] - 0s 24us/step - loss: 8.2395 - val_loss: 11.7126\n",
      "Epoch 695/1000\n",
      "272/272 [==============================] - 0s 27us/step - loss: 8.2217 - val_loss: 11.6905\n",
      "Epoch 696/1000\n",
      "272/272 [==============================] - 0s 23us/step - loss: 8.2047 - val_loss: 11.6679\n",
      "Epoch 697/1000\n",
      "272/272 [==============================] - 0s 25us/step - loss: 8.1872 - val_loss: 11.6446\n",
      "Epoch 698/1000\n",
      "272/272 [==============================] - 0s 20us/step - loss: 8.1692 - val_loss: 11.6222\n",
      "Epoch 699/1000\n",
      "272/272 [==============================] - 0s 21us/step - loss: 8.1517 - val_loss: 11.5991\n",
      "Epoch 700/1000\n",
      "272/272 [==============================] - 0s 22us/step - loss: 8.1347 - val_loss: 11.5745\n",
      "Epoch 701/1000\n",
      "272/272 [==============================] - 0s 24us/step - loss: 8.1165 - val_loss: 11.5508\n",
      "Epoch 702/1000\n",
      "272/272 [==============================] - 0s 22us/step - loss: 8.0989 - val_loss: 11.5261\n",
      "Epoch 703/1000\n",
      "272/272 [==============================] - 0s 25us/step - loss: 8.0812 - val_loss: 11.5009\n",
      "Epoch 704/1000\n",
      "272/272 [==============================] - 0s 24us/step - loss: 8.0635 - val_loss: 11.4746\n",
      "Epoch 705/1000\n",
      "272/272 [==============================] - 0s 22us/step - loss: 8.0463 - val_loss: 11.4485\n",
      "Epoch 706/1000\n",
      "272/272 [==============================] - 0s 18us/step - loss: 8.0289 - val_loss: 11.4224\n",
      "Epoch 707/1000\n",
      "272/272 [==============================] - 0s 23us/step - loss: 8.0117 - val_loss: 11.3959\n",
      "Epoch 708/1000\n",
      "272/272 [==============================] - 0s 22us/step - loss: 7.9947 - val_loss: 11.3691\n",
      "Epoch 709/1000\n",
      "272/272 [==============================] - 0s 28us/step - loss: 7.9782 - val_loss: 11.3431\n",
      "Epoch 710/1000\n",
      "272/272 [==============================] - 0s 25us/step - loss: 7.9602 - val_loss: 11.3198\n",
      "Epoch 711/1000\n",
      "272/272 [==============================] - 0s 27us/step - loss: 7.9429 - val_loss: 11.2971\n",
      "Epoch 712/1000\n",
      "272/272 [==============================] - 0s 24us/step - loss: 7.9244 - val_loss: 11.2755\n",
      "Epoch 713/1000\n",
      "272/272 [==============================] - 0s 22us/step - loss: 7.9077 - val_loss: 11.2545\n",
      "Epoch 714/1000\n",
      "272/272 [==============================] - 0s 18us/step - loss: 7.8895 - val_loss: 11.2333\n",
      "Epoch 715/1000\n",
      "272/272 [==============================] - 0s 23us/step - loss: 7.8728 - val_loss: 11.2111\n",
      "Epoch 716/1000\n",
      "272/272 [==============================] - 0s 21us/step - loss: 7.8547 - val_loss: 11.1880\n",
      "Epoch 717/1000\n",
      "272/272 [==============================] - 0s 22us/step - loss: 7.8384 - val_loss: 11.1635\n",
      "Epoch 718/1000\n",
      "272/272 [==============================] - 0s 23us/step - loss: 7.8202 - val_loss: 11.1423\n",
      "Epoch 719/1000\n",
      "272/272 [==============================] - 0s 25us/step - loss: 7.8022 - val_loss: 11.1226\n",
      "Epoch 720/1000\n",
      "272/272 [==============================] - 0s 25us/step - loss: 7.7856 - val_loss: 11.1038\n",
      "Epoch 721/1000\n",
      "272/272 [==============================] - 0s 25us/step - loss: 7.7683 - val_loss: 11.0831\n",
      "Epoch 722/1000\n",
      "272/272 [==============================] - 0s 26us/step - loss: 7.7514 - val_loss: 11.0612\n",
      "Epoch 723/1000\n",
      "272/272 [==============================] - 0s 23us/step - loss: 7.7345 - val_loss: 11.0385\n",
      "Epoch 724/1000\n",
      "272/272 [==============================] - 0s 26us/step - loss: 7.7174 - val_loss: 11.0136\n",
      "Epoch 725/1000\n",
      "272/272 [==============================] - 0s 30us/step - loss: 7.6999 - val_loss: 10.9882\n",
      "Epoch 726/1000\n",
      "272/272 [==============================] - 0s 22us/step - loss: 7.6821 - val_loss: 10.9640\n",
      "Epoch 727/1000\n",
      "272/272 [==============================] - 0s 30us/step - loss: 7.6650 - val_loss: 10.9388\n",
      "Epoch 728/1000\n",
      "272/272 [==============================] - 0s 21us/step - loss: 7.6469 - val_loss: 10.9137\n",
      "Epoch 729/1000\n",
      "272/272 [==============================] - 0s 27us/step - loss: 7.6294 - val_loss: 10.8868\n",
      "Epoch 730/1000\n",
      "272/272 [==============================] - 0s 21us/step - loss: 7.6113 - val_loss: 10.8597\n",
      "Epoch 731/1000\n",
      "272/272 [==============================] - 0s 24us/step - loss: 7.5938 - val_loss: 10.8313\n",
      "Epoch 732/1000\n",
      "272/272 [==============================] - 0s 28us/step - loss: 7.5762 - val_loss: 10.8020\n",
      "Epoch 733/1000\n",
      "272/272 [==============================] - 0s 25us/step - loss: 7.5595 - val_loss: 10.7714\n",
      "Epoch 734/1000\n",
      "272/272 [==============================] - 0s 28us/step - loss: 7.5422 - val_loss: 10.7424\n",
      "Epoch 735/1000\n",
      "272/272 [==============================] - 0s 31us/step - loss: 7.5255 - val_loss: 10.7153\n",
      "Epoch 736/1000\n",
      "272/272 [==============================] - 0s 25us/step - loss: 7.5080 - val_loss: 10.6891\n",
      "Epoch 737/1000\n",
      "272/272 [==============================] - 0s 33us/step - loss: 7.4916 - val_loss: 10.6618\n",
      "Epoch 738/1000\n",
      "272/272 [==============================] - 0s 31us/step - loss: 7.4747 - val_loss: 10.6355\n",
      "Epoch 739/1000\n",
      "272/272 [==============================] - 0s 32us/step - loss: 7.4577 - val_loss: 10.6101\n",
      "Epoch 740/1000\n",
      "272/272 [==============================] - 0s 32us/step - loss: 7.4398 - val_loss: 10.5870\n",
      "Epoch 741/1000\n",
      "272/272 [==============================] - 0s 25us/step - loss: 7.4220 - val_loss: 10.5638\n",
      "Epoch 742/1000\n",
      "272/272 [==============================] - 0s 33us/step - loss: 7.4046 - val_loss: 10.5397\n",
      "Epoch 743/1000\n",
      "272/272 [==============================] - 0s 28us/step - loss: 7.3861 - val_loss: 10.5183\n",
      "Epoch 744/1000\n",
      "272/272 [==============================] - 0s 32us/step - loss: 7.3674 - val_loss: 10.4981\n",
      "Epoch 745/1000\n",
      "272/272 [==============================] - 0s 33us/step - loss: 7.3506 - val_loss: 10.4790\n",
      "Epoch 746/1000\n",
      "272/272 [==============================] - 0s 29us/step - loss: 7.3329 - val_loss: 10.4602\n",
      "Epoch 747/1000\n",
      "272/272 [==============================] - 0s 32us/step - loss: 7.3163 - val_loss: 10.4408\n",
      "Epoch 748/1000\n",
      "272/272 [==============================] - 0s 33us/step - loss: 7.2993 - val_loss: 10.4202\n",
      "Epoch 749/1000\n",
      "272/272 [==============================] - 0s 27us/step - loss: 7.2819 - val_loss: 10.3974\n",
      "Epoch 750/1000\n",
      "272/272 [==============================] - ETA: 0s - loss: 7.201 - 0s 36us/step - loss: 7.2647 - val_loss: 10.3728\n",
      "Epoch 751/1000\n",
      "272/272 [==============================] - 0s 25us/step - loss: 7.2473 - val_loss: 10.3480\n",
      "Epoch 752/1000\n",
      "272/272 [==============================] - 0s 32us/step - loss: 7.2296 - val_loss: 10.3234\n",
      "Epoch 753/1000\n",
      "272/272 [==============================] - 0s 28us/step - loss: 7.2121 - val_loss: 10.2975\n",
      "Epoch 754/1000\n",
      "272/272 [==============================] - 0s 26us/step - loss: 7.1953 - val_loss: 10.2703\n",
      "Epoch 755/1000\n",
      "272/272 [==============================] - 0s 17us/step - loss: 7.1776 - val_loss: 10.2435\n",
      "Epoch 756/1000\n",
      "272/272 [==============================] - 0s 28us/step - loss: 7.1615 - val_loss: 10.2167\n",
      "Epoch 757/1000\n",
      "272/272 [==============================] - 0s 24us/step - loss: 7.1442 - val_loss: 10.1922\n",
      "Epoch 758/1000\n",
      "272/272 [==============================] - 0s 32us/step - loss: 7.1264 - val_loss: 10.1696\n",
      "Epoch 759/1000\n",
      "272/272 [==============================] - 0s 22us/step - loss: 7.1102 - val_loss: 10.1473\n",
      "Epoch 760/1000\n",
      "272/272 [==============================] - 0s 34us/step - loss: 7.0927 - val_loss: 10.1252\n",
      "Epoch 761/1000\n",
      "272/272 [==============================] - 0s 29us/step - loss: 7.0762 - val_loss: 10.1024\n",
      "Epoch 762/1000\n",
      "272/272 [==============================] - 0s 34us/step - loss: 7.0589 - val_loss: 10.0794\n",
      "Epoch 763/1000\n",
      "272/272 [==============================] - 0s 36us/step - loss: 7.0419 - val_loss: 10.0571\n",
      "Epoch 764/1000\n",
      "272/272 [==============================] - 0s 23us/step - loss: 7.0250 - val_loss: 10.0345\n",
      "Epoch 765/1000\n",
      "272/272 [==============================] - 0s 26us/step - loss: 7.0077 - val_loss: 10.0104\n",
      "Epoch 766/1000\n",
      "272/272 [==============================] - 0s 28us/step - loss: 6.9908 - val_loss: 9.9863\n",
      "Epoch 767/1000\n",
      "272/272 [==============================] - 0s 28us/step - loss: 6.9740 - val_loss: 9.9621\n",
      "Epoch 768/1000\n",
      "272/272 [==============================] - 0s 31us/step - loss: 6.9567 - val_loss: 9.9376\n",
      "Epoch 769/1000\n",
      "272/272 [==============================] - 0s 26us/step - loss: 6.9402 - val_loss: 9.9135\n",
      "Epoch 770/1000\n",
      "272/272 [==============================] - 0s 31us/step - loss: 6.9234 - val_loss: 9.8915\n",
      "Epoch 771/1000\n",
      "272/272 [==============================] - 0s 25us/step - loss: 6.9065 - val_loss: 9.8692\n",
      "Epoch 772/1000\n",
      "272/272 [==============================] - 0s 25us/step - loss: 6.8892 - val_loss: 9.8458\n",
      "Epoch 773/1000\n",
      "272/272 [==============================] - 0s 35us/step - loss: 6.8735 - val_loss: 9.8217\n",
      "Epoch 774/1000\n",
      "272/272 [==============================] - 0s 33us/step - loss: 6.8560 - val_loss: 9.7991\n",
      "Epoch 775/1000\n",
      "272/272 [==============================] - 0s 27us/step - loss: 6.8393 - val_loss: 9.7760\n",
      "Epoch 776/1000\n",
      "272/272 [==============================] - 0s 31us/step - loss: 6.8219 - val_loss: 9.7539\n",
      "Epoch 777/1000\n",
      "272/272 [==============================] - 0s 31us/step - loss: 6.8046 - val_loss: 9.7317\n",
      "Epoch 778/1000\n",
      "272/272 [==============================] - 0s 32us/step - loss: 6.7876 - val_loss: 9.7096\n",
      "Epoch 779/1000\n",
      "272/272 [==============================] - 0s 38us/step - loss: 6.7711 - val_loss: 9.6864\n",
      "Epoch 780/1000\n",
      "272/272 [==============================] - 0s 24us/step - loss: 6.7542 - val_loss: 9.6627\n",
      "Epoch 781/1000\n",
      "272/272 [==============================] - 0s 28us/step - loss: 6.7370 - val_loss: 9.6385\n",
      "Epoch 782/1000\n",
      "272/272 [==============================] - 0s 23us/step - loss: 6.7201 - val_loss: 9.6121\n",
      "Epoch 783/1000\n",
      "272/272 [==============================] - 0s 24us/step - loss: 6.7028 - val_loss: 9.5857\n",
      "Epoch 784/1000\n",
      "272/272 [==============================] - 0s 28us/step - loss: 6.6859 - val_loss: 9.5589\n",
      "Epoch 785/1000\n",
      "272/272 [==============================] - 0s 25us/step - loss: 6.6686 - val_loss: 9.5312\n",
      "Epoch 786/1000\n",
      "272/272 [==============================] - 0s 36us/step - loss: 6.6507 - val_loss: 9.5050\n",
      "Epoch 787/1000\n",
      "272/272 [==============================] - 0s 25us/step - loss: 6.6339 - val_loss: 9.4799\n",
      "Epoch 788/1000\n",
      "272/272 [==============================] - 0s 27us/step - loss: 6.6168 - val_loss: 9.4548\n",
      "Epoch 789/1000\n",
      "272/272 [==============================] - 0s 26us/step - loss: 6.6001 - val_loss: 9.4292\n",
      "Epoch 790/1000\n",
      "272/272 [==============================] - 0s 25us/step - loss: 6.5829 - val_loss: 9.4027\n",
      "Epoch 791/1000\n",
      "272/272 [==============================] - 0s 22us/step - loss: 6.5658 - val_loss: 9.3764\n",
      "Epoch 792/1000\n",
      "272/272 [==============================] - 0s 23us/step - loss: 6.5485 - val_loss: 9.3509\n",
      "Epoch 793/1000\n",
      "272/272 [==============================] - 0s 24us/step - loss: 6.5308 - val_loss: 9.3262\n",
      "Epoch 794/1000\n",
      "272/272 [==============================] - 0s 26us/step - loss: 6.5135 - val_loss: 9.3015\n",
      "Epoch 795/1000\n",
      "272/272 [==============================] - 0s 24us/step - loss: 6.4961 - val_loss: 9.2760\n",
      "Epoch 796/1000\n",
      "272/272 [==============================] - 0s 36us/step - loss: 6.4785 - val_loss: 9.2507\n",
      "Epoch 797/1000\n",
      "272/272 [==============================] - 0s 58us/step - loss: 6.4613 - val_loss: 9.2255\n",
      "Epoch 798/1000\n",
      "272/272 [==============================] - 0s 43us/step - loss: 6.4430 - val_loss: 9.2017\n",
      "Epoch 799/1000\n",
      "272/272 [==============================] - 0s 31us/step - loss: 6.4260 - val_loss: 9.1783\n",
      "Epoch 800/1000\n",
      "272/272 [==============================] - 0s 25us/step - loss: 6.4087 - val_loss: 9.1532\n",
      "Epoch 801/1000\n",
      "272/272 [==============================] - 0s 29us/step - loss: 6.3928 - val_loss: 9.1270\n",
      "Epoch 802/1000\n",
      "272/272 [==============================] - 0s 37us/step - loss: 6.3747 - val_loss: 9.0995\n",
      "Epoch 803/1000\n",
      "272/272 [==============================] - ETA: 0s - loss: 6.722 - 0s 21us/step - loss: 6.3575 - val_loss: 9.0742\n",
      "Epoch 804/1000\n",
      "272/272 [==============================] - 0s 26us/step - loss: 6.3397 - val_loss: 9.0493\n",
      "Epoch 805/1000\n",
      "272/272 [==============================] - 0s 24us/step - loss: 6.3228 - val_loss: 9.0238\n",
      "Epoch 806/1000\n",
      "272/272 [==============================] - 0s 28us/step - loss: 6.3057 - val_loss: 8.9980\n",
      "Epoch 807/1000\n",
      "272/272 [==============================] - 0s 24us/step - loss: 6.2887 - val_loss: 8.9724\n",
      "Epoch 808/1000\n",
      "272/272 [==============================] - 0s 28us/step - loss: 6.2715 - val_loss: 8.9482\n",
      "Epoch 809/1000\n",
      "272/272 [==============================] - 0s 50us/step - loss: 6.2543 - val_loss: 8.9254\n",
      "Epoch 810/1000\n",
      "272/272 [==============================] - 0s 23us/step - loss: 6.2376 - val_loss: 8.9023\n",
      "Epoch 811/1000\n",
      "272/272 [==============================] - 0s 26us/step - loss: 6.2208 - val_loss: 8.8796\n",
      "Epoch 812/1000\n",
      "272/272 [==============================] - 0s 35us/step - loss: 6.2045 - val_loss: 8.8577\n",
      "Epoch 813/1000\n",
      "272/272 [==============================] - 0s 24us/step - loss: 6.1873 - val_loss: 8.8367\n",
      "Epoch 814/1000\n",
      "272/272 [==============================] - 0s 26us/step - loss: 6.1703 - val_loss: 8.8141\n",
      "Epoch 815/1000\n",
      "272/272 [==============================] - 0s 24us/step - loss: 6.1538 - val_loss: 8.7901\n",
      "Epoch 816/1000\n",
      "272/272 [==============================] - 0s 23us/step - loss: 6.1372 - val_loss: 8.7643\n",
      "Epoch 817/1000\n",
      "272/272 [==============================] - 0s 22us/step - loss: 6.1200 - val_loss: 8.7388\n",
      "Epoch 818/1000\n",
      "272/272 [==============================] - 0s 19us/step - loss: 6.1035 - val_loss: 8.7140\n",
      "Epoch 819/1000\n",
      "272/272 [==============================] - 0s 23us/step - loss: 6.0868 - val_loss: 8.6901\n",
      "Epoch 820/1000\n",
      "272/272 [==============================] - 0s 20us/step - loss: 6.0698 - val_loss: 8.6668\n",
      "Epoch 821/1000\n",
      "272/272 [==============================] - 0s 26us/step - loss: 6.0534 - val_loss: 8.6434\n",
      "Epoch 822/1000\n",
      "272/272 [==============================] - 0s 21us/step - loss: 6.0365 - val_loss: 8.6215\n",
      "Epoch 823/1000\n",
      "272/272 [==============================] - 0s 23us/step - loss: 6.0197 - val_loss: 8.5994\n",
      "Epoch 824/1000\n",
      "272/272 [==============================] - 0s 21us/step - loss: 6.0031 - val_loss: 8.5776\n",
      "Epoch 825/1000\n",
      "272/272 [==============================] - 0s 23us/step - loss: 5.9871 - val_loss: 8.5561\n",
      "Epoch 826/1000\n",
      "272/272 [==============================] - 0s 27us/step - loss: 5.9704 - val_loss: 8.5336\n",
      "Epoch 827/1000\n",
      "272/272 [==============================] - 0s 22us/step - loss: 5.9545 - val_loss: 8.5102\n",
      "Epoch 828/1000\n",
      "272/272 [==============================] - 0s 23us/step - loss: 5.9376 - val_loss: 8.4870\n",
      "Epoch 829/1000\n",
      "272/272 [==============================] - 0s 22us/step - loss: 5.9212 - val_loss: 8.4627\n",
      "Epoch 830/1000\n",
      "272/272 [==============================] - 0s 29us/step - loss: 5.9055 - val_loss: 8.4382\n",
      "Epoch 831/1000\n",
      "272/272 [==============================] - 0s 25us/step - loss: 5.8883 - val_loss: 8.4143\n",
      "Epoch 832/1000\n",
      "272/272 [==============================] - 0s 25us/step - loss: 5.8722 - val_loss: 8.3903\n",
      "Epoch 833/1000\n",
      "272/272 [==============================] - 0s 27us/step - loss: 5.8555 - val_loss: 8.3659\n",
      "Epoch 834/1000\n",
      "272/272 [==============================] - 0s 27us/step - loss: 5.8399 - val_loss: 8.3387\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 835/1000\n",
      "272/272 [==============================] - 0s 30us/step - loss: 5.8234 - val_loss: 8.3116\n",
      "Epoch 836/1000\n",
      "272/272 [==============================] - 0s 31us/step - loss: 5.8064 - val_loss: 8.2850\n",
      "Epoch 837/1000\n",
      "272/272 [==============================] - 0s 26us/step - loss: 5.7898 - val_loss: 8.2583\n",
      "Epoch 838/1000\n",
      "272/272 [==============================] - 0s 31us/step - loss: 5.7730 - val_loss: 8.2320\n",
      "Epoch 839/1000\n",
      "272/272 [==============================] - 0s 30us/step - loss: 5.7568 - val_loss: 8.2048\n",
      "Epoch 840/1000\n",
      "272/272 [==============================] - 0s 23us/step - loss: 5.7412 - val_loss: 8.1772\n",
      "Epoch 841/1000\n",
      "272/272 [==============================] - ETA: 0s - loss: 5.279 - 0s 24us/step - loss: 5.7237 - val_loss: 8.1493\n",
      "Epoch 842/1000\n",
      "272/272 [==============================] - 0s 29us/step - loss: 5.7081 - val_loss: 8.1206\n",
      "Epoch 843/1000\n",
      "272/272 [==============================] - 0s 32us/step - loss: 5.6916 - val_loss: 8.0928\n",
      "Epoch 844/1000\n",
      "272/272 [==============================] - 0s 20us/step - loss: 5.6758 - val_loss: 8.0666\n",
      "Epoch 845/1000\n",
      "272/272 [==============================] - 0s 26us/step - loss: 5.6594 - val_loss: 8.0423\n",
      "Epoch 846/1000\n",
      "272/272 [==============================] - 0s 26us/step - loss: 5.6435 - val_loss: 8.0191\n",
      "Epoch 847/1000\n",
      "272/272 [==============================] - 0s 30us/step - loss: 5.6277 - val_loss: 7.9965\n",
      "Epoch 848/1000\n",
      "272/272 [==============================] - 0s 34us/step - loss: 5.6119 - val_loss: 7.9743\n",
      "Epoch 849/1000\n",
      "272/272 [==============================] - 0s 24us/step - loss: 5.5963 - val_loss: 7.9514\n",
      "Epoch 850/1000\n",
      "272/272 [==============================] - 0s 40us/step - loss: 5.5810 - val_loss: 7.9280\n",
      "Epoch 851/1000\n",
      "272/272 [==============================] - 0s 25us/step - loss: 5.5649 - val_loss: 7.9056\n",
      "Epoch 852/1000\n",
      "272/272 [==============================] - 0s 29us/step - loss: 5.5491 - val_loss: 7.8816\n",
      "Epoch 853/1000\n",
      "272/272 [==============================] - 0s 24us/step - loss: 5.5332 - val_loss: 7.8568\n",
      "Epoch 854/1000\n",
      "272/272 [==============================] - 0s 30us/step - loss: 5.5180 - val_loss: 7.8334\n",
      "Epoch 855/1000\n",
      "272/272 [==============================] - 0s 22us/step - loss: 5.5021 - val_loss: 7.8124\n",
      "Epoch 856/1000\n",
      "272/272 [==============================] - 0s 24us/step - loss: 5.4860 - val_loss: 7.7917\n",
      "Epoch 857/1000\n",
      "272/272 [==============================] - 0s 24us/step - loss: 5.4703 - val_loss: 7.7694\n",
      "Epoch 858/1000\n",
      "272/272 [==============================] - ETA: 0s - loss: 5.410 - 0s 30us/step - loss: 5.4551 - val_loss: 7.7467\n",
      "Epoch 859/1000\n",
      "272/272 [==============================] - 0s 29us/step - loss: 5.4394 - val_loss: 7.7230\n",
      "Epoch 860/1000\n",
      "272/272 [==============================] - 0s 30us/step - loss: 5.4242 - val_loss: 7.6996\n",
      "Epoch 861/1000\n",
      "272/272 [==============================] - 0s 34us/step - loss: 5.4083 - val_loss: 7.6771\n",
      "Epoch 862/1000\n",
      "272/272 [==============================] - 0s 26us/step - loss: 5.3928 - val_loss: 7.6544\n",
      "Epoch 863/1000\n",
      "272/272 [==============================] - 0s 27us/step - loss: 5.3780 - val_loss: 7.6310\n",
      "Epoch 864/1000\n",
      "272/272 [==============================] - 0s 23us/step - loss: 5.3626 - val_loss: 7.6073\n",
      "Epoch 865/1000\n",
      "272/272 [==============================] - 0s 35us/step - loss: 5.3478 - val_loss: 7.5845\n",
      "Epoch 866/1000\n",
      "272/272 [==============================] - 0s 20us/step - loss: 5.3323 - val_loss: 7.5621\n",
      "Epoch 867/1000\n",
      "272/272 [==============================] - 0s 26us/step - loss: 5.3170 - val_loss: 7.5392\n",
      "Epoch 868/1000\n",
      "272/272 [==============================] - 0s 28us/step - loss: 5.3026 - val_loss: 7.5159\n",
      "Epoch 869/1000\n",
      "272/272 [==============================] - 0s 23us/step - loss: 5.2872 - val_loss: 7.4930\n",
      "Epoch 870/1000\n",
      "272/272 [==============================] - 0s 24us/step - loss: 5.2731 - val_loss: 7.4696\n",
      "Epoch 871/1000\n",
      "272/272 [==============================] - 0s 30us/step - loss: 5.2580 - val_loss: 7.4468\n",
      "Epoch 872/1000\n",
      "272/272 [==============================] - 0s 25us/step - loss: 5.2441 - val_loss: 7.4240\n",
      "Epoch 873/1000\n",
      "272/272 [==============================] - 0s 29us/step - loss: 5.2292 - val_loss: 7.4016\n",
      "Epoch 874/1000\n",
      "272/272 [==============================] - 0s 20us/step - loss: 5.2139 - val_loss: 7.3791\n",
      "Epoch 875/1000\n",
      "272/272 [==============================] - 0s 29us/step - loss: 5.1997 - val_loss: 7.3556\n",
      "Epoch 876/1000\n",
      "272/272 [==============================] - 0s 28us/step - loss: 5.1848 - val_loss: 7.3332\n",
      "Epoch 877/1000\n",
      "272/272 [==============================] - 0s 22us/step - loss: 5.1705 - val_loss: 7.3105\n",
      "Epoch 878/1000\n",
      "272/272 [==============================] - 0s 20us/step - loss: 5.1551 - val_loss: 7.2877\n",
      "Epoch 879/1000\n",
      "272/272 [==============================] - 0s 27us/step - loss: 5.1417 - val_loss: 7.2646\n",
      "Epoch 880/1000\n",
      "272/272 [==============================] - 0s 18us/step - loss: 5.1265 - val_loss: 7.2436\n",
      "Epoch 881/1000\n",
      "272/272 [==============================] - 0s 25us/step - loss: 5.1123 - val_loss: 7.2242\n",
      "Epoch 882/1000\n",
      "272/272 [==============================] - 0s 23us/step - loss: 5.0988 - val_loss: 7.2057\n",
      "Epoch 883/1000\n",
      "272/272 [==============================] - 0s 30us/step - loss: 5.0854 - val_loss: 7.1877\n",
      "Epoch 884/1000\n",
      "272/272 [==============================] - 0s 23us/step - loss: 5.0715 - val_loss: 7.1695\n",
      "Epoch 885/1000\n",
      "272/272 [==============================] - 0s 30us/step - loss: 5.0576 - val_loss: 7.1509\n",
      "Epoch 886/1000\n",
      "272/272 [==============================] - 0s 30us/step - loss: 5.0435 - val_loss: 7.1321\n",
      "Epoch 887/1000\n",
      "272/272 [==============================] - 0s 29us/step - loss: 5.0301 - val_loss: 7.1134\n",
      "Epoch 888/1000\n",
      "272/272 [==============================] - 0s 29us/step - loss: 5.0162 - val_loss: 7.0941\n",
      "Epoch 889/1000\n",
      "272/272 [==============================] - 0s 23us/step - loss: 5.0019 - val_loss: 7.0752\n",
      "Epoch 890/1000\n",
      "272/272 [==============================] - 0s 25us/step - loss: 4.9886 - val_loss: 7.0570\n",
      "Epoch 891/1000\n",
      "272/272 [==============================] - 0s 25us/step - loss: 4.9750 - val_loss: 7.0387\n",
      "Epoch 892/1000\n",
      "272/272 [==============================] - 0s 26us/step - loss: 4.9611 - val_loss: 7.0201\n",
      "Epoch 893/1000\n",
      "272/272 [==============================] - 0s 31us/step - loss: 4.9475 - val_loss: 7.0006\n",
      "Epoch 894/1000\n",
      "272/272 [==============================] - 0s 32us/step - loss: 4.9340 - val_loss: 6.9817\n",
      "Epoch 895/1000\n",
      "272/272 [==============================] - 0s 26us/step - loss: 4.9205 - val_loss: 6.9628\n",
      "Epoch 896/1000\n",
      "272/272 [==============================] - 0s 32us/step - loss: 4.9061 - val_loss: 6.9425\n",
      "Epoch 897/1000\n",
      "272/272 [==============================] - 0s 23us/step - loss: 4.8928 - val_loss: 6.9218\n",
      "Epoch 898/1000\n",
      "272/272 [==============================] - 0s 33us/step - loss: 4.8800 - val_loss: 6.9009\n",
      "Epoch 899/1000\n",
      "272/272 [==============================] - 0s 22us/step - loss: 4.8664 - val_loss: 6.8799\n",
      "Epoch 900/1000\n",
      "272/272 [==============================] - 0s 32us/step - loss: 4.8525 - val_loss: 6.8587\n",
      "Epoch 901/1000\n",
      "272/272 [==============================] - 0s 31us/step - loss: 4.8378 - val_loss: 6.8373\n",
      "Epoch 902/1000\n",
      "272/272 [==============================] - 0s 29us/step - loss: 4.8248 - val_loss: 6.8160\n",
      "Epoch 903/1000\n",
      "272/272 [==============================] - 0s 31us/step - loss: 4.8104 - val_loss: 6.7953\n",
      "Epoch 904/1000\n",
      "272/272 [==============================] - 0s 23us/step - loss: 4.7970 - val_loss: 6.7745\n",
      "Epoch 905/1000\n",
      "272/272 [==============================] - 0s 47us/step - loss: 4.7835 - val_loss: 6.7527\n",
      "Epoch 906/1000\n",
      "272/272 [==============================] - 0s 24us/step - loss: 4.7705 - val_loss: 6.7309\n",
      "Epoch 907/1000\n",
      "272/272 [==============================] - 0s 23us/step - loss: 4.7559 - val_loss: 6.7097\n",
      "Epoch 908/1000\n",
      "272/272 [==============================] - 0s 35us/step - loss: 4.7429 - val_loss: 6.6876\n",
      "Epoch 909/1000\n",
      "272/272 [==============================] - 0s 25us/step - loss: 4.7295 - val_loss: 6.6654\n",
      "Epoch 910/1000\n",
      "272/272 [==============================] - 0s 31us/step - loss: 4.7159 - val_loss: 6.6436\n",
      "Epoch 911/1000\n",
      "272/272 [==============================] - 0s 32us/step - loss: 4.7027 - val_loss: 6.6220\n",
      "Epoch 912/1000\n",
      "272/272 [==============================] - 0s 27us/step - loss: 4.6898 - val_loss: 6.5998\n",
      "Epoch 913/1000\n",
      "272/272 [==============================] - 0s 17us/step - loss: 4.6772 - val_loss: 6.5781\n",
      "Epoch 914/1000\n",
      "272/272 [==============================] - 0s 22us/step - loss: 4.6636 - val_loss: 6.5579\n",
      "Epoch 915/1000\n",
      "272/272 [==============================] - 0s 19us/step - loss: 4.6509 - val_loss: 6.5374\n",
      "Epoch 916/1000\n",
      "272/272 [==============================] - 0s 35us/step - loss: 4.6380 - val_loss: 6.5180\n",
      "Epoch 917/1000\n",
      "272/272 [==============================] - 0s 20us/step - loss: 4.6252 - val_loss: 6.4994\n",
      "Epoch 918/1000\n",
      "272/272 [==============================] - 0s 35us/step - loss: 4.6124 - val_loss: 6.4818\n",
      "Epoch 919/1000\n",
      "272/272 [==============================] - 0s 22us/step - loss: 4.5998 - val_loss: 6.4647\n",
      "Epoch 920/1000\n",
      "272/272 [==============================] - 0s 29us/step - loss: 4.5873 - val_loss: 6.4467\n",
      "Epoch 921/1000\n",
      "272/272 [==============================] - 0s 20us/step - loss: 4.5743 - val_loss: 6.4276\n",
      "Epoch 922/1000\n",
      "272/272 [==============================] - 0s 26us/step - loss: 4.5613 - val_loss: 6.4077\n",
      "Epoch 923/1000\n",
      "272/272 [==============================] - 0s 23us/step - loss: 4.5493 - val_loss: 6.3872\n",
      "Epoch 924/1000\n",
      "272/272 [==============================] - 0s 23us/step - loss: 4.5363 - val_loss: 6.3661\n",
      "Epoch 925/1000\n",
      "272/272 [==============================] - 0s 23us/step - loss: 4.5240 - val_loss: 6.3448\n",
      "Epoch 926/1000\n",
      "272/272 [==============================] - 0s 33us/step - loss: 4.5111 - val_loss: 6.3240\n",
      "Epoch 927/1000\n",
      "272/272 [==============================] - 0s 24us/step - loss: 4.4992 - val_loss: 6.3051\n",
      "Epoch 928/1000\n",
      "272/272 [==============================] - 0s 22us/step - loss: 4.4863 - val_loss: 6.2868\n",
      "Epoch 929/1000\n",
      "272/272 [==============================] - 0s 25us/step - loss: 4.4744 - val_loss: 6.2690\n",
      "Epoch 930/1000\n",
      "272/272 [==============================] - 0s 43us/step - loss: 4.4625 - val_loss: 6.2512\n",
      "Epoch 931/1000\n",
      "272/272 [==============================] - 0s 24us/step - loss: 4.4501 - val_loss: 6.2330\n",
      "Epoch 932/1000\n",
      "272/272 [==============================] - 0s 28us/step - loss: 4.4384 - val_loss: 6.2152\n",
      "Epoch 933/1000\n",
      "272/272 [==============================] - 0s 19us/step - loss: 4.4262 - val_loss: 6.1976\n",
      "Epoch 934/1000\n",
      "272/272 [==============================] - 0s 24us/step - loss: 4.4141 - val_loss: 6.1799\n",
      "Epoch 935/1000\n",
      "272/272 [==============================] - 0s 20us/step - loss: 4.4024 - val_loss: 6.1617\n",
      "Epoch 936/1000\n",
      "272/272 [==============================] - 0s 23us/step - loss: 4.3901 - val_loss: 6.1438\n",
      "Epoch 937/1000\n",
      "272/272 [==============================] - 0s 23us/step - loss: 4.3778 - val_loss: 6.1260\n",
      "Epoch 938/1000\n",
      "272/272 [==============================] - 0s 25us/step - loss: 4.3662 - val_loss: 6.1083\n",
      "Epoch 939/1000\n",
      "272/272 [==============================] - 0s 22us/step - loss: 4.3541 - val_loss: 6.0914\n",
      "Epoch 940/1000\n",
      "272/272 [==============================] - 0s 27us/step - loss: 4.3418 - val_loss: 6.0746\n",
      "Epoch 941/1000\n",
      "272/272 [==============================] - 0s 23us/step - loss: 4.3300 - val_loss: 6.0574\n",
      "Epoch 942/1000\n",
      "272/272 [==============================] - 0s 30us/step - loss: 4.3182 - val_loss: 6.0399\n",
      "Epoch 943/1000\n",
      "272/272 [==============================] - 0s 23us/step - loss: 4.3058 - val_loss: 6.0223\n",
      "Epoch 944/1000\n",
      "272/272 [==============================] - 0s 29us/step - loss: 4.2943 - val_loss: 6.0045\n",
      "Epoch 945/1000\n",
      "272/272 [==============================] - 0s 23us/step - loss: 4.2819 - val_loss: 5.9858\n",
      "Epoch 946/1000\n",
      "272/272 [==============================] - 0s 26us/step - loss: 4.2702 - val_loss: 5.9669\n",
      "Epoch 947/1000\n",
      "272/272 [==============================] - 0s 21us/step - loss: 4.2587 - val_loss: 5.9485\n",
      "Epoch 948/1000\n",
      "272/272 [==============================] - 0s 33us/step - loss: 4.2468 - val_loss: 5.9306\n",
      "Epoch 949/1000\n",
      "272/272 [==============================] - 0s 22us/step - loss: 4.2351 - val_loss: 5.9129\n",
      "Epoch 950/1000\n",
      "272/272 [==============================] - 0s 25us/step - loss: 4.2231 - val_loss: 5.8949\n",
      "Epoch 951/1000\n",
      "272/272 [==============================] - 0s 20us/step - loss: 4.2122 - val_loss: 5.8763\n",
      "Epoch 952/1000\n",
      "272/272 [==============================] - 0s 28us/step - loss: 4.2006 - val_loss: 5.8581\n",
      "Epoch 953/1000\n",
      "272/272 [==============================] - 0s 27us/step - loss: 4.1891 - val_loss: 5.8397\n",
      "Epoch 954/1000\n",
      "272/272 [==============================] - 0s 30us/step - loss: 4.1780 - val_loss: 5.8204\n",
      "Epoch 955/1000\n",
      "272/272 [==============================] - 0s 27us/step - loss: 4.1667 - val_loss: 5.8006\n",
      "Epoch 956/1000\n",
      "272/272 [==============================] - 0s 31us/step - loss: 4.1558 - val_loss: 5.7806\n",
      "Epoch 957/1000\n",
      "272/272 [==============================] - 0s 26us/step - loss: 4.1441 - val_loss: 5.7612\n",
      "Epoch 958/1000\n",
      "272/272 [==============================] - 0s 33us/step - loss: 4.1325 - val_loss: 5.7426\n",
      "Epoch 959/1000\n",
      "272/272 [==============================] - 0s 25us/step - loss: 4.1212 - val_loss: 5.7240\n",
      "Epoch 960/1000\n",
      "272/272 [==============================] - 0s 21us/step - loss: 4.1099 - val_loss: 5.7046\n",
      "Epoch 961/1000\n",
      "272/272 [==============================] - 0s 29us/step - loss: 4.0994 - val_loss: 5.6846\n",
      "Epoch 962/1000\n",
      "272/272 [==============================] - 0s 26us/step - loss: 4.0873 - val_loss: 5.6652\n",
      "Epoch 963/1000\n",
      "272/272 [==============================] - 0s 27us/step - loss: 4.0760 - val_loss: 5.6460\n",
      "Epoch 964/1000\n",
      "272/272 [==============================] - 0s 27us/step - loss: 4.0654 - val_loss: 5.6271\n",
      "Epoch 965/1000\n",
      "272/272 [==============================] - 0s 26us/step - loss: 4.0551 - val_loss: 5.6089\n",
      "Epoch 966/1000\n",
      "272/272 [==============================] - 0s 21us/step - loss: 4.0456 - val_loss: 5.5921\n",
      "Epoch 967/1000\n",
      "272/272 [==============================] - 0s 25us/step - loss: 4.0346 - val_loss: 5.5746\n",
      "Epoch 968/1000\n",
      "272/272 [==============================] - 0s 20us/step - loss: 4.0240 - val_loss: 5.5568\n",
      "Epoch 969/1000\n",
      "272/272 [==============================] - 0s 30us/step - loss: 4.0119 - val_loss: 5.5382\n",
      "Epoch 970/1000\n",
      "272/272 [==============================] - 0s 20us/step - loss: 3.9999 - val_loss: 5.5184\n",
      "Epoch 971/1000\n",
      "272/272 [==============================] - 0s 27us/step - loss: 3.9880 - val_loss: 5.4986\n",
      "Epoch 972/1000\n",
      "272/272 [==============================] - 0s 26us/step - loss: 3.9780 - val_loss: 5.4793\n",
      "Epoch 973/1000\n",
      "272/272 [==============================] - 0s 27us/step - loss: 3.9690 - val_loss: 5.4605\n",
      "Epoch 974/1000\n",
      "272/272 [==============================] - 0s 28us/step - loss: 3.9611 - val_loss: 5.4421\n",
      "Epoch 975/1000\n",
      "272/272 [==============================] - 0s 32us/step - loss: 3.9531 - val_loss: 5.4241\n",
      "Epoch 976/1000\n",
      "272/272 [==============================] - 0s 25us/step - loss: 3.9437 - val_loss: 5.4070\n",
      "Epoch 977/1000\n",
      "272/272 [==============================] - 0s 27us/step - loss: 3.9339 - val_loss: 5.3901\n",
      "Epoch 978/1000\n",
      "272/272 [==============================] - 0s 24us/step - loss: 3.9235 - val_loss: 5.3741\n",
      "Epoch 979/1000\n",
      "272/272 [==============================] - 0s 40us/step - loss: 3.9126 - val_loss: 5.3595\n",
      "Epoch 980/1000\n",
      "272/272 [==============================] - 0s 29us/step - loss: 3.9018 - val_loss: 5.3451\n",
      "Epoch 981/1000\n",
      "272/272 [==============================] - 0s 33us/step - loss: 3.8933 - val_loss: 5.3305\n",
      "Epoch 982/1000\n",
      "272/272 [==============================] - 0s 25us/step - loss: 3.8841 - val_loss: 5.3148\n",
      "Epoch 983/1000\n",
      "272/272 [==============================] - 0s 22us/step - loss: 3.8744 - val_loss: 5.2998\n",
      "Epoch 984/1000\n",
      "272/272 [==============================] - 0s 29us/step - loss: 3.8659 - val_loss: 5.2837\n",
      "Epoch 985/1000\n",
      "272/272 [==============================] - 0s 28us/step - loss: 3.8571 - val_loss: 5.2651\n",
      "Epoch 986/1000\n",
      "272/272 [==============================] - 0s 22us/step - loss: 3.8469 - val_loss: 5.2466\n",
      "Epoch 987/1000\n",
      "272/272 [==============================] - 0s 25us/step - loss: 3.8378 - val_loss: 5.2282\n",
      "Epoch 988/1000\n",
      "272/272 [==============================] - 0s 28us/step - loss: 3.8286 - val_loss: 5.2103\n",
      "Epoch 989/1000\n",
      "272/272 [==============================] - ETA: 0s - loss: 3.430 - 0s 28us/step - loss: 3.8194 - val_loss: 5.1918\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 990/1000\n",
      "272/272 [==============================] - 0s 34us/step - loss: 3.8117 - val_loss: 5.1740\n",
      "Epoch 991/1000\n",
      "272/272 [==============================] - 0s 23us/step - loss: 3.8022 - val_loss: 5.1580\n",
      "Epoch 992/1000\n",
      "272/272 [==============================] - 0s 25us/step - loss: 3.7929 - val_loss: 5.1430\n",
      "Epoch 993/1000\n",
      "272/272 [==============================] - 0s 23us/step - loss: 3.7838 - val_loss: 5.1282\n",
      "Epoch 994/1000\n",
      "272/272 [==============================] - 0s 26us/step - loss: 3.7761 - val_loss: 5.1123\n",
      "Epoch 995/1000\n",
      "272/272 [==============================] - 0s 22us/step - loss: 3.7673 - val_loss: 5.0957\n",
      "Epoch 996/1000\n",
      "272/272 [==============================] - 0s 26us/step - loss: 3.7588 - val_loss: 5.0781\n",
      "Epoch 997/1000\n",
      "272/272 [==============================] - 0s 23us/step - loss: 3.7496 - val_loss: 5.0633\n",
      "Epoch 998/1000\n",
      "272/272 [==============================] - 0s 30us/step - loss: 3.7399 - val_loss: 5.0484\n",
      "Epoch 999/1000\n",
      "272/272 [==============================] - 0s 19us/step - loss: 3.7320 - val_loss: 5.0321\n",
      "Epoch 1000/1000\n",
      "272/272 [==============================] - 0s 21us/step - loss: 3.7224 - val_loss: 5.0161\n",
      "Train on 273 samples, validate on 31 samples\n",
      "Epoch 1/1000\n",
      "273/273 [==============================] - 0s 473us/step - loss: 21.7286 - val_loss: 22.2207\n",
      "Epoch 2/1000\n",
      "273/273 [==============================] - 0s 17us/step - loss: 21.7059 - val_loss: 22.2221\n",
      "Epoch 3/1000\n",
      "273/273 [==============================] - 0s 18us/step - loss: 21.6830 - val_loss: 22.2219\n",
      "Train on 409 samples, validate on 46 samples\n",
      "Epoch 1/1000\n",
      "409/409 [==============================] - 0s 371us/step - loss: 22.1142 - val_loss: 22.5516\n",
      "Epoch 2/1000\n",
      "409/409 [==============================] - 0s 15us/step - loss: 22.0797 - val_loss: 22.5338\n",
      "Epoch 3/1000\n",
      "409/409 [==============================] - 0s 18us/step - loss: 22.0490 - val_loss: 22.5184\n",
      "Epoch 4/1000\n",
      "409/409 [==============================] - 0s 20us/step - loss: 22.0178 - val_loss: 22.4987\n",
      "Epoch 5/1000\n",
      "409/409 [==============================] - 0s 16us/step - loss: 21.9855 - val_loss: 22.4750\n",
      "Epoch 6/1000\n",
      "409/409 [==============================] - 0s 14us/step - loss: 21.9546 - val_loss: 22.4508\n",
      "Epoch 7/1000\n",
      "409/409 [==============================] - 0s 20us/step - loss: 21.9249 - val_loss: 22.4259\n",
      "Epoch 8/1000\n",
      "409/409 [==============================] - 0s 21us/step - loss: 21.8942 - val_loss: 22.3988\n",
      "Epoch 9/1000\n",
      "409/409 [==============================] - 0s 21us/step - loss: 21.8642 - val_loss: 22.3707\n",
      "Epoch 10/1000\n",
      "409/409 [==============================] - 0s 23us/step - loss: 21.8335 - val_loss: 22.3461\n",
      "Epoch 11/1000\n",
      "409/409 [==============================] - 0s 16us/step - loss: 21.8029 - val_loss: 22.3226\n",
      "Epoch 12/1000\n",
      "409/409 [==============================] - 0s 21us/step - loss: 21.7727 - val_loss: 22.3016\n",
      "Epoch 13/1000\n",
      "409/409 [==============================] - 0s 15us/step - loss: 21.7414 - val_loss: 22.2796\n",
      "Epoch 14/1000\n",
      "409/409 [==============================] - 0s 20us/step - loss: 21.7096 - val_loss: 22.2581\n",
      "Epoch 15/1000\n",
      "409/409 [==============================] - 0s 17us/step - loss: 21.6807 - val_loss: 22.2375\n",
      "Epoch 16/1000\n",
      "409/409 [==============================] - 0s 21us/step - loss: 21.6481 - val_loss: 22.2150\n",
      "Epoch 17/1000\n",
      "409/409 [==============================] - 0s 29us/step - loss: 21.6167 - val_loss: 22.1927\n",
      "Epoch 18/1000\n",
      "409/409 [==============================] - 0s 15us/step - loss: 21.5860 - val_loss: 22.1702\n",
      "Epoch 19/1000\n",
      "409/409 [==============================] - 0s 20us/step - loss: 21.5557 - val_loss: 22.1492\n",
      "Epoch 20/1000\n",
      "409/409 [==============================] - 0s 22us/step - loss: 21.5248 - val_loss: 22.1287\n",
      "Epoch 21/1000\n",
      "409/409 [==============================] - 0s 21us/step - loss: 21.4934 - val_loss: 22.1079\n",
      "Epoch 22/1000\n",
      "409/409 [==============================] - 0s 19us/step - loss: 21.4617 - val_loss: 22.0871\n",
      "Epoch 23/1000\n",
      "409/409 [==============================] - 0s 18us/step - loss: 21.4307 - val_loss: 22.0672\n",
      "Epoch 24/1000\n",
      "409/409 [==============================] - 0s 17us/step - loss: 21.3992 - val_loss: 22.0477\n",
      "Epoch 25/1000\n",
      "409/409 [==============================] - 0s 15us/step - loss: 21.3690 - val_loss: 22.0270\n",
      "Epoch 26/1000\n",
      "409/409 [==============================] - 0s 22us/step - loss: 21.3377 - val_loss: 22.0023\n",
      "Epoch 27/1000\n",
      "409/409 [==============================] - 0s 18us/step - loss: 21.3075 - val_loss: 21.9746\n",
      "Epoch 28/1000\n",
      "409/409 [==============================] - 0s 18us/step - loss: 21.2774 - val_loss: 21.9483\n",
      "Epoch 29/1000\n",
      "409/409 [==============================] - 0s 24us/step - loss: 21.2467 - val_loss: 21.9236\n",
      "Epoch 30/1000\n",
      "409/409 [==============================] - 0s 23us/step - loss: 21.2165 - val_loss: 21.8976\n",
      "Epoch 31/1000\n",
      "409/409 [==============================] - 0s 21us/step - loss: 21.1858 - val_loss: 21.8713\n",
      "Epoch 32/1000\n",
      "409/409 [==============================] - 0s 21us/step - loss: 21.1551 - val_loss: 21.8472\n",
      "Epoch 33/1000\n",
      "409/409 [==============================] - 0s 25us/step - loss: 21.1252 - val_loss: 21.8220\n",
      "Epoch 34/1000\n",
      "409/409 [==============================] - 0s 18us/step - loss: 21.0951 - val_loss: 21.7954\n",
      "Epoch 35/1000\n",
      "409/409 [==============================] - 0s 25us/step - loss: 21.0642 - val_loss: 21.7663\n",
      "Epoch 36/1000\n",
      "409/409 [==============================] - 0s 20us/step - loss: 21.0339 - val_loss: 21.7372\n",
      "Epoch 37/1000\n",
      "409/409 [==============================] - 0s 18us/step - loss: 21.0039 - val_loss: 21.7066\n",
      "Epoch 38/1000\n",
      "409/409 [==============================] - 0s 24us/step - loss: 20.9743 - val_loss: 21.6744\n",
      "Epoch 39/1000\n",
      "409/409 [==============================] - 0s 17us/step - loss: 20.9438 - val_loss: 21.6440\n",
      "Epoch 40/1000\n",
      "409/409 [==============================] - 0s 23us/step - loss: 20.9141 - val_loss: 21.6138\n",
      "Epoch 41/1000\n",
      "409/409 [==============================] - 0s 17us/step - loss: 20.8835 - val_loss: 21.5840\n",
      "Epoch 42/1000\n",
      "409/409 [==============================] - 0s 26us/step - loss: 20.8543 - val_loss: 21.5530\n",
      "Epoch 43/1000\n",
      "409/409 [==============================] - 0s 28us/step - loss: 20.8241 - val_loss: 21.5243\n",
      "Epoch 44/1000\n",
      "409/409 [==============================] - 0s 29us/step - loss: 20.7941 - val_loss: 21.4965\n",
      "Epoch 45/1000\n",
      "409/409 [==============================] - 0s 19us/step - loss: 20.7637 - val_loss: 21.4682\n",
      "Epoch 46/1000\n",
      "409/409 [==============================] - 0s 22us/step - loss: 20.7334 - val_loss: 21.4392\n",
      "Epoch 47/1000\n",
      "409/409 [==============================] - 0s 23us/step - loss: 20.7034 - val_loss: 21.4093\n",
      "Epoch 48/1000\n",
      "409/409 [==============================] - 0s 21us/step - loss: 20.6736 - val_loss: 21.3773\n",
      "Epoch 49/1000\n",
      "409/409 [==============================] - 0s 22us/step - loss: 20.6435 - val_loss: 21.3485\n",
      "Epoch 50/1000\n",
      "409/409 [==============================] - 0s 16us/step - loss: 20.6130 - val_loss: 21.3239\n",
      "Epoch 51/1000\n",
      "409/409 [==============================] - 0s 20us/step - loss: 20.5841 - val_loss: 21.3004\n",
      "Epoch 52/1000\n",
      "409/409 [==============================] - 0s 25us/step - loss: 20.5518 - val_loss: 21.2736\n",
      "Epoch 53/1000\n",
      "409/409 [==============================] - 0s 21us/step - loss: 20.5214 - val_loss: 21.2476\n",
      "Epoch 54/1000\n",
      "409/409 [==============================] - 0s 25us/step - loss: 20.4913 - val_loss: 21.2215\n",
      "Epoch 55/1000\n",
      "409/409 [==============================] - 0s 21us/step - loss: 20.4612 - val_loss: 21.1961\n",
      "Epoch 56/1000\n",
      "409/409 [==============================] - 0s 20us/step - loss: 20.4302 - val_loss: 21.1684\n",
      "Epoch 57/1000\n",
      "409/409 [==============================] - 0s 26us/step - loss: 20.4000 - val_loss: 21.1427\n",
      "Epoch 58/1000\n",
      "409/409 [==============================] - 0s 19us/step - loss: 20.3695 - val_loss: 21.1166\n",
      "Epoch 59/1000\n",
      "409/409 [==============================] - 0s 19us/step - loss: 20.3391 - val_loss: 21.0887\n",
      "Epoch 60/1000\n",
      "409/409 [==============================] - 0s 21us/step - loss: 20.3088 - val_loss: 21.0586\n",
      "Epoch 61/1000\n",
      "409/409 [==============================] - 0s 21us/step - loss: 20.2790 - val_loss: 21.0292\n",
      "Epoch 62/1000\n",
      "409/409 [==============================] - 0s 20us/step - loss: 20.2487 - val_loss: 21.0023\n",
      "Epoch 63/1000\n",
      "409/409 [==============================] - 0s 18us/step - loss: 20.2187 - val_loss: 20.9749\n",
      "Epoch 64/1000\n",
      "409/409 [==============================] - 0s 22us/step - loss: 20.1883 - val_loss: 20.9477\n",
      "Epoch 65/1000\n",
      "409/409 [==============================] - 0s 20us/step - loss: 20.1578 - val_loss: 20.9222\n",
      "Epoch 66/1000\n",
      "409/409 [==============================] - 0s 15us/step - loss: 20.1271 - val_loss: 20.8970\n",
      "Epoch 67/1000\n",
      "409/409 [==============================] - 0s 20us/step - loss: 20.0974 - val_loss: 20.8718\n",
      "Epoch 68/1000\n",
      "409/409 [==============================] - 0s 15us/step - loss: 20.0669 - val_loss: 20.8474\n",
      "Epoch 69/1000\n",
      "409/409 [==============================] - 0s 19us/step - loss: 20.0358 - val_loss: 20.8222\n",
      "Epoch 70/1000\n",
      "409/409 [==============================] - 0s 18us/step - loss: 20.0054 - val_loss: 20.7948\n",
      "Epoch 71/1000\n",
      "409/409 [==============================] - 0s 20us/step - loss: 19.9751 - val_loss: 20.7650\n",
      "Epoch 72/1000\n",
      "409/409 [==============================] - 0s 19us/step - loss: 19.9447 - val_loss: 20.7338\n",
      "Epoch 73/1000\n",
      "409/409 [==============================] - 0s 25us/step - loss: 19.9145 - val_loss: 20.7015\n",
      "Epoch 74/1000\n",
      "409/409 [==============================] - 0s 18us/step - loss: 19.8858 - val_loss: 20.6680\n",
      "Epoch 75/1000\n",
      "409/409 [==============================] - 0s 19us/step - loss: 19.8553 - val_loss: 20.6359\n",
      "Epoch 76/1000\n",
      "409/409 [==============================] - 0s 26us/step - loss: 19.8260 - val_loss: 20.6042\n",
      "Epoch 77/1000\n",
      "409/409 [==============================] - 0s 22us/step - loss: 19.7962 - val_loss: 20.5692\n",
      "Epoch 78/1000\n",
      "409/409 [==============================] - 0s 19us/step - loss: 19.7667 - val_loss: 20.5317\n",
      "Epoch 79/1000\n",
      "409/409 [==============================] - 0s 18us/step - loss: 19.7374 - val_loss: 20.4938\n",
      "Epoch 80/1000\n",
      "409/409 [==============================] - 0s 20us/step - loss: 19.7083 - val_loss: 20.4580\n",
      "Epoch 81/1000\n",
      "409/409 [==============================] - 0s 20us/step - loss: 19.6797 - val_loss: 20.4263\n",
      "Epoch 82/1000\n",
      "409/409 [==============================] - 0s 16us/step - loss: 19.6495 - val_loss: 20.3975\n",
      "Epoch 83/1000\n",
      "409/409 [==============================] - 0s 24us/step - loss: 19.6194 - val_loss: 20.3707\n",
      "Epoch 84/1000\n",
      "409/409 [==============================] - 0s 16us/step - loss: 19.5888 - val_loss: 20.3427\n",
      "Epoch 85/1000\n",
      "409/409 [==============================] - 0s 19us/step - loss: 19.5594 - val_loss: 20.3112\n",
      "Epoch 86/1000\n",
      "409/409 [==============================] - 0s 18us/step - loss: 19.5291 - val_loss: 20.2816\n",
      "Epoch 87/1000\n",
      "409/409 [==============================] - 0s 23us/step - loss: 19.4987 - val_loss: 20.2545\n",
      "Epoch 88/1000\n",
      "409/409 [==============================] - 0s 17us/step - loss: 19.4689 - val_loss: 20.2276\n",
      "Epoch 89/1000\n",
      "409/409 [==============================] - 0s 24us/step - loss: 19.4381 - val_loss: 20.1991\n",
      "Epoch 90/1000\n",
      "409/409 [==============================] - 0s 27us/step - loss: 19.4080 - val_loss: 20.1662\n",
      "Epoch 91/1000\n",
      "409/409 [==============================] - 0s 18us/step - loss: 19.3785 - val_loss: 20.1341\n",
      "Epoch 92/1000\n",
      "409/409 [==============================] - 0s 20us/step - loss: 19.3484 - val_loss: 20.1043\n",
      "Epoch 93/1000\n",
      "409/409 [==============================] - 0s 17us/step - loss: 19.3184 - val_loss: 20.0733\n",
      "Epoch 94/1000\n",
      "409/409 [==============================] - 0s 25us/step - loss: 19.2887 - val_loss: 20.0425\n",
      "Epoch 95/1000\n",
      "409/409 [==============================] - 0s 21us/step - loss: 19.2586 - val_loss: 20.0129\n",
      "Epoch 96/1000\n",
      "409/409 [==============================] - 0s 18us/step - loss: 19.2286 - val_loss: 19.9828\n",
      "Epoch 97/1000\n",
      "409/409 [==============================] - 0s 27us/step - loss: 19.1988 - val_loss: 19.9508\n",
      "Epoch 98/1000\n",
      "409/409 [==============================] - 0s 16us/step - loss: 19.1688 - val_loss: 19.9201\n",
      "Epoch 99/1000\n",
      "409/409 [==============================] - 0s 20us/step - loss: 19.1388 - val_loss: 19.8916\n",
      "Epoch 100/1000\n",
      "409/409 [==============================] - 0s 20us/step - loss: 19.1087 - val_loss: 19.8617\n",
      "Epoch 101/1000\n",
      "409/409 [==============================] - 0s 19us/step - loss: 19.0790 - val_loss: 19.8302\n",
      "Epoch 102/1000\n",
      "409/409 [==============================] - 0s 25us/step - loss: 19.0490 - val_loss: 19.7979\n",
      "Epoch 103/1000\n",
      "409/409 [==============================] - 0s 20us/step - loss: 19.0193 - val_loss: 19.7672\n",
      "Epoch 104/1000\n",
      "409/409 [==============================] - 0s 19us/step - loss: 18.9890 - val_loss: 19.7388\n",
      "Epoch 105/1000\n",
      "409/409 [==============================] - 0s 19us/step - loss: 18.9589 - val_loss: 19.7104\n",
      "Epoch 106/1000\n",
      "409/409 [==============================] - 0s 19us/step - loss: 18.9290 - val_loss: 19.6853\n",
      "Epoch 107/1000\n",
      "409/409 [==============================] - 0s 20us/step - loss: 18.8979 - val_loss: 19.6618\n",
      "Epoch 108/1000\n",
      "409/409 [==============================] - 0s 20us/step - loss: 18.8676 - val_loss: 19.6382\n",
      "Epoch 109/1000\n",
      "409/409 [==============================] - 0s 24us/step - loss: 18.8368 - val_loss: 19.6121\n",
      "Epoch 110/1000\n",
      "409/409 [==============================] - 0s 19us/step - loss: 18.8064 - val_loss: 19.5863\n",
      "Epoch 111/1000\n",
      "409/409 [==============================] - 0s 18us/step - loss: 18.7755 - val_loss: 19.5629\n",
      "Epoch 112/1000\n",
      "409/409 [==============================] - 0s 17us/step - loss: 18.7444 - val_loss: 19.5381\n",
      "Epoch 113/1000\n",
      "409/409 [==============================] - ETA: 0s - loss: 19.00 - 0s 21us/step - loss: 18.7150 - val_loss: 19.5108\n",
      "Epoch 114/1000\n",
      "409/409 [==============================] - 0s 20us/step - loss: 18.6843 - val_loss: 19.4809\n",
      "Epoch 115/1000\n",
      "409/409 [==============================] - 0s 21us/step - loss: 18.6544 - val_loss: 19.4487\n",
      "Epoch 116/1000\n",
      "409/409 [==============================] - 0s 22us/step - loss: 18.6231 - val_loss: 19.4120\n",
      "Epoch 117/1000\n",
      "409/409 [==============================] - 0s 17us/step - loss: 18.5939 - val_loss: 19.3718\n",
      "Epoch 118/1000\n",
      "409/409 [==============================] - 0s 26us/step - loss: 18.5651 - val_loss: 19.3332\n",
      "Epoch 119/1000\n",
      "409/409 [==============================] - 0s 22us/step - loss: 18.5361 - val_loss: 19.2983\n",
      "Epoch 120/1000\n",
      "409/409 [==============================] - 0s 22us/step - loss: 18.5077 - val_loss: 19.2672\n",
      "Epoch 121/1000\n",
      "409/409 [==============================] - 0s 18us/step - loss: 18.4783 - val_loss: 19.2362\n",
      "Epoch 122/1000\n",
      "409/409 [==============================] - 0s 19us/step - loss: 18.4479 - val_loss: 19.2065\n",
      "Epoch 123/1000\n",
      "409/409 [==============================] - 0s 21us/step - loss: 18.4180 - val_loss: 19.1783\n",
      "Epoch 124/1000\n",
      "409/409 [==============================] - 0s 19us/step - loss: 18.3877 - val_loss: 19.1504\n",
      "Epoch 125/1000\n",
      "409/409 [==============================] - 0s 19us/step - loss: 18.3574 - val_loss: 19.1247\n",
      "Epoch 126/1000\n",
      "409/409 [==============================] - 0s 30us/step - loss: 18.3268 - val_loss: 19.0995\n",
      "Epoch 127/1000\n",
      "409/409 [==============================] - 0s 24us/step - loss: 18.2965 - val_loss: 19.0746\n",
      "Epoch 128/1000\n",
      "409/409 [==============================] - 0s 24us/step - loss: 18.2656 - val_loss: 19.0511\n",
      "Epoch 129/1000\n",
      "409/409 [==============================] - 0s 26us/step - loss: 18.2360 - val_loss: 19.0275\n",
      "Epoch 130/1000\n",
      "409/409 [==============================] - 0s 27us/step - loss: 18.2046 - val_loss: 19.0012\n",
      "Epoch 131/1000\n",
      "409/409 [==============================] - 0s 37us/step - loss: 18.1740 - val_loss: 18.9712\n",
      "Epoch 132/1000\n",
      "409/409 [==============================] - 0s 66us/step - loss: 18.1438 - val_loss: 18.9390\n",
      "Epoch 133/1000\n",
      "409/409 [==============================] - 0s 30us/step - loss: 18.1131 - val_loss: 18.9073\n",
      "Epoch 134/1000\n",
      "409/409 [==============================] - 0s 34us/step - loss: 18.0850 - val_loss: 18.8766\n",
      "Epoch 135/1000\n",
      "409/409 [==============================] - 0s 29us/step - loss: 18.0548 - val_loss: 18.8496\n",
      "Epoch 136/1000\n",
      "409/409 [==============================] - 0s 40us/step - loss: 18.0239 - val_loss: 18.8240\n",
      "Epoch 137/1000\n",
      "409/409 [==============================] - 0s 39us/step - loss: 17.9936 - val_loss: 18.7969\n",
      "Epoch 138/1000\n",
      "409/409 [==============================] - 0s 27us/step - loss: 17.9631 - val_loss: 18.7682\n",
      "Epoch 139/1000\n",
      "409/409 [==============================] - 0s 23us/step - loss: 17.9328 - val_loss: 18.7364\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 140/1000\n",
      "409/409 [==============================] - 0s 28us/step - loss: 17.9034 - val_loss: 18.7040\n",
      "Epoch 141/1000\n",
      "409/409 [==============================] - 0s 31us/step - loss: 17.8736 - val_loss: 18.6745\n",
      "Epoch 142/1000\n",
      "409/409 [==============================] - 0s 29us/step - loss: 17.8434 - val_loss: 18.6454\n",
      "Epoch 143/1000\n",
      "409/409 [==============================] - 0s 26us/step - loss: 17.8133 - val_loss: 18.6142\n",
      "Epoch 144/1000\n",
      "409/409 [==============================] - 0s 27us/step - loss: 17.7835 - val_loss: 18.5839\n",
      "Epoch 145/1000\n",
      "409/409 [==============================] - 0s 17us/step - loss: 17.7537 - val_loss: 18.5537\n",
      "Epoch 146/1000\n",
      "409/409 [==============================] - 0s 24us/step - loss: 17.7238 - val_loss: 18.5242\n",
      "Epoch 147/1000\n",
      "409/409 [==============================] - 0s 23us/step - loss: 17.6937 - val_loss: 18.4915\n",
      "Epoch 148/1000\n",
      "409/409 [==============================] - 0s 19us/step - loss: 17.6639 - val_loss: 18.4611\n",
      "Epoch 149/1000\n",
      "409/409 [==============================] - 0s 23us/step - loss: 17.6341 - val_loss: 18.4289\n",
      "Epoch 150/1000\n",
      "409/409 [==============================] - 0s 31us/step - loss: 17.6046 - val_loss: 18.3960\n",
      "Epoch 151/1000\n",
      "409/409 [==============================] - 0s 23us/step - loss: 17.5746 - val_loss: 18.3633\n",
      "Epoch 152/1000\n",
      "409/409 [==============================] - 0s 19us/step - loss: 17.5449 - val_loss: 18.3302\n",
      "Epoch 153/1000\n",
      "409/409 [==============================] - 0s 27us/step - loss: 17.5155 - val_loss: 18.2981\n",
      "Epoch 154/1000\n",
      "409/409 [==============================] - 0s 25us/step - loss: 17.4862 - val_loss: 18.2676\n",
      "Epoch 155/1000\n",
      "409/409 [==============================] - 0s 23us/step - loss: 17.4561 - val_loss: 18.2397\n",
      "Epoch 156/1000\n",
      "409/409 [==============================] - 0s 19us/step - loss: 17.4252 - val_loss: 18.2147\n",
      "Epoch 157/1000\n",
      "409/409 [==============================] - 0s 23us/step - loss: 17.3951 - val_loss: 18.1886\n",
      "Epoch 158/1000\n",
      "409/409 [==============================] - 0s 36us/step - loss: 17.3641 - val_loss: 18.1620\n",
      "Epoch 159/1000\n",
      "409/409 [==============================] - 0s 26us/step - loss: 17.3338 - val_loss: 18.1327\n",
      "Epoch 160/1000\n",
      "409/409 [==============================] - 0s 21us/step - loss: 17.3035 - val_loss: 18.1048\n",
      "Epoch 161/1000\n",
      "409/409 [==============================] - 0s 21us/step - loss: 17.2732 - val_loss: 18.0797\n",
      "Epoch 162/1000\n",
      "409/409 [==============================] - 0s 27us/step - loss: 17.2426 - val_loss: 18.0555\n",
      "Epoch 163/1000\n",
      "409/409 [==============================] - 0s 24us/step - loss: 17.2130 - val_loss: 18.0283\n",
      "Epoch 164/1000\n",
      "409/409 [==============================] - 0s 39us/step - loss: 17.1820 - val_loss: 18.0010\n",
      "Epoch 165/1000\n",
      "409/409 [==============================] - 0s 26us/step - loss: 17.1517 - val_loss: 17.9760\n",
      "Epoch 166/1000\n",
      "409/409 [==============================] - 0s 24us/step - loss: 17.1207 - val_loss: 17.9534\n",
      "Epoch 167/1000\n",
      "409/409 [==============================] - 0s 21us/step - loss: 17.0894 - val_loss: 17.9342\n",
      "Epoch 168/1000\n",
      "409/409 [==============================] - 0s 21us/step - loss: 17.0581 - val_loss: 17.9176\n",
      "Epoch 169/1000\n",
      "409/409 [==============================] - 0s 23us/step - loss: 17.0283 - val_loss: 17.8978\n",
      "Epoch 170/1000\n",
      "409/409 [==============================] - 0s 23us/step - loss: 16.9965 - val_loss: 17.8707\n",
      "Epoch 171/1000\n",
      "409/409 [==============================] - 0s 24us/step - loss: 16.9660 - val_loss: 17.8431\n",
      "Epoch 172/1000\n",
      "409/409 [==============================] - 0s 19us/step - loss: 16.9357 - val_loss: 17.8172\n",
      "Epoch 173/1000\n",
      "409/409 [==============================] - 0s 22us/step - loss: 16.9048 - val_loss: 17.7941\n",
      "Epoch 174/1000\n",
      "409/409 [==============================] - 0s 24us/step - loss: 16.8739 - val_loss: 17.7708\n",
      "Epoch 175/1000\n",
      "409/409 [==============================] - 0s 28us/step - loss: 16.8442 - val_loss: 17.7494\n",
      "Epoch 176/1000\n",
      "409/409 [==============================] - 0s 31us/step - loss: 16.8134 - val_loss: 17.7268\n",
      "Epoch 177/1000\n",
      "409/409 [==============================] - 0s 28us/step - loss: 16.7817 - val_loss: 17.6988\n",
      "Epoch 178/1000\n",
      "409/409 [==============================] - 0s 30us/step - loss: 16.7515 - val_loss: 17.6692\n",
      "Epoch 179/1000\n",
      "409/409 [==============================] - 0s 21us/step - loss: 16.7218 - val_loss: 17.6391\n",
      "Epoch 180/1000\n",
      "409/409 [==============================] - 0s 27us/step - loss: 16.6920 - val_loss: 17.6100\n",
      "Epoch 181/1000\n",
      "409/409 [==============================] - 0s 27us/step - loss: 16.6614 - val_loss: 17.5816\n",
      "Epoch 182/1000\n",
      "409/409 [==============================] - 0s 24us/step - loss: 16.6314 - val_loss: 17.5513\n",
      "Epoch 183/1000\n",
      "409/409 [==============================] - 0s 21us/step - loss: 16.6017 - val_loss: 17.5234\n",
      "Epoch 184/1000\n",
      "409/409 [==============================] - 0s 22us/step - loss: 16.5711 - val_loss: 17.4950\n",
      "Epoch 185/1000\n",
      "409/409 [==============================] - 0s 27us/step - loss: 16.5406 - val_loss: 17.4666\n",
      "Epoch 186/1000\n",
      "409/409 [==============================] - 0s 27us/step - loss: 16.5104 - val_loss: 17.4394\n",
      "Epoch 187/1000\n",
      "409/409 [==============================] - 0s 41us/step - loss: 16.4806 - val_loss: 17.4121\n",
      "Epoch 188/1000\n",
      "409/409 [==============================] - ETA: 0s - loss: 16.52 - 0s 16us/step - loss: 16.4501 - val_loss: 17.3843\n",
      "Epoch 189/1000\n",
      "409/409 [==============================] - 0s 22us/step - loss: 16.4197 - val_loss: 17.3575\n",
      "Epoch 190/1000\n",
      "409/409 [==============================] - 0s 26us/step - loss: 16.3897 - val_loss: 17.3320\n",
      "Epoch 191/1000\n",
      "409/409 [==============================] - 0s 20us/step - loss: 16.3598 - val_loss: 17.3054\n",
      "Epoch 192/1000\n",
      "409/409 [==============================] - 0s 28us/step - loss: 16.3288 - val_loss: 17.2781\n",
      "Epoch 193/1000\n",
      "409/409 [==============================] - 0s 21us/step - loss: 16.2984 - val_loss: 17.2527\n",
      "Epoch 194/1000\n",
      "409/409 [==============================] - 0s 20us/step - loss: 16.2675 - val_loss: 17.2277\n",
      "Epoch 195/1000\n",
      "409/409 [==============================] - 0s 19us/step - loss: 16.2373 - val_loss: 17.2001\n",
      "Epoch 196/1000\n",
      "409/409 [==============================] - 0s 20us/step - loss: 16.2071 - val_loss: 17.1679\n",
      "Epoch 197/1000\n",
      "409/409 [==============================] - 0s 19us/step - loss: 16.1781 - val_loss: 17.1333\n",
      "Epoch 198/1000\n",
      "409/409 [==============================] - 0s 21us/step - loss: 16.1476 - val_loss: 17.1010\n",
      "Epoch 199/1000\n",
      "409/409 [==============================] - 0s 24us/step - loss: 16.1181 - val_loss: 17.0733\n",
      "Epoch 200/1000\n",
      "409/409 [==============================] - 0s 20us/step - loss: 16.0876 - val_loss: 17.0483\n",
      "Epoch 201/1000\n",
      "409/409 [==============================] - 0s 29us/step - loss: 16.0565 - val_loss: 17.0239\n",
      "Epoch 202/1000\n",
      "409/409 [==============================] - 0s 32us/step - loss: 16.0264 - val_loss: 16.9992\n",
      "Epoch 203/1000\n",
      "409/409 [==============================] - 0s 18us/step - loss: 15.9957 - val_loss: 16.9767\n",
      "Epoch 204/1000\n",
      "409/409 [==============================] - 0s 18us/step - loss: 15.9654 - val_loss: 16.9536\n",
      "Epoch 205/1000\n",
      "409/409 [==============================] - 0s 20us/step - loss: 15.9346 - val_loss: 16.9268\n",
      "Epoch 206/1000\n",
      "409/409 [==============================] - 0s 19us/step - loss: 15.9040 - val_loss: 16.9000\n",
      "Epoch 207/1000\n",
      "409/409 [==============================] - 0s 18us/step - loss: 15.8739 - val_loss: 16.8717\n",
      "Epoch 208/1000\n",
      "409/409 [==============================] - 0s 18us/step - loss: 15.8438 - val_loss: 16.8437\n",
      "Epoch 209/1000\n",
      "409/409 [==============================] - 0s 20us/step - loss: 15.8134 - val_loss: 16.8157\n",
      "Epoch 210/1000\n",
      "409/409 [==============================] - 0s 20us/step - loss: 15.7834 - val_loss: 16.7900\n",
      "Epoch 211/1000\n",
      "409/409 [==============================] - 0s 18us/step - loss: 15.7537 - val_loss: 16.7654\n",
      "Epoch 212/1000\n",
      "409/409 [==============================] - 0s 16us/step - loss: 15.7220 - val_loss: 16.7374\n",
      "Epoch 213/1000\n",
      "409/409 [==============================] - 0s 22us/step - loss: 15.6919 - val_loss: 16.7121\n",
      "Epoch 214/1000\n",
      "409/409 [==============================] - 0s 18us/step - loss: 15.6610 - val_loss: 16.6874\n",
      "Epoch 215/1000\n",
      "409/409 [==============================] - 0s 20us/step - loss: 15.6318 - val_loss: 16.6619\n",
      "Epoch 216/1000\n",
      "409/409 [==============================] - 0s 17us/step - loss: 15.6003 - val_loss: 16.6329\n",
      "Epoch 217/1000\n",
      "409/409 [==============================] - 0s 20us/step - loss: 15.5701 - val_loss: 16.6013\n",
      "Epoch 218/1000\n",
      "409/409 [==============================] - 0s 22us/step - loss: 15.5413 - val_loss: 16.5703\n",
      "Epoch 219/1000\n",
      "409/409 [==============================] - 0s 22us/step - loss: 15.5104 - val_loss: 16.5434\n",
      "Epoch 220/1000\n",
      "409/409 [==============================] - 0s 21us/step - loss: 15.4801 - val_loss: 16.5155\n",
      "Epoch 221/1000\n",
      "409/409 [==============================] - 0s 23us/step - loss: 15.4498 - val_loss: 16.4872\n",
      "Epoch 222/1000\n",
      "409/409 [==============================] - 0s 18us/step - loss: 15.4199 - val_loss: 16.4610\n",
      "Epoch 223/1000\n",
      "409/409 [==============================] - 0s 23us/step - loss: 15.3891 - val_loss: 16.4349\n",
      "Epoch 224/1000\n",
      "409/409 [==============================] - 0s 22us/step - loss: 15.3588 - val_loss: 16.4066\n",
      "Epoch 225/1000\n",
      "409/409 [==============================] - 0s 19us/step - loss: 15.3290 - val_loss: 16.3792\n",
      "Epoch 226/1000\n",
      "409/409 [==============================] - 0s 22us/step - loss: 15.2983 - val_loss: 16.3529\n",
      "Epoch 227/1000\n",
      "409/409 [==============================] - 0s 18us/step - loss: 15.2681 - val_loss: 16.3241\n",
      "Epoch 228/1000\n",
      "409/409 [==============================] - 0s 22us/step - loss: 15.2379 - val_loss: 16.2938\n",
      "Epoch 229/1000\n",
      "409/409 [==============================] - 0s 21us/step - loss: 15.2078 - val_loss: 16.2628\n",
      "Epoch 230/1000\n",
      "409/409 [==============================] - 0s 26us/step - loss: 15.1781 - val_loss: 16.2342\n",
      "Epoch 231/1000\n",
      "409/409 [==============================] - 0s 19us/step - loss: 15.1477 - val_loss: 16.2077\n",
      "Epoch 232/1000\n",
      "409/409 [==============================] - 0s 27us/step - loss: 15.1169 - val_loss: 16.1842\n",
      "Epoch 233/1000\n",
      "409/409 [==============================] - 0s 26us/step - loss: 15.0866 - val_loss: 16.1644\n",
      "Epoch 234/1000\n",
      "409/409 [==============================] - 0s 21us/step - loss: 15.0548 - val_loss: 16.1442\n",
      "Epoch 235/1000\n",
      "409/409 [==============================] - 0s 19us/step - loss: 15.0244 - val_loss: 16.1215\n",
      "Epoch 236/1000\n",
      "409/409 [==============================] - 0s 20us/step - loss: 14.9938 - val_loss: 16.0960\n",
      "Epoch 237/1000\n",
      "409/409 [==============================] - 0s 21us/step - loss: 14.9634 - val_loss: 16.0660\n",
      "Epoch 238/1000\n",
      "409/409 [==============================] - 0s 32us/step - loss: 14.9333 - val_loss: 16.0341\n",
      "Epoch 239/1000\n",
      "409/409 [==============================] - 0s 23us/step - loss: 14.9034 - val_loss: 16.0031\n",
      "Epoch 240/1000\n",
      "409/409 [==============================] - 0s 22us/step - loss: 14.8739 - val_loss: 15.9714\n",
      "Epoch 241/1000\n",
      "409/409 [==============================] - 0s 23us/step - loss: 14.8456 - val_loss: 15.9421\n",
      "Epoch 242/1000\n",
      "409/409 [==============================] - 0s 24us/step - loss: 14.8137 - val_loss: 15.9162\n",
      "Epoch 243/1000\n",
      "409/409 [==============================] - 0s 23us/step - loss: 14.7830 - val_loss: 15.8874\n",
      "Epoch 244/1000\n",
      "409/409 [==============================] - 0s 26us/step - loss: 14.7530 - val_loss: 15.8608\n",
      "Epoch 245/1000\n",
      "409/409 [==============================] - 0s 19us/step - loss: 14.7229 - val_loss: 15.8334\n",
      "Epoch 246/1000\n",
      "409/409 [==============================] - 0s 18us/step - loss: 14.6927 - val_loss: 15.8091\n",
      "Epoch 247/1000\n",
      "409/409 [==============================] - 0s 24us/step - loss: 14.6624 - val_loss: 15.7863\n",
      "Epoch 248/1000\n",
      "409/409 [==============================] - 0s 18us/step - loss: 14.6318 - val_loss: 15.7628\n",
      "Epoch 249/1000\n",
      "409/409 [==============================] - 0s 19us/step - loss: 14.6014 - val_loss: 15.7368\n",
      "Epoch 250/1000\n",
      "409/409 [==============================] - 0s 23us/step - loss: 14.5711 - val_loss: 15.7087\n",
      "Epoch 251/1000\n",
      "409/409 [==============================] - 0s 20us/step - loss: 14.5408 - val_loss: 15.6808\n",
      "Epoch 252/1000\n",
      "409/409 [==============================] - 0s 16us/step - loss: 14.5111 - val_loss: 15.6521\n",
      "Epoch 253/1000\n",
      "409/409 [==============================] - 0s 22us/step - loss: 14.4809 - val_loss: 15.6263\n",
      "Epoch 254/1000\n",
      "409/409 [==============================] - 0s 24us/step - loss: 14.4499 - val_loss: 15.6029\n",
      "Epoch 255/1000\n",
      "409/409 [==============================] - 0s 20us/step - loss: 14.4189 - val_loss: 15.5805\n",
      "Epoch 256/1000\n",
      "409/409 [==============================] - 0s 18us/step - loss: 14.3889 - val_loss: 15.5558\n",
      "Epoch 257/1000\n",
      "409/409 [==============================] - 0s 28us/step - loss: 14.3584 - val_loss: 15.5284\n",
      "Epoch 258/1000\n",
      "409/409 [==============================] - 0s 19us/step - loss: 14.3282 - val_loss: 15.5029\n",
      "Epoch 259/1000\n",
      "409/409 [==============================] - 0s 19us/step - loss: 14.2980 - val_loss: 15.4789\n",
      "Epoch 260/1000\n",
      "409/409 [==============================] - 0s 21us/step - loss: 14.2670 - val_loss: 15.4532\n",
      "Epoch 261/1000\n",
      "409/409 [==============================] - 0s 20us/step - loss: 14.2368 - val_loss: 15.4286\n",
      "Epoch 262/1000\n",
      "409/409 [==============================] - 0s 28us/step - loss: 14.2064 - val_loss: 15.4033\n",
      "Epoch 263/1000\n",
      "409/409 [==============================] - 0s 30us/step - loss: 14.1760 - val_loss: 15.3780\n",
      "Epoch 264/1000\n",
      "409/409 [==============================] - 0s 40us/step - loss: 14.1459 - val_loss: 15.3545\n",
      "Epoch 265/1000\n",
      "409/409 [==============================] - 0s 19us/step - loss: 14.1153 - val_loss: 15.3316\n",
      "Epoch 266/1000\n",
      "409/409 [==============================] - 0s 21us/step - loss: 14.0845 - val_loss: 15.3082\n",
      "Epoch 267/1000\n",
      "409/409 [==============================] - 0s 26us/step - loss: 14.0533 - val_loss: 15.2867\n",
      "Epoch 268/1000\n",
      "409/409 [==============================] - 0s 24us/step - loss: 14.0224 - val_loss: 15.2669\n",
      "Epoch 269/1000\n",
      "409/409 [==============================] - 0s 23us/step - loss: 13.9916 - val_loss: 15.2459\n",
      "Epoch 270/1000\n",
      "409/409 [==============================] - 0s 21us/step - loss: 13.9604 - val_loss: 15.2224\n",
      "Epoch 271/1000\n",
      "409/409 [==============================] - 0s 17us/step - loss: 13.9295 - val_loss: 15.2012\n",
      "Epoch 272/1000\n",
      "409/409 [==============================] - 0s 19us/step - loss: 13.8993 - val_loss: 15.1797\n",
      "Epoch 273/1000\n",
      "409/409 [==============================] - 0s 23us/step - loss: 13.8682 - val_loss: 15.1552\n",
      "Epoch 274/1000\n",
      "409/409 [==============================] - 0s 20us/step - loss: 13.8377 - val_loss: 15.1285\n",
      "Epoch 275/1000\n",
      "409/409 [==============================] - 0s 28us/step - loss: 13.8075 - val_loss: 15.0997\n",
      "Epoch 276/1000\n",
      "409/409 [==============================] - 0s 25us/step - loss: 13.7780 - val_loss: 15.0707\n",
      "Epoch 277/1000\n",
      "409/409 [==============================] - 0s 20us/step - loss: 13.7490 - val_loss: 15.0394\n",
      "Epoch 278/1000\n",
      "409/409 [==============================] - 0s 20us/step - loss: 13.7189 - val_loss: 15.0064\n",
      "Epoch 279/1000\n",
      "409/409 [==============================] - 0s 16us/step - loss: 13.6894 - val_loss: 14.9742\n",
      "Epoch 280/1000\n",
      "409/409 [==============================] - 0s 32us/step - loss: 13.6613 - val_loss: 14.9405\n",
      "Epoch 281/1000\n",
      "409/409 [==============================] - 0s 19us/step - loss: 13.6331 - val_loss: 14.9090\n",
      "Epoch 282/1000\n",
      "409/409 [==============================] - 0s 24us/step - loss: 13.6018 - val_loss: 14.8779\n",
      "Epoch 283/1000\n",
      "409/409 [==============================] - 0s 21us/step - loss: 13.5728 - val_loss: 14.8453\n",
      "Epoch 284/1000\n",
      "409/409 [==============================] - 0s 25us/step - loss: 13.5454 - val_loss: 14.8145\n",
      "Epoch 285/1000\n",
      "409/409 [==============================] - 0s 20us/step - loss: 13.5164 - val_loss: 14.7852\n",
      "Epoch 286/1000\n",
      "409/409 [==============================] - 0s 18us/step - loss: 13.4873 - val_loss: 14.7575\n",
      "Epoch 287/1000\n",
      "409/409 [==============================] - 0s 31us/step - loss: 13.4576 - val_loss: 14.7305\n",
      "Epoch 288/1000\n",
      "409/409 [==============================] - 0s 31us/step - loss: 13.4276 - val_loss: 14.7059\n",
      "Epoch 289/1000\n",
      "409/409 [==============================] - 0s 32us/step - loss: 13.3976 - val_loss: 14.6813\n",
      "Epoch 290/1000\n",
      "409/409 [==============================] - 0s 24us/step - loss: 13.3673 - val_loss: 14.6566\n",
      "Epoch 291/1000\n",
      "409/409 [==============================] - 0s 18us/step - loss: 13.3362 - val_loss: 14.6330\n",
      "Epoch 292/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "409/409 [==============================] - 0s 17us/step - loss: 13.3046 - val_loss: 14.6095\n",
      "Epoch 293/1000\n",
      "409/409 [==============================] - 0s 22us/step - loss: 13.2733 - val_loss: 14.5882\n",
      "Epoch 294/1000\n",
      "409/409 [==============================] - 0s 29us/step - loss: 13.2428 - val_loss: 14.5678\n",
      "Epoch 295/1000\n",
      "409/409 [==============================] - 0s 24us/step - loss: 13.2101 - val_loss: 14.5459\n",
      "Epoch 296/1000\n",
      "409/409 [==============================] - 0s 21us/step - loss: 13.1794 - val_loss: 14.5207\n",
      "Epoch 297/1000\n",
      "409/409 [==============================] - 0s 27us/step - loss: 13.1480 - val_loss: 14.4973\n",
      "Epoch 298/1000\n",
      "409/409 [==============================] - 0s 20us/step - loss: 13.1180 - val_loss: 14.4717\n",
      "Epoch 299/1000\n",
      "409/409 [==============================] - 0s 19us/step - loss: 13.0871 - val_loss: 14.4419\n",
      "Epoch 300/1000\n",
      "409/409 [==============================] - 0s 33us/step - loss: 13.0575 - val_loss: 14.4117\n",
      "Epoch 301/1000\n",
      "409/409 [==============================] - 0s 20us/step - loss: 13.0275 - val_loss: 14.3830\n",
      "Epoch 302/1000\n",
      "409/409 [==============================] - 0s 24us/step - loss: 12.9975 - val_loss: 14.3563\n",
      "Epoch 303/1000\n",
      "409/409 [==============================] - 0s 21us/step - loss: 12.9673 - val_loss: 14.3302\n",
      "Epoch 304/1000\n",
      "409/409 [==============================] - 0s 19us/step - loss: 12.9369 - val_loss: 14.3038\n",
      "Epoch 305/1000\n",
      "409/409 [==============================] - 0s 26us/step - loss: 12.9067 - val_loss: 14.2762\n",
      "Epoch 306/1000\n",
      "409/409 [==============================] - 0s 22us/step - loss: 12.8767 - val_loss: 14.2494\n",
      "Epoch 307/1000\n",
      "409/409 [==============================] - 0s 27us/step - loss: 12.8468 - val_loss: 14.2195\n",
      "Epoch 308/1000\n",
      "409/409 [==============================] - 0s 24us/step - loss: 12.8171 - val_loss: 14.1894\n",
      "Epoch 309/1000\n",
      "409/409 [==============================] - 0s 23us/step - loss: 12.7871 - val_loss: 14.1636\n",
      "Epoch 310/1000\n",
      "409/409 [==============================] - 0s 21us/step - loss: 12.7572 - val_loss: 14.1378\n",
      "Epoch 311/1000\n",
      "409/409 [==============================] - 0s 21us/step - loss: 12.7263 - val_loss: 14.1107\n",
      "Epoch 312/1000\n",
      "409/409 [==============================] - 0s 24us/step - loss: 12.6965 - val_loss: 14.0834\n",
      "Epoch 313/1000\n",
      "409/409 [==============================] - 0s 22us/step - loss: 12.6665 - val_loss: 14.0556\n",
      "Epoch 314/1000\n",
      "409/409 [==============================] - 0s 20us/step - loss: 12.6363 - val_loss: 14.0265\n",
      "Epoch 315/1000\n",
      "409/409 [==============================] - 0s 23us/step - loss: 12.6065 - val_loss: 13.9970\n",
      "Epoch 316/1000\n",
      "409/409 [==============================] - 0s 20us/step - loss: 12.5775 - val_loss: 13.9650\n",
      "Epoch 317/1000\n",
      "409/409 [==============================] - 0s 20us/step - loss: 12.5490 - val_loss: 13.9334\n",
      "Epoch 318/1000\n",
      "409/409 [==============================] - 0s 19us/step - loss: 12.5200 - val_loss: 13.9065\n",
      "Epoch 319/1000\n",
      "409/409 [==============================] - 0s 22us/step - loss: 12.4897 - val_loss: 13.8804\n",
      "Epoch 320/1000\n",
      "409/409 [==============================] - 0s 27us/step - loss: 12.4598 - val_loss: 13.8513\n",
      "Epoch 321/1000\n",
      "409/409 [==============================] - 0s 28us/step - loss: 12.4298 - val_loss: 13.8218\n",
      "Epoch 322/1000\n",
      "409/409 [==============================] - 0s 24us/step - loss: 12.4010 - val_loss: 13.7978\n",
      "Epoch 323/1000\n",
      "409/409 [==============================] - 0s 19us/step - loss: 12.3686 - val_loss: 13.7745\n",
      "Epoch 324/1000\n",
      "409/409 [==============================] - 0s 24us/step - loss: 12.3388 - val_loss: 13.7503\n",
      "Epoch 325/1000\n",
      "409/409 [==============================] - 0s 24us/step - loss: 12.3078 - val_loss: 13.7241\n",
      "Epoch 326/1000\n",
      "409/409 [==============================] - 0s 30us/step - loss: 12.2775 - val_loss: 13.6978\n",
      "Epoch 327/1000\n",
      "409/409 [==============================] - 0s 22us/step - loss: 12.2471 - val_loss: 13.6709\n",
      "Epoch 328/1000\n",
      "409/409 [==============================] - 0s 24us/step - loss: 12.2171 - val_loss: 13.6407\n",
      "Epoch 329/1000\n",
      "409/409 [==============================] - 0s 21us/step - loss: 12.1875 - val_loss: 13.6107\n",
      "Epoch 330/1000\n",
      "409/409 [==============================] - 0s 23us/step - loss: 12.1577 - val_loss: 13.5844\n",
      "Epoch 331/1000\n",
      "409/409 [==============================] - 0s 21us/step - loss: 12.1276 - val_loss: 13.5606\n",
      "Epoch 332/1000\n",
      "409/409 [==============================] - 0s 26us/step - loss: 12.0981 - val_loss: 13.5382\n",
      "Epoch 333/1000\n",
      "409/409 [==============================] - 0s 26us/step - loss: 12.0673 - val_loss: 13.5148\n",
      "Epoch 334/1000\n",
      "409/409 [==============================] - 0s 20us/step - loss: 12.0371 - val_loss: 13.4899\n",
      "Epoch 335/1000\n",
      "409/409 [==============================] - 0s 18us/step - loss: 12.0071 - val_loss: 13.4620\n",
      "Epoch 336/1000\n",
      "409/409 [==============================] - 0s 21us/step - loss: 11.9772 - val_loss: 13.4330\n",
      "Epoch 337/1000\n",
      "409/409 [==============================] - 0s 20us/step - loss: 11.9476 - val_loss: 13.4047\n",
      "Epoch 338/1000\n",
      "409/409 [==============================] - 0s 23us/step - loss: 11.9175 - val_loss: 13.3753\n",
      "Epoch 339/1000\n",
      "409/409 [==============================] - 0s 25us/step - loss: 11.8877 - val_loss: 13.3490\n",
      "Epoch 340/1000\n",
      "409/409 [==============================] - 0s 18us/step - loss: 11.8576 - val_loss: 13.3212\n",
      "Epoch 341/1000\n",
      "409/409 [==============================] - 0s 17us/step - loss: 11.8270 - val_loss: 13.2913\n",
      "Epoch 342/1000\n",
      "409/409 [==============================] - 0s 20us/step - loss: 11.7968 - val_loss: 13.2642\n",
      "Epoch 343/1000\n",
      "409/409 [==============================] - 0s 23us/step - loss: 11.7663 - val_loss: 13.2378\n",
      "Epoch 344/1000\n",
      "409/409 [==============================] - 0s 32us/step - loss: 11.7363 - val_loss: 13.2112\n",
      "Epoch 345/1000\n",
      "409/409 [==============================] - 0s 21us/step - loss: 11.7060 - val_loss: 13.1878\n",
      "Epoch 346/1000\n",
      "409/409 [==============================] - 0s 17us/step - loss: 11.6752 - val_loss: 13.1675\n",
      "Epoch 347/1000\n",
      "409/409 [==============================] - 0s 20us/step - loss: 11.6446 - val_loss: 13.1444\n",
      "Epoch 348/1000\n",
      "409/409 [==============================] - 0s 20us/step - loss: 11.6148 - val_loss: 13.1153\n",
      "Epoch 349/1000\n",
      "409/409 [==============================] - 0s 25us/step - loss: 11.5849 - val_loss: 13.0837\n",
      "Epoch 350/1000\n",
      "409/409 [==============================] - 0s 26us/step - loss: 11.5555 - val_loss: 13.0553\n",
      "Epoch 351/1000\n",
      "409/409 [==============================] - 0s 19us/step - loss: 11.5255 - val_loss: 13.0307\n",
      "Epoch 352/1000\n",
      "409/409 [==============================] - 0s 25us/step - loss: 11.4954 - val_loss: 13.0034\n",
      "Epoch 353/1000\n",
      "409/409 [==============================] - 0s 20us/step - loss: 11.4649 - val_loss: 12.9733\n",
      "Epoch 354/1000\n",
      "409/409 [==============================] - 0s 23us/step - loss: 11.4356 - val_loss: 12.9448\n",
      "Epoch 355/1000\n",
      "409/409 [==============================] - 0s 21us/step - loss: 11.4059 - val_loss: 12.9173\n",
      "Epoch 356/1000\n",
      "409/409 [==============================] - 0s 21us/step - loss: 11.3765 - val_loss: 12.8927\n",
      "Epoch 357/1000\n",
      "409/409 [==============================] - 0s 25us/step - loss: 11.3456 - val_loss: 12.8666\n",
      "Epoch 358/1000\n",
      "409/409 [==============================] - 0s 24us/step - loss: 11.3156 - val_loss: 12.8398\n",
      "Epoch 359/1000\n",
      "409/409 [==============================] - 0s 25us/step - loss: 11.2862 - val_loss: 12.8115\n",
      "Epoch 360/1000\n",
      "409/409 [==============================] - 0s 27us/step - loss: 11.2565 - val_loss: 12.7871\n",
      "Epoch 361/1000\n",
      "409/409 [==============================] - 0s 16us/step - loss: 11.2264 - val_loss: 12.7634\n",
      "Epoch 362/1000\n",
      "409/409 [==============================] - 0s 17us/step - loss: 11.1968 - val_loss: 12.7400\n",
      "Epoch 363/1000\n",
      "409/409 [==============================] - 0s 14us/step - loss: 11.1670 - val_loss: 12.7158\n",
      "Epoch 364/1000\n",
      "409/409 [==============================] - 0s 28us/step - loss: 11.1369 - val_loss: 12.6949\n",
      "Epoch 365/1000\n",
      "409/409 [==============================] - 0s 26us/step - loss: 11.1075 - val_loss: 12.6737\n",
      "Epoch 366/1000\n",
      "409/409 [==============================] - 0s 23us/step - loss: 11.0769 - val_loss: 12.6528\n",
      "Epoch 367/1000\n",
      "409/409 [==============================] - 0s 18us/step - loss: 11.0457 - val_loss: 12.6357\n",
      "Epoch 368/1000\n",
      "409/409 [==============================] - 0s 26us/step - loss: 11.0138 - val_loss: 12.6176\n",
      "Epoch 369/1000\n",
      "409/409 [==============================] - 0s 23us/step - loss: 10.9821 - val_loss: 12.5976\n",
      "Epoch 370/1000\n",
      "409/409 [==============================] - 0s 18us/step - loss: 10.9510 - val_loss: 12.5792\n",
      "Epoch 371/1000\n",
      "409/409 [==============================] - 0s 23us/step - loss: 10.9211 - val_loss: 12.5570\n",
      "Epoch 372/1000\n",
      "409/409 [==============================] - 0s 44us/step - loss: 10.8905 - val_loss: 12.5311\n",
      "Epoch 373/1000\n",
      "409/409 [==============================] - 0s 18us/step - loss: 10.8602 - val_loss: 12.5080\n",
      "Epoch 374/1000\n",
      "409/409 [==============================] - 0s 27us/step - loss: 10.8289 - val_loss: 12.4877\n",
      "Epoch 375/1000\n",
      "409/409 [==============================] - 0s 24us/step - loss: 10.7980 - val_loss: 12.4689\n",
      "Epoch 376/1000\n",
      "409/409 [==============================] - 0s 26us/step - loss: 10.7682 - val_loss: 12.4496\n",
      "Epoch 377/1000\n",
      "409/409 [==============================] - 0s 30us/step - loss: 10.7365 - val_loss: 12.4282\n",
      "Epoch 378/1000\n",
      "409/409 [==============================] - 0s 23us/step - loss: 10.7058 - val_loss: 12.4072\n",
      "Epoch 379/1000\n",
      "409/409 [==============================] - ETA: 0s - loss: 10.84 - 0s 16us/step - loss: 10.6753 - val_loss: 12.3841\n",
      "Epoch 380/1000\n",
      "409/409 [==============================] - 0s 28us/step - loss: 10.6446 - val_loss: 12.3587\n",
      "Epoch 381/1000\n",
      "409/409 [==============================] - 0s 25us/step - loss: 10.6143 - val_loss: 12.3328\n",
      "Epoch 382/1000\n",
      "409/409 [==============================] - 0s 18us/step - loss: 10.5844 - val_loss: 12.3028\n",
      "Epoch 383/1000\n",
      "409/409 [==============================] - 0s 25us/step - loss: 10.5542 - val_loss: 12.2719\n",
      "Epoch 384/1000\n",
      "409/409 [==============================] - 0s 25us/step - loss: 10.5250 - val_loss: 12.2416\n",
      "Epoch 385/1000\n",
      "409/409 [==============================] - 0s 28us/step - loss: 10.4948 - val_loss: 12.2164\n",
      "Epoch 386/1000\n",
      "409/409 [==============================] - 0s 26us/step - loss: 10.4641 - val_loss: 12.1921\n",
      "Epoch 387/1000\n",
      "409/409 [==============================] - 0s 28us/step - loss: 10.4348 - val_loss: 12.1655\n",
      "Epoch 388/1000\n",
      "409/409 [==============================] - 0s 17us/step - loss: 10.4050 - val_loss: 12.1430\n",
      "Epoch 389/1000\n",
      "409/409 [==============================] - 0s 23us/step - loss: 10.3759 - val_loss: 12.1206\n",
      "Epoch 390/1000\n",
      "409/409 [==============================] - 0s 21us/step - loss: 10.3456 - val_loss: 12.0971\n",
      "Epoch 391/1000\n",
      "409/409 [==============================] - 0s 19us/step - loss: 10.3157 - val_loss: 12.0747\n",
      "Epoch 392/1000\n",
      "409/409 [==============================] - 0s 21us/step - loss: 10.2862 - val_loss: 12.0543\n",
      "Epoch 393/1000\n",
      "409/409 [==============================] - 0s 27us/step - loss: 10.2570 - val_loss: 12.0304\n",
      "Epoch 394/1000\n",
      "409/409 [==============================] - 0s 23us/step - loss: 10.2274 - val_loss: 12.0006\n",
      "Epoch 395/1000\n",
      "409/409 [==============================] - 0s 23us/step - loss: 10.1989 - val_loss: 11.9717\n",
      "Epoch 396/1000\n",
      "409/409 [==============================] - 0s 24us/step - loss: 10.1688 - val_loss: 11.9472\n",
      "Epoch 397/1000\n",
      "409/409 [==============================] - 0s 20us/step - loss: 10.1391 - val_loss: 11.9261\n",
      "Epoch 398/1000\n",
      "409/409 [==============================] - 0s 27us/step - loss: 10.1097 - val_loss: 11.9056\n",
      "Epoch 399/1000\n",
      "409/409 [==============================] - 0s 27us/step - loss: 10.0795 - val_loss: 11.8843\n",
      "Epoch 400/1000\n",
      "409/409 [==============================] - 0s 22us/step - loss: 10.0507 - val_loss: 11.8601\n",
      "Epoch 401/1000\n",
      "409/409 [==============================] - 0s 28us/step - loss: 10.0218 - val_loss: 11.8361\n",
      "Epoch 402/1000\n",
      "409/409 [==============================] - 0s 19us/step - loss: 9.9930 - val_loss: 11.8142\n",
      "Epoch 403/1000\n",
      "409/409 [==============================] - 0s 17us/step - loss: 9.9646 - val_loss: 11.7939\n",
      "Epoch 404/1000\n",
      "409/409 [==============================] - 0s 20us/step - loss: 9.9361 - val_loss: 11.7760\n",
      "Epoch 405/1000\n",
      "409/409 [==============================] - 0s 17us/step - loss: 9.9061 - val_loss: 11.7580\n",
      "Epoch 406/1000\n",
      "409/409 [==============================] - 0s 32us/step - loss: 9.8781 - val_loss: 11.7405\n",
      "Epoch 407/1000\n",
      "409/409 [==============================] - 0s 26us/step - loss: 9.8487 - val_loss: 11.7219\n",
      "Epoch 408/1000\n",
      "409/409 [==============================] - 0s 22us/step - loss: 9.8201 - val_loss: 11.7014\n",
      "Epoch 409/1000\n",
      "409/409 [==============================] - 0s 23us/step - loss: 9.7907 - val_loss: 11.6783\n",
      "Epoch 410/1000\n",
      "409/409 [==============================] - 0s 25us/step - loss: 9.7609 - val_loss: 11.6541\n",
      "Epoch 411/1000\n",
      "409/409 [==============================] - 0s 16us/step - loss: 9.7316 - val_loss: 11.6310\n",
      "Epoch 412/1000\n",
      "409/409 [==============================] - 0s 21us/step - loss: 9.7027 - val_loss: 11.6092\n",
      "Epoch 413/1000\n",
      "409/409 [==============================] - 0s 16us/step - loss: 9.6741 - val_loss: 11.5872\n",
      "Epoch 414/1000\n",
      "409/409 [==============================] - 0s 22us/step - loss: 9.6460 - val_loss: 11.5631\n",
      "Epoch 415/1000\n",
      "409/409 [==============================] - 0s 24us/step - loss: 9.6176 - val_loss: 11.5396\n",
      "Epoch 416/1000\n",
      "409/409 [==============================] - 0s 19us/step - loss: 9.5896 - val_loss: 11.5158\n",
      "Epoch 417/1000\n",
      "409/409 [==============================] - 0s 22us/step - loss: 9.5611 - val_loss: 11.4933\n",
      "Epoch 418/1000\n",
      "409/409 [==============================] - 0s 26us/step - loss: 9.5344 - val_loss: 11.4710\n",
      "Epoch 419/1000\n",
      "409/409 [==============================] - 0s 29us/step - loss: 9.5074 - val_loss: 11.4472\n",
      "Epoch 420/1000\n",
      "409/409 [==============================] - 0s 21us/step - loss: 9.4800 - val_loss: 11.4218\n",
      "Epoch 421/1000\n",
      "409/409 [==============================] - 0s 20us/step - loss: 9.4516 - val_loss: 11.3997\n",
      "Epoch 422/1000\n",
      "409/409 [==============================] - 0s 24us/step - loss: 9.4238 - val_loss: 11.3799\n",
      "Epoch 423/1000\n",
      "409/409 [==============================] - 0s 22us/step - loss: 9.3960 - val_loss: 11.3586\n",
      "Epoch 424/1000\n",
      "409/409 [==============================] - 0s 18us/step - loss: 9.3674 - val_loss: 11.3348\n",
      "Epoch 425/1000\n",
      "409/409 [==============================] - 0s 22us/step - loss: 9.3398 - val_loss: 11.3064\n",
      "Epoch 426/1000\n",
      "409/409 [==============================] - 0s 18us/step - loss: 9.3099 - val_loss: 11.2774\n",
      "Epoch 427/1000\n",
      "409/409 [==============================] - 0s 19us/step - loss: 9.2802 - val_loss: 11.2498\n",
      "Epoch 428/1000\n",
      "409/409 [==============================] - 0s 19us/step - loss: 9.2508 - val_loss: 11.2226\n",
      "Epoch 429/1000\n",
      "409/409 [==============================] - 0s 20us/step - loss: 9.2213 - val_loss: 11.1960\n",
      "Epoch 430/1000\n",
      "409/409 [==============================] - 0s 25us/step - loss: 9.1920 - val_loss: 11.1674\n",
      "Epoch 431/1000\n",
      "409/409 [==============================] - 0s 20us/step - loss: 9.1627 - val_loss: 11.1328\n",
      "Epoch 432/1000\n",
      "409/409 [==============================] - 0s 22us/step - loss: 9.1318 - val_loss: 11.1026\n",
      "Epoch 433/1000\n",
      "409/409 [==============================] - 0s 21us/step - loss: 9.1026 - val_loss: 11.0724\n",
      "Epoch 434/1000\n",
      "409/409 [==============================] - 0s 21us/step - loss: 9.0726 - val_loss: 11.0413\n",
      "Epoch 435/1000\n",
      "409/409 [==============================] - 0s 22us/step - loss: 9.0426 - val_loss: 11.0068\n",
      "Epoch 436/1000\n",
      "409/409 [==============================] - 0s 20us/step - loss: 9.0134 - val_loss: 10.9672\n",
      "Epoch 437/1000\n",
      "409/409 [==============================] - 0s 20us/step - loss: 8.9856 - val_loss: 10.9260\n",
      "Epoch 438/1000\n",
      "409/409 [==============================] - ETA: 0s - loss: 8.755 - 0s 22us/step - loss: 8.9558 - val_loss: 10.8835\n",
      "Epoch 439/1000\n",
      "409/409 [==============================] - 0s 20us/step - loss: 8.9284 - val_loss: 10.8437\n",
      "Epoch 440/1000\n",
      "409/409 [==============================] - 0s 27us/step - loss: 8.8992 - val_loss: 10.8081\n",
      "Epoch 441/1000\n",
      "409/409 [==============================] - 0s 29us/step - loss: 8.8705 - val_loss: 10.7771\n",
      "Epoch 442/1000\n",
      "409/409 [==============================] - 0s 16us/step - loss: 8.8427 - val_loss: 10.7454\n",
      "Epoch 443/1000\n",
      "409/409 [==============================] - 0s 23us/step - loss: 8.8155 - val_loss: 10.7127\n",
      "Epoch 444/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "409/409 [==============================] - 0s 18us/step - loss: 8.7865 - val_loss: 10.6840\n",
      "Epoch 445/1000\n",
      "409/409 [==============================] - 0s 22us/step - loss: 8.7581 - val_loss: 10.6549\n",
      "Epoch 446/1000\n",
      "409/409 [==============================] - 0s 24us/step - loss: 8.7293 - val_loss: 10.6248\n",
      "Epoch 447/1000\n",
      "409/409 [==============================] - 0s 17us/step - loss: 8.7007 - val_loss: 10.5964\n",
      "Epoch 448/1000\n",
      "409/409 [==============================] - 0s 27us/step - loss: 8.6722 - val_loss: 10.5687\n",
      "Epoch 449/1000\n",
      "409/409 [==============================] - 0s 25us/step - loss: 8.6442 - val_loss: 10.5403\n",
      "Epoch 450/1000\n",
      "409/409 [==============================] - 0s 23us/step - loss: 8.6163 - val_loss: 10.5110\n",
      "Epoch 451/1000\n",
      "409/409 [==============================] - 0s 21us/step - loss: 8.5886 - val_loss: 10.4774\n",
      "Epoch 452/1000\n",
      "409/409 [==============================] - 0s 25us/step - loss: 8.5609 - val_loss: 10.4434\n",
      "Epoch 453/1000\n",
      "409/409 [==============================] - 0s 23us/step - loss: 8.5338 - val_loss: 10.4124\n",
      "Epoch 454/1000\n",
      "409/409 [==============================] - 0s 21us/step - loss: 8.5072 - val_loss: 10.3824\n",
      "Epoch 455/1000\n",
      "409/409 [==============================] - 0s 18us/step - loss: 8.4798 - val_loss: 10.3518\n",
      "Epoch 456/1000\n",
      "409/409 [==============================] - 0s 20us/step - loss: 8.4527 - val_loss: 10.3204\n",
      "Epoch 457/1000\n",
      "409/409 [==============================] - 0s 19us/step - loss: 8.4247 - val_loss: 10.2891\n",
      "Epoch 458/1000\n",
      "409/409 [==============================] - 0s 21us/step - loss: 8.3979 - val_loss: 10.2584\n",
      "Epoch 459/1000\n",
      "409/409 [==============================] - 0s 21us/step - loss: 8.3700 - val_loss: 10.2315\n",
      "Epoch 460/1000\n",
      "409/409 [==============================] - 0s 20us/step - loss: 8.3424 - val_loss: 10.2032\n",
      "Epoch 461/1000\n",
      "409/409 [==============================] - 0s 20us/step - loss: 8.3136 - val_loss: 10.1728\n",
      "Epoch 462/1000\n",
      "409/409 [==============================] - 0s 20us/step - loss: 8.2856 - val_loss: 10.1415\n",
      "Epoch 463/1000\n",
      "409/409 [==============================] - 0s 18us/step - loss: 8.2581 - val_loss: 10.1074\n",
      "Epoch 464/1000\n",
      "409/409 [==============================] - 0s 20us/step - loss: 8.2309 - val_loss: 10.0767\n",
      "Epoch 465/1000\n",
      "409/409 [==============================] - 0s 20us/step - loss: 8.2028 - val_loss: 10.0493\n",
      "Epoch 466/1000\n",
      "409/409 [==============================] - 0s 22us/step - loss: 8.1758 - val_loss: 10.0231\n",
      "Epoch 467/1000\n",
      "409/409 [==============================] - 0s 19us/step - loss: 8.1482 - val_loss: 9.9977\n",
      "Epoch 468/1000\n",
      "409/409 [==============================] - 0s 23us/step - loss: 8.1213 - val_loss: 9.9726\n",
      "Epoch 469/1000\n",
      "409/409 [==============================] - 0s 23us/step - loss: 8.0947 - val_loss: 9.9484\n",
      "Epoch 470/1000\n",
      "409/409 [==============================] - 0s 20us/step - loss: 8.0673 - val_loss: 9.9243\n",
      "Epoch 471/1000\n",
      "409/409 [==============================] - 0s 24us/step - loss: 8.0391 - val_loss: 9.8996\n",
      "Epoch 472/1000\n",
      "409/409 [==============================] - 0s 31us/step - loss: 8.0119 - val_loss: 9.8737\n",
      "Epoch 473/1000\n",
      "409/409 [==============================] - 0s 34us/step - loss: 7.9841 - val_loss: 9.8483\n",
      "Epoch 474/1000\n",
      "409/409 [==============================] - 0s 23us/step - loss: 7.9566 - val_loss: 9.8238\n",
      "Epoch 475/1000\n",
      "409/409 [==============================] - 0s 25us/step - loss: 7.9292 - val_loss: 9.7976\n",
      "Epoch 476/1000\n",
      "409/409 [==============================] - 0s 29us/step - loss: 7.9021 - val_loss: 9.7711\n",
      "Epoch 477/1000\n",
      "409/409 [==============================] - 0s 22us/step - loss: 7.8752 - val_loss: 9.7477\n",
      "Epoch 478/1000\n",
      "409/409 [==============================] - 0s 17us/step - loss: 7.8464 - val_loss: 9.7231\n",
      "Epoch 479/1000\n",
      "409/409 [==============================] - 0s 19us/step - loss: 7.8192 - val_loss: 9.6971\n",
      "Epoch 480/1000\n",
      "409/409 [==============================] - 0s 28us/step - loss: 7.7911 - val_loss: 9.6719\n",
      "Epoch 481/1000\n",
      "409/409 [==============================] - 0s 19us/step - loss: 7.7638 - val_loss: 9.6475\n",
      "Epoch 482/1000\n",
      "409/409 [==============================] - 0s 26us/step - loss: 7.7373 - val_loss: 9.6200\n",
      "Epoch 483/1000\n",
      "409/409 [==============================] - 0s 24us/step - loss: 7.7093 - val_loss: 9.5898\n",
      "Epoch 484/1000\n",
      "409/409 [==============================] - 0s 31us/step - loss: 7.6813 - val_loss: 9.5611\n",
      "Epoch 485/1000\n",
      "409/409 [==============================] - 0s 20us/step - loss: 7.6546 - val_loss: 9.5327\n",
      "Epoch 486/1000\n",
      "409/409 [==============================] - 0s 22us/step - loss: 7.6266 - val_loss: 9.5065\n",
      "Epoch 487/1000\n",
      "409/409 [==============================] - 0s 25us/step - loss: 7.5987 - val_loss: 9.4811\n",
      "Epoch 488/1000\n",
      "409/409 [==============================] - 0s 18us/step - loss: 7.5713 - val_loss: 9.4562\n",
      "Epoch 489/1000\n",
      "409/409 [==============================] - 0s 23us/step - loss: 7.5445 - val_loss: 9.4343\n",
      "Epoch 490/1000\n",
      "409/409 [==============================] - 0s 21us/step - loss: 7.5173 - val_loss: 9.4112\n",
      "Epoch 491/1000\n",
      "409/409 [==============================] - 0s 25us/step - loss: 7.4896 - val_loss: 9.3847\n",
      "Epoch 492/1000\n",
      "409/409 [==============================] - 0s 21us/step - loss: 7.4621 - val_loss: 9.3587\n",
      "Epoch 493/1000\n",
      "409/409 [==============================] - 0s 21us/step - loss: 7.4345 - val_loss: 9.3318\n",
      "Epoch 494/1000\n",
      "409/409 [==============================] - 0s 20us/step - loss: 7.4078 - val_loss: 9.3017\n",
      "Epoch 495/1000\n",
      "409/409 [==============================] - 0s 23us/step - loss: 7.3803 - val_loss: 9.2692\n",
      "Epoch 496/1000\n",
      "409/409 [==============================] - 0s 25us/step - loss: 7.3541 - val_loss: 9.2391\n",
      "Epoch 497/1000\n",
      "409/409 [==============================] - 0s 21us/step - loss: 7.3277 - val_loss: 9.2119\n",
      "Epoch 498/1000\n",
      "409/409 [==============================] - 0s 21us/step - loss: 7.3010 - val_loss: 9.1842\n",
      "Epoch 499/1000\n",
      "409/409 [==============================] - 0s 22us/step - loss: 7.2743 - val_loss: 9.1570\n",
      "Epoch 500/1000\n",
      "409/409 [==============================] - 0s 19us/step - loss: 7.2479 - val_loss: 9.1327\n",
      "Epoch 501/1000\n",
      "409/409 [==============================] - 0s 27us/step - loss: 7.2203 - val_loss: 9.1082\n",
      "Epoch 502/1000\n",
      "409/409 [==============================] - 0s 20us/step - loss: 7.1925 - val_loss: 9.0813\n",
      "Epoch 503/1000\n",
      "409/409 [==============================] - 0s 22us/step - loss: 7.1638 - val_loss: 9.0536\n",
      "Epoch 504/1000\n",
      "409/409 [==============================] - 0s 34us/step - loss: 7.1370 - val_loss: 9.0272\n",
      "Epoch 505/1000\n",
      "409/409 [==============================] - 0s 17us/step - loss: 7.1095 - val_loss: 9.0037\n",
      "Epoch 506/1000\n",
      "409/409 [==============================] - 0s 27us/step - loss: 7.0834 - val_loss: 8.9813\n",
      "Epoch 507/1000\n",
      "409/409 [==============================] - 0s 21us/step - loss: 7.0572 - val_loss: 8.9578\n",
      "Epoch 508/1000\n",
      "409/409 [==============================] - 0s 23us/step - loss: 7.0305 - val_loss: 8.9330\n",
      "Epoch 509/1000\n",
      "409/409 [==============================] - 0s 21us/step - loss: 7.0038 - val_loss: 8.9061\n",
      "Epoch 510/1000\n",
      "409/409 [==============================] - 0s 18us/step - loss: 6.9781 - val_loss: 8.8792\n",
      "Epoch 511/1000\n",
      "409/409 [==============================] - 0s 24us/step - loss: 6.9532 - val_loss: 8.8526\n",
      "Epoch 512/1000\n",
      "409/409 [==============================] - 0s 19us/step - loss: 6.9281 - val_loss: 8.8264\n",
      "Epoch 513/1000\n",
      "409/409 [==============================] - 0s 20us/step - loss: 6.9033 - val_loss: 8.7991\n",
      "Epoch 514/1000\n",
      "409/409 [==============================] - 0s 25us/step - loss: 6.8783 - val_loss: 8.7731\n",
      "Epoch 515/1000\n",
      "409/409 [==============================] - 0s 22us/step - loss: 6.8529 - val_loss: 8.7451\n",
      "Epoch 516/1000\n",
      "409/409 [==============================] - 0s 23us/step - loss: 6.8269 - val_loss: 8.7144\n",
      "Epoch 517/1000\n",
      "409/409 [==============================] - 0s 25us/step - loss: 6.8013 - val_loss: 8.6845\n",
      "Epoch 518/1000\n",
      "409/409 [==============================] - 0s 20us/step - loss: 6.7766 - val_loss: 8.6581\n",
      "Epoch 519/1000\n",
      "409/409 [==============================] - 0s 29us/step - loss: 6.7518 - val_loss: 8.6328\n",
      "Epoch 520/1000\n",
      "409/409 [==============================] - 0s 26us/step - loss: 6.7276 - val_loss: 8.6065\n",
      "Epoch 521/1000\n",
      "409/409 [==============================] - 0s 30us/step - loss: 6.7031 - val_loss: 8.5787\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 522/1000\n",
      "409/409 [==============================] - 0s 24us/step - loss: 6.6787 - val_loss: 8.5502\n",
      "Epoch 523/1000\n",
      "409/409 [==============================] - 0s 29us/step - loss: 6.6526 - val_loss: 8.5227\n",
      "Epoch 524/1000\n",
      "409/409 [==============================] - 0s 35us/step - loss: 6.6286 - val_loss: 8.4939\n",
      "Epoch 525/1000\n",
      "409/409 [==============================] - 0s 26us/step - loss: 6.6030 - val_loss: 8.4642\n",
      "Epoch 526/1000\n",
      "409/409 [==============================] - 0s 21us/step - loss: 6.5783 - val_loss: 8.4354\n",
      "Epoch 527/1000\n",
      "409/409 [==============================] - 0s 25us/step - loss: 6.5527 - val_loss: 8.4061\n",
      "Epoch 528/1000\n",
      "409/409 [==============================] - 0s 18us/step - loss: 6.5264 - val_loss: 8.3780\n",
      "Epoch 529/1000\n",
      "409/409 [==============================] - 0s 24us/step - loss: 6.4989 - val_loss: 8.3501\n",
      "Epoch 530/1000\n",
      "409/409 [==============================] - 0s 23us/step - loss: 6.4723 - val_loss: 8.3221\n",
      "Epoch 531/1000\n",
      "409/409 [==============================] - 0s 17us/step - loss: 6.4445 - val_loss: 8.2924\n",
      "Epoch 532/1000\n",
      "409/409 [==============================] - 0s 19us/step - loss: 6.4208 - val_loss: 8.2625\n",
      "Epoch 533/1000\n",
      "409/409 [==============================] - 0s 17us/step - loss: 6.3954 - val_loss: 8.2331\n",
      "Epoch 534/1000\n",
      "409/409 [==============================] - 0s 19us/step - loss: 6.3661 - val_loss: 8.2039\n",
      "Epoch 535/1000\n",
      "409/409 [==============================] - 0s 24us/step - loss: 6.3401 - val_loss: 8.1759\n",
      "Epoch 536/1000\n",
      "409/409 [==============================] - 0s 17us/step - loss: 6.3141 - val_loss: 8.1497\n",
      "Epoch 537/1000\n",
      "409/409 [==============================] - 0s 19us/step - loss: 6.2880 - val_loss: 8.1243\n",
      "Epoch 538/1000\n",
      "409/409 [==============================] - 0s 20us/step - loss: 6.2633 - val_loss: 8.1000\n",
      "Epoch 539/1000\n",
      "409/409 [==============================] - 0s 17us/step - loss: 6.2374 - val_loss: 8.0775\n",
      "Epoch 540/1000\n",
      "409/409 [==============================] - 0s 21us/step - loss: 6.2124 - val_loss: 8.0566\n",
      "Epoch 541/1000\n",
      "409/409 [==============================] - 0s 20us/step - loss: 6.1868 - val_loss: 8.0336\n",
      "Epoch 542/1000\n",
      "409/409 [==============================] - 0s 22us/step - loss: 6.1616 - val_loss: 8.0062\n",
      "Epoch 543/1000\n",
      "409/409 [==============================] - 0s 19us/step - loss: 6.1369 - val_loss: 7.9803\n",
      "Epoch 544/1000\n",
      "409/409 [==============================] - 0s 22us/step - loss: 6.1100 - val_loss: 7.9550\n",
      "Epoch 545/1000\n",
      "409/409 [==============================] - ETA: 0s - loss: 5.979 - 0s 20us/step - loss: 6.0826 - val_loss: 7.9277\n",
      "Epoch 546/1000\n",
      "409/409 [==============================] - 0s 22us/step - loss: 6.0557 - val_loss: 7.8998\n",
      "Epoch 547/1000\n",
      "409/409 [==============================] - 0s 18us/step - loss: 6.0267 - val_loss: 7.8740\n",
      "Epoch 548/1000\n",
      "409/409 [==============================] - 0s 17us/step - loss: 6.0018 - val_loss: 7.8483\n",
      "Epoch 549/1000\n",
      "409/409 [==============================] - 0s 22us/step - loss: 5.9770 - val_loss: 7.8234\n",
      "Epoch 550/1000\n",
      "409/409 [==============================] - 0s 24us/step - loss: 5.9521 - val_loss: 7.7965\n",
      "Epoch 551/1000\n",
      "409/409 [==============================] - 0s 25us/step - loss: 5.9267 - val_loss: 7.7669\n",
      "Epoch 552/1000\n",
      "409/409 [==============================] - 0s 19us/step - loss: 5.8988 - val_loss: 7.7384\n",
      "Epoch 553/1000\n",
      "409/409 [==============================] - 0s 16us/step - loss: 5.8710 - val_loss: 7.7108\n",
      "Epoch 554/1000\n",
      "409/409 [==============================] - 0s 21us/step - loss: 5.8438 - val_loss: 7.6841\n",
      "Epoch 555/1000\n",
      "409/409 [==============================] - 0s 22us/step - loss: 5.8172 - val_loss: 7.6579\n",
      "Epoch 556/1000\n",
      "409/409 [==============================] - 0s 20us/step - loss: 5.7898 - val_loss: 7.6311\n",
      "Epoch 557/1000\n",
      "409/409 [==============================] - 0s 18us/step - loss: 5.7645 - val_loss: 7.6031\n",
      "Epoch 558/1000\n",
      "409/409 [==============================] - 0s 17us/step - loss: 5.7386 - val_loss: 7.5778\n",
      "Epoch 559/1000\n",
      "409/409 [==============================] - 0s 19us/step - loss: 5.7147 - val_loss: 7.5533\n",
      "Epoch 560/1000\n",
      "409/409 [==============================] - 0s 24us/step - loss: 5.6886 - val_loss: 7.5286\n",
      "Epoch 561/1000\n",
      "409/409 [==============================] - 0s 23us/step - loss: 5.6651 - val_loss: 7.5043\n",
      "Epoch 562/1000\n",
      "409/409 [==============================] - 0s 25us/step - loss: 5.6394 - val_loss: 7.4785\n",
      "Epoch 563/1000\n",
      "409/409 [==============================] - 0s 18us/step - loss: 5.6131 - val_loss: 7.4525\n",
      "Epoch 564/1000\n",
      "409/409 [==============================] - 0s 19us/step - loss: 5.5877 - val_loss: 7.4269\n",
      "Epoch 565/1000\n",
      "409/409 [==============================] - 0s 19us/step - loss: 5.5613 - val_loss: 7.4025\n",
      "Epoch 566/1000\n",
      "409/409 [==============================] - 0s 20us/step - loss: 5.5381 - val_loss: 7.3785\n",
      "Epoch 567/1000\n",
      "409/409 [==============================] - 0s 20us/step - loss: 5.5136 - val_loss: 7.3528\n",
      "Epoch 568/1000\n",
      "409/409 [==============================] - 0s 23us/step - loss: 5.4887 - val_loss: 7.3267\n",
      "Epoch 569/1000\n",
      "409/409 [==============================] - 0s 30us/step - loss: 5.4633 - val_loss: 7.3025\n",
      "Epoch 570/1000\n",
      "409/409 [==============================] - 0s 26us/step - loss: 5.4367 - val_loss: 7.2789\n",
      "Epoch 571/1000\n",
      "409/409 [==============================] - 0s 25us/step - loss: 5.4116 - val_loss: 7.2537\n",
      "Epoch 572/1000\n",
      "409/409 [==============================] - 0s 20us/step - loss: 5.3863 - val_loss: 7.2279\n",
      "Epoch 573/1000\n",
      "409/409 [==============================] - 0s 24us/step - loss: 5.3623 - val_loss: 7.2007\n",
      "Epoch 574/1000\n",
      "409/409 [==============================] - 0s 17us/step - loss: 5.3362 - val_loss: 7.1729\n",
      "Epoch 575/1000\n",
      "409/409 [==============================] - 0s 22us/step - loss: 5.3120 - val_loss: 7.1435\n",
      "Epoch 576/1000\n",
      "409/409 [==============================] - 0s 20us/step - loss: 5.2881 - val_loss: 7.1150\n",
      "Epoch 577/1000\n",
      "409/409 [==============================] - 0s 28us/step - loss: 5.2630 - val_loss: 7.0898\n",
      "Epoch 578/1000\n",
      "409/409 [==============================] - 0s 27us/step - loss: 5.2400 - val_loss: 7.0649\n",
      "Epoch 579/1000\n",
      "409/409 [==============================] - 0s 29us/step - loss: 5.2158 - val_loss: 7.0404\n",
      "Epoch 580/1000\n",
      "409/409 [==============================] - 0s 19us/step - loss: 5.1923 - val_loss: 7.0164\n",
      "Epoch 581/1000\n",
      "409/409 [==============================] - 0s 21us/step - loss: 5.1690 - val_loss: 6.9927\n",
      "Epoch 582/1000\n",
      "409/409 [==============================] - 0s 22us/step - loss: 5.1453 - val_loss: 6.9692\n",
      "Epoch 583/1000\n",
      "409/409 [==============================] - 0s 20us/step - loss: 5.1223 - val_loss: 6.9461\n",
      "Epoch 584/1000\n",
      "409/409 [==============================] - 0s 21us/step - loss: 5.0997 - val_loss: 6.9219\n",
      "Epoch 585/1000\n",
      "409/409 [==============================] - 0s 25us/step - loss: 5.0753 - val_loss: 6.8983\n",
      "Epoch 586/1000\n",
      "409/409 [==============================] - 0s 23us/step - loss: 5.0516 - val_loss: 6.8741\n",
      "Epoch 587/1000\n",
      "409/409 [==============================] - 0s 23us/step - loss: 5.0288 - val_loss: 6.8487\n",
      "Epoch 588/1000\n",
      "409/409 [==============================] - 0s 18us/step - loss: 5.0077 - val_loss: 6.8247\n",
      "Epoch 589/1000\n",
      "409/409 [==============================] - 0s 19us/step - loss: 4.9873 - val_loss: 6.8035\n",
      "Epoch 590/1000\n",
      "409/409 [==============================] - 0s 21us/step - loss: 4.9676 - val_loss: 6.7830\n",
      "Epoch 591/1000\n",
      "409/409 [==============================] - 0s 20us/step - loss: 4.9465 - val_loss: 6.7622\n",
      "Epoch 592/1000\n",
      "409/409 [==============================] - 0s 22us/step - loss: 4.9249 - val_loss: 6.7410\n",
      "Epoch 593/1000\n",
      "409/409 [==============================] - 0s 22us/step - loss: 4.9029 - val_loss: 6.7202\n",
      "Epoch 594/1000\n",
      "409/409 [==============================] - 0s 26us/step - loss: 4.8809 - val_loss: 6.7007\n",
      "Epoch 595/1000\n",
      "409/409 [==============================] - 0s 24us/step - loss: 4.8590 - val_loss: 6.6802\n",
      "Epoch 596/1000\n",
      "409/409 [==============================] - 0s 17us/step - loss: 4.8378 - val_loss: 6.6591\n",
      "Epoch 597/1000\n",
      "409/409 [==============================] - 0s 18us/step - loss: 4.8156 - val_loss: 6.6369\n",
      "Epoch 598/1000\n",
      "409/409 [==============================] - 0s 18us/step - loss: 4.7936 - val_loss: 6.6133\n",
      "Epoch 599/1000\n",
      "409/409 [==============================] - 0s 20us/step - loss: 4.7726 - val_loss: 6.5883\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 600/1000\n",
      "409/409 [==============================] - 0s 23us/step - loss: 4.7524 - val_loss: 6.5632\n",
      "Epoch 601/1000\n",
      "409/409 [==============================] - 0s 19us/step - loss: 4.7296 - val_loss: 6.5391\n",
      "Epoch 602/1000\n",
      "409/409 [==============================] - 0s 21us/step - loss: 4.7065 - val_loss: 6.5160\n",
      "Epoch 603/1000\n",
      "409/409 [==============================] - 0s 24us/step - loss: 4.6843 - val_loss: 6.4928\n",
      "Epoch 604/1000\n",
      "409/409 [==============================] - 0s 20us/step - loss: 4.6617 - val_loss: 6.4690\n",
      "Epoch 605/1000\n",
      "409/409 [==============================] - 0s 23us/step - loss: 4.6374 - val_loss: 6.4442\n",
      "Epoch 606/1000\n",
      "409/409 [==============================] - 0s 17us/step - loss: 4.6138 - val_loss: 6.4201\n",
      "Epoch 607/1000\n",
      "409/409 [==============================] - 0s 20us/step - loss: 4.5908 - val_loss: 6.3957\n",
      "Epoch 608/1000\n",
      "409/409 [==============================] - 0s 29us/step - loss: 4.5675 - val_loss: 6.3701\n",
      "Epoch 609/1000\n",
      "409/409 [==============================] - 0s 23us/step - loss: 4.5449 - val_loss: 6.3453\n",
      "Epoch 610/1000\n",
      "409/409 [==============================] - 0s 22us/step - loss: 4.5224 - val_loss: 6.3210\n",
      "Epoch 611/1000\n",
      "409/409 [==============================] - 0s 20us/step - loss: 4.5009 - val_loss: 6.2972\n",
      "Epoch 612/1000\n",
      "409/409 [==============================] - 0s 23us/step - loss: 4.4801 - val_loss: 6.2735\n",
      "Epoch 613/1000\n",
      "409/409 [==============================] - 0s 25us/step - loss: 4.4603 - val_loss: 6.2492\n",
      "Epoch 614/1000\n",
      "409/409 [==============================] - 0s 19us/step - loss: 4.4411 - val_loss: 6.2241\n",
      "Epoch 615/1000\n",
      "409/409 [==============================] - 0s 20us/step - loss: 4.4227 - val_loss: 6.2004\n",
      "Epoch 616/1000\n",
      "409/409 [==============================] - 0s 21us/step - loss: 4.4036 - val_loss: 6.1792\n",
      "Epoch 617/1000\n",
      "409/409 [==============================] - 0s 19us/step - loss: 4.3841 - val_loss: 6.1592\n",
      "Epoch 618/1000\n",
      "409/409 [==============================] - 0s 23us/step - loss: 4.3648 - val_loss: 6.1415\n",
      "Epoch 619/1000\n",
      "409/409 [==============================] - 0s 21us/step - loss: 4.3457 - val_loss: 6.1260\n",
      "Epoch 620/1000\n",
      "409/409 [==============================] - 0s 22us/step - loss: 4.3275 - val_loss: 6.1119\n",
      "Epoch 621/1000\n",
      "409/409 [==============================] - 0s 24us/step - loss: 4.3095 - val_loss: 6.0962\n",
      "Epoch 622/1000\n",
      "409/409 [==============================] - 0s 26us/step - loss: 4.2914 - val_loss: 6.0833\n",
      "Epoch 623/1000\n",
      "409/409 [==============================] - 0s 20us/step - loss: 4.2743 - val_loss: 6.0714\n",
      "Epoch 624/1000\n",
      "409/409 [==============================] - 0s 31us/step - loss: 4.2563 - val_loss: 6.0562\n",
      "Epoch 625/1000\n",
      "409/409 [==============================] - 0s 24us/step - loss: 4.2390 - val_loss: 6.0350\n",
      "Epoch 626/1000\n",
      "409/409 [==============================] - 0s 24us/step - loss: 4.2212 - val_loss: 6.0138\n",
      "Epoch 627/1000\n",
      "409/409 [==============================] - 0s 21us/step - loss: 4.2030 - val_loss: 5.9902\n",
      "Epoch 628/1000\n",
      "409/409 [==============================] - ETA: 0s - loss: 3.749 - 0s 20us/step - loss: 4.1853 - val_loss: 5.9648\n",
      "Epoch 629/1000\n",
      "409/409 [==============================] - 0s 20us/step - loss: 4.1702 - val_loss: 5.9424\n",
      "Epoch 630/1000\n",
      "409/409 [==============================] - 0s 23us/step - loss: 4.1538 - val_loss: 5.9265\n",
      "Epoch 631/1000\n",
      "409/409 [==============================] - 0s 19us/step - loss: 4.1377 - val_loss: 5.9144\n",
      "Epoch 632/1000\n",
      "409/409 [==============================] - 0s 23us/step - loss: 4.1223 - val_loss: 5.9054\n",
      "Epoch 633/1000\n",
      "409/409 [==============================] - 0s 22us/step - loss: 4.1073 - val_loss: 5.8968\n",
      "Epoch 634/1000\n",
      "409/409 [==============================] - 0s 23us/step - loss: 4.0918 - val_loss: 5.8853\n",
      "Epoch 635/1000\n",
      "409/409 [==============================] - 0s 21us/step - loss: 4.0766 - val_loss: 5.8709\n",
      "Epoch 636/1000\n",
      "409/409 [==============================] - 0s 41us/step - loss: 4.0618 - val_loss: 5.8538\n",
      "Epoch 637/1000\n",
      "409/409 [==============================] - 0s 32us/step - loss: 4.0474 - val_loss: 5.8356\n",
      "Epoch 638/1000\n",
      "409/409 [==============================] - 0s 25us/step - loss: 4.0328 - val_loss: 5.8177\n",
      "Epoch 639/1000\n",
      "409/409 [==============================] - 0s 20us/step - loss: 4.0188 - val_loss: 5.8064\n",
      "Epoch 640/1000\n",
      "409/409 [==============================] - 0s 26us/step - loss: 4.0058 - val_loss: 5.7922\n",
      "Epoch 641/1000\n",
      "409/409 [==============================] - 0s 19us/step - loss: 3.9918 - val_loss: 5.7739\n",
      "Epoch 642/1000\n",
      "409/409 [==============================] - 0s 22us/step - loss: 3.9775 - val_loss: 5.7540\n",
      "Epoch 643/1000\n",
      "409/409 [==============================] - 0s 29us/step - loss: 3.9623 - val_loss: 5.7358\n",
      "Epoch 644/1000\n",
      "409/409 [==============================] - 0s 26us/step - loss: 3.9476 - val_loss: 5.7162\n",
      "Epoch 645/1000\n",
      "409/409 [==============================] - ETA: 0s - loss: 3.819 - 0s 18us/step - loss: 3.9331 - val_loss: 5.6967\n",
      "Epoch 646/1000\n",
      "409/409 [==============================] - 0s 26us/step - loss: 3.9191 - val_loss: 5.6777\n",
      "Epoch 647/1000\n",
      "409/409 [==============================] - 0s 20us/step - loss: 3.9054 - val_loss: 5.6593\n",
      "Epoch 648/1000\n",
      "409/409 [==============================] - 0s 20us/step - loss: 3.8934 - val_loss: 5.6381\n",
      "Epoch 649/1000\n",
      "409/409 [==============================] - 0s 25us/step - loss: 3.8826 - val_loss: 5.6188\n",
      "Epoch 650/1000\n",
      "409/409 [==============================] - 0s 20us/step - loss: 3.8706 - val_loss: 5.5998\n",
      "Epoch 651/1000\n",
      "409/409 [==============================] - 0s 19us/step - loss: 3.8584 - val_loss: 5.5838\n",
      "Epoch 652/1000\n",
      "409/409 [==============================] - 0s 23us/step - loss: 3.8455 - val_loss: 5.5707\n",
      "Epoch 653/1000\n",
      "409/409 [==============================] - 0s 17us/step - loss: 3.8305 - val_loss: 5.5603\n",
      "Epoch 654/1000\n",
      "409/409 [==============================] - 0s 21us/step - loss: 3.8145 - val_loss: 5.5499\n",
      "Epoch 655/1000\n",
      "409/409 [==============================] - 0s 18us/step - loss: 3.8001 - val_loss: 5.5375\n",
      "Epoch 656/1000\n",
      "409/409 [==============================] - 0s 24us/step - loss: 3.7868 - val_loss: 5.5239\n",
      "Epoch 657/1000\n",
      "409/409 [==============================] - 0s 17us/step - loss: 3.7736 - val_loss: 5.5124\n",
      "Epoch 658/1000\n",
      "409/409 [==============================] - 0s 28us/step - loss: 3.7609 - val_loss: 5.5011\n",
      "Epoch 659/1000\n",
      "409/409 [==============================] - 0s 24us/step - loss: 3.7498 - val_loss: 5.4877\n",
      "Epoch 660/1000\n",
      "409/409 [==============================] - 0s 24us/step - loss: 3.7373 - val_loss: 5.4717\n",
      "Epoch 661/1000\n",
      "409/409 [==============================] - 0s 20us/step - loss: 3.7265 - val_loss: 5.4557\n",
      "Epoch 662/1000\n",
      "409/409 [==============================] - 0s 24us/step - loss: 3.7154 - val_loss: 5.4406\n",
      "Epoch 663/1000\n",
      "409/409 [==============================] - 0s 24us/step - loss: 3.7050 - val_loss: 5.4251\n",
      "Epoch 664/1000\n",
      "409/409 [==============================] - 0s 17us/step - loss: 3.6962 - val_loss: 5.4106\n",
      "Epoch 665/1000\n",
      "409/409 [==============================] - 0s 24us/step - loss: 3.6883 - val_loss: 5.3982\n",
      "Epoch 666/1000\n",
      "409/409 [==============================] - 0s 20us/step - loss: 3.6777 - val_loss: 5.3892\n",
      "Epoch 667/1000\n",
      "409/409 [==============================] - 0s 24us/step - loss: 3.6646 - val_loss: 5.3824\n",
      "Epoch 668/1000\n",
      "409/409 [==============================] - 0s 24us/step - loss: 3.6507 - val_loss: 5.3754\n",
      "Epoch 669/1000\n",
      "409/409 [==============================] - 0s 21us/step - loss: 3.6386 - val_loss: 5.3682\n",
      "Epoch 670/1000\n",
      "409/409 [==============================] - 0s 26us/step - loss: 3.6259 - val_loss: 5.3587\n",
      "Epoch 671/1000\n",
      "409/409 [==============================] - 0s 26us/step - loss: 3.6152 - val_loss: 5.3523\n",
      "Epoch 672/1000\n",
      "409/409 [==============================] - 0s 19us/step - loss: 3.6071 - val_loss: 5.3450\n",
      "Epoch 673/1000\n",
      "409/409 [==============================] - 0s 20us/step - loss: 3.6012 - val_loss: 5.3338\n",
      "Epoch 674/1000\n",
      "409/409 [==============================] - 0s 20us/step - loss: 3.5904 - val_loss: 5.3201\n",
      "Epoch 675/1000\n",
      "409/409 [==============================] - 0s 23us/step - loss: 3.5794 - val_loss: 5.3025\n",
      "Epoch 676/1000\n",
      "409/409 [==============================] - 0s 20us/step - loss: 3.5663 - val_loss: 5.2805\n",
      "Epoch 677/1000\n",
      "409/409 [==============================] - 0s 24us/step - loss: 3.5518 - val_loss: 5.2589\n",
      "Epoch 678/1000\n",
      "409/409 [==============================] - 0s 27us/step - loss: 3.5403 - val_loss: 5.2398\n",
      "Epoch 679/1000\n",
      "409/409 [==============================] - 0s 21us/step - loss: 3.5310 - val_loss: 5.2213\n",
      "Epoch 680/1000\n",
      "409/409 [==============================] - 0s 18us/step - loss: 3.5221 - val_loss: 5.2035\n",
      "Epoch 681/1000\n",
      "409/409 [==============================] - 0s 27us/step - loss: 3.5138 - val_loss: 5.1878\n",
      "Epoch 682/1000\n",
      "409/409 [==============================] - 0s 24us/step - loss: 3.5058 - val_loss: 5.1743\n",
      "Epoch 683/1000\n",
      "409/409 [==============================] - 0s 26us/step - loss: 3.4968 - val_loss: 5.1607\n",
      "Epoch 684/1000\n",
      "409/409 [==============================] - 0s 24us/step - loss: 3.4891 - val_loss: 5.1492\n",
      "Epoch 685/1000\n",
      "409/409 [==============================] - 0s 17us/step - loss: 3.4810 - val_loss: 5.1406\n",
      "Epoch 686/1000\n",
      "409/409 [==============================] - 0s 27us/step - loss: 3.4729 - val_loss: 5.1330\n",
      "Epoch 687/1000\n",
      "409/409 [==============================] - 0s 20us/step - loss: 3.4634 - val_loss: 5.1276\n",
      "Epoch 688/1000\n",
      "409/409 [==============================] - 0s 24us/step - loss: 3.4545 - val_loss: 5.1232\n",
      "Epoch 689/1000\n",
      "409/409 [==============================] - 0s 22us/step - loss: 3.4495 - val_loss: 5.1186\n",
      "Epoch 690/1000\n",
      "409/409 [==============================] - 0s 23us/step - loss: 3.4419 - val_loss: 5.1134\n",
      "Epoch 691/1000\n",
      "409/409 [==============================] - 0s 20us/step - loss: 3.4358 - val_loss: 5.1032\n",
      "Epoch 692/1000\n",
      "409/409 [==============================] - 0s 25us/step - loss: 3.4276 - val_loss: 5.0917\n",
      "Epoch 693/1000\n",
      "409/409 [==============================] - 0s 23us/step - loss: 3.4191 - val_loss: 5.0744\n",
      "Epoch 694/1000\n",
      "409/409 [==============================] - 0s 18us/step - loss: 3.4090 - val_loss: 5.0533\n",
      "Epoch 695/1000\n",
      "409/409 [==============================] - 0s 20us/step - loss: 3.3981 - val_loss: 5.0347\n",
      "Epoch 696/1000\n",
      "409/409 [==============================] - 0s 21us/step - loss: 3.3873 - val_loss: 5.0169\n",
      "Epoch 697/1000\n",
      "409/409 [==============================] - 0s 23us/step - loss: 3.3782 - val_loss: 4.9985\n",
      "Epoch 698/1000\n",
      "409/409 [==============================] - 0s 28us/step - loss: 3.3695 - val_loss: 4.9831\n",
      "Epoch 699/1000\n",
      "409/409 [==============================] - 0s 19us/step - loss: 3.3619 - val_loss: 4.9681\n",
      "Epoch 700/1000\n",
      "409/409 [==============================] - 0s 21us/step - loss: 3.3557 - val_loss: 4.9532\n",
      "Epoch 701/1000\n",
      "409/409 [==============================] - 0s 19us/step - loss: 3.3501 - val_loss: 4.9377\n",
      "Epoch 702/1000\n",
      "409/409 [==============================] - 0s 22us/step - loss: 3.3459 - val_loss: 4.9245\n",
      "Epoch 703/1000\n",
      "409/409 [==============================] - 0s 31us/step - loss: 3.3395 - val_loss: 4.9153\n",
      "Epoch 704/1000\n",
      "409/409 [==============================] - 0s 18us/step - loss: 3.3315 - val_loss: 4.9073\n",
      "Epoch 705/1000\n",
      "409/409 [==============================] - ETA: 0s - loss: 3.089 - 0s 20us/step - loss: 3.3239 - val_loss: 4.8975\n",
      "Epoch 706/1000\n",
      "409/409 [==============================] - 0s 23us/step - loss: 3.3158 - val_loss: 4.8891\n",
      "Epoch 707/1000\n",
      "409/409 [==============================] - 0s 25us/step - loss: 3.3077 - val_loss: 4.8769\n",
      "Epoch 708/1000\n",
      "409/409 [==============================] - 0s 18us/step - loss: 3.3004 - val_loss: 4.8627\n",
      "Epoch 709/1000\n",
      "409/409 [==============================] - 0s 19us/step - loss: 3.2935 - val_loss: 4.8473\n",
      "Epoch 710/1000\n",
      "409/409 [==============================] - 0s 23us/step - loss: 3.2865 - val_loss: 4.8319\n",
      "Epoch 711/1000\n",
      "409/409 [==============================] - 0s 24us/step - loss: 3.2801 - val_loss: 4.8210\n",
      "Epoch 712/1000\n",
      "409/409 [==============================] - 0s 26us/step - loss: 3.2730 - val_loss: 4.8097\n",
      "Epoch 713/1000\n",
      "409/409 [==============================] - 0s 19us/step - loss: 3.2659 - val_loss: 4.7962\n",
      "Epoch 714/1000\n",
      "409/409 [==============================] - 0s 25us/step - loss: 3.2586 - val_loss: 4.7806\n",
      "Epoch 715/1000\n",
      "409/409 [==============================] - 0s 36us/step - loss: 3.2516 - val_loss: 4.7651\n",
      "Epoch 716/1000\n",
      "409/409 [==============================] - 0s 44us/step - loss: 3.2443 - val_loss: 4.7533\n",
      "Epoch 717/1000\n",
      "409/409 [==============================] - 0s 15us/step - loss: 3.2385 - val_loss: 4.7456\n",
      "Epoch 718/1000\n",
      "409/409 [==============================] - 0s 20us/step - loss: 3.2317 - val_loss: 4.7395\n",
      "Epoch 719/1000\n",
      "409/409 [==============================] - 0s 16us/step - loss: 3.2259 - val_loss: 4.7306\n",
      "Epoch 720/1000\n",
      "409/409 [==============================] - 0s 15us/step - loss: 3.2209 - val_loss: 4.7184\n",
      "Epoch 721/1000\n",
      "409/409 [==============================] - 0s 14us/step - loss: 3.2162 - val_loss: 4.7110\n",
      "Epoch 722/1000\n",
      "409/409 [==============================] - 0s 14us/step - loss: 3.2109 - val_loss: 4.7105\n",
      "Epoch 723/1000\n",
      "409/409 [==============================] - 0s 14us/step - loss: 3.2056 - val_loss: 4.7051\n",
      "Epoch 724/1000\n",
      "409/409 [==============================] - 0s 14us/step - loss: 3.1997 - val_loss: 4.6890\n",
      "Epoch 725/1000\n",
      "409/409 [==============================] - 0s 13us/step - loss: 3.1955 - val_loss: 4.6656\n",
      "Epoch 726/1000\n",
      "409/409 [==============================] - 0s 21us/step - loss: 3.1893 - val_loss: 4.6428\n",
      "Epoch 727/1000\n",
      "409/409 [==============================] - 0s 24us/step - loss: 3.1847 - val_loss: 4.6211\n",
      "Epoch 728/1000\n",
      "409/409 [==============================] - 0s 24us/step - loss: 3.1807 - val_loss: 4.6024\n",
      "Epoch 729/1000\n",
      "409/409 [==============================] - 0s 29us/step - loss: 3.1775 - val_loss: 4.5878\n",
      "Epoch 730/1000\n",
      "409/409 [==============================] - 0s 29us/step - loss: 3.1731 - val_loss: 4.5762\n",
      "Epoch 731/1000\n",
      "409/409 [==============================] - 0s 26us/step - loss: 3.1688 - val_loss: 4.5636\n",
      "Epoch 732/1000\n",
      "409/409 [==============================] - 0s 32us/step - loss: 3.1646 - val_loss: 4.5532\n",
      "Epoch 733/1000\n",
      "409/409 [==============================] - 0s 27us/step - loss: 3.1596 - val_loss: 4.5455\n",
      "Epoch 734/1000\n",
      "409/409 [==============================] - 0s 24us/step - loss: 3.1531 - val_loss: 4.5391\n",
      "Epoch 735/1000\n",
      "409/409 [==============================] - 0s 19us/step - loss: 3.1469 - val_loss: 4.5370\n",
      "Epoch 736/1000\n",
      "409/409 [==============================] - 0s 20us/step - loss: 3.1402 - val_loss: 4.5330\n",
      "Epoch 737/1000\n",
      "409/409 [==============================] - 0s 30us/step - loss: 3.1344 - val_loss: 4.5270\n",
      "Epoch 738/1000\n",
      "409/409 [==============================] - 0s 22us/step - loss: 3.1292 - val_loss: 4.5171\n",
      "Epoch 739/1000\n",
      "409/409 [==============================] - 0s 27us/step - loss: 3.1242 - val_loss: 4.4996\n",
      "Epoch 740/1000\n",
      "409/409 [==============================] - 0s 25us/step - loss: 3.1223 - val_loss: 4.4798\n",
      "Epoch 741/1000\n",
      "409/409 [==============================] - 0s 21us/step - loss: 3.1193 - val_loss: 4.4685\n",
      "Epoch 742/1000\n",
      "409/409 [==============================] - 0s 27us/step - loss: 3.1161 - val_loss: 4.4583\n",
      "Epoch 743/1000\n",
      "409/409 [==============================] - 0s 20us/step - loss: 3.1132 - val_loss: 4.4502\n",
      "Epoch 744/1000\n",
      "409/409 [==============================] - 0s 22us/step - loss: 3.1085 - val_loss: 4.4438\n",
      "Epoch 745/1000\n",
      "409/409 [==============================] - 0s 23us/step - loss: 3.1016 - val_loss: 4.4351\n",
      "Epoch 746/1000\n",
      "409/409 [==============================] - 0s 20us/step - loss: 3.0940 - val_loss: 4.4303\n",
      "Epoch 747/1000\n",
      "409/409 [==============================] - 0s 25us/step - loss: 3.0869 - val_loss: 4.4266\n",
      "Epoch 748/1000\n",
      "409/409 [==============================] - 0s 24us/step - loss: 3.0799 - val_loss: 4.4211\n",
      "Epoch 749/1000\n",
      "409/409 [==============================] - 0s 19us/step - loss: 3.0737 - val_loss: 4.4146\n",
      "Epoch 750/1000\n",
      "409/409 [==============================] - 0s 25us/step - loss: 3.0687 - val_loss: 4.4116\n",
      "Epoch 751/1000\n",
      "409/409 [==============================] - 0s 23us/step - loss: 3.0633 - val_loss: 4.4048\n",
      "Epoch 752/1000\n",
      "409/409 [==============================] - 0s 18us/step - loss: 3.0588 - val_loss: 4.3918\n",
      "Epoch 753/1000\n",
      "409/409 [==============================] - 0s 21us/step - loss: 3.0558 - val_loss: 4.3812\n",
      "Epoch 754/1000\n",
      "409/409 [==============================] - 0s 23us/step - loss: 3.0534 - val_loss: 4.3745\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 755/1000\n",
      "409/409 [==============================] - 0s 23us/step - loss: 3.0500 - val_loss: 4.3692\n",
      "Epoch 756/1000\n",
      "409/409 [==============================] - 0s 19us/step - loss: 3.0467 - val_loss: 4.3597\n",
      "Epoch 757/1000\n",
      "409/409 [==============================] - 0s 22us/step - loss: 3.0432 - val_loss: 4.3448\n",
      "Epoch 758/1000\n",
      "409/409 [==============================] - 0s 23us/step - loss: 3.0403 - val_loss: 4.3400\n",
      "Epoch 759/1000\n",
      "409/409 [==============================] - 0s 26us/step - loss: 3.0353 - val_loss: 4.3406\n",
      "Epoch 760/1000\n",
      "409/409 [==============================] - 0s 27us/step - loss: 3.0314 - val_loss: 4.3385\n",
      "Epoch 761/1000\n",
      "409/409 [==============================] - 0s 22us/step - loss: 3.0278 - val_loss: 4.3292\n",
      "Epoch 762/1000\n",
      "409/409 [==============================] - 0s 22us/step - loss: 3.0242 - val_loss: 4.3168\n",
      "Epoch 763/1000\n",
      "409/409 [==============================] - 0s 18us/step - loss: 3.0218 - val_loss: 4.3066\n",
      "Epoch 764/1000\n",
      "409/409 [==============================] - 0s 21us/step - loss: 3.0202 - val_loss: 4.2916\n",
      "Epoch 765/1000\n",
      "409/409 [==============================] - 0s 21us/step - loss: 3.0174 - val_loss: 4.2789\n",
      "Epoch 766/1000\n",
      "409/409 [==============================] - 0s 18us/step - loss: 3.0164 - val_loss: 4.2699\n",
      "Epoch 767/1000\n",
      "409/409 [==============================] - 0s 25us/step - loss: 3.0135 - val_loss: 4.2676\n",
      "Epoch 768/1000\n",
      "409/409 [==============================] - 0s 19us/step - loss: 3.0100 - val_loss: 4.2705\n",
      "Epoch 769/1000\n",
      "409/409 [==============================] - 0s 20us/step - loss: 3.0063 - val_loss: 4.2717\n"
     ]
    }
   ],
   "source": [
    "selected_estimator=estimator['DSVR5'];\n",
    "selected_estimator.fit(X_train,y_train);\n",
    "DeepSVR_scores = selected_estimator.score(X_test, y_test);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP MAE: 9.13440084725\n",
      "SVR MAE: 9.14771273212\n",
      "DeepSVR MAE: 4.20074236627\n"
     ]
    }
   ],
   "source": [
    "print('MLP MAE: ' + str(MLP_scores));\n",
    "print('SVR MAE: ' + str(SVR_scores));\n",
    "print('DeepSVR MAE: ' + str(DeepSVR_scores));"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tensorflow)",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
