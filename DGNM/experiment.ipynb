{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DeepSVR Experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select Dataset for Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select 'housing' or 'space_ga'\n",
    "dataset='housing'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Standard Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # General numerical operations\n",
    "from sklearn.neural_network import MLPRegressor # Standard Neural Net\n",
    "import tensorflow as tf # Deep Learning\n",
    "from tensorflow.python.framework import ops # Tensorflow options\n",
    "import pandas as pd # Data frame\n",
    "import matplotlib.pyplot as plt # Plotting function\n",
    "import sklearn.datasets as sk_dat;\n",
    "from sklearn.utils import shuffle;\n",
    "from sklearn.metrics import make_scorer, mean_absolute_error,accuracy_score, r2_score\n",
    "from sklearn.model_selection import GridSearchCV;\n",
    "from sklearn.pipeline import Pipeline;\n",
    "from sklearn.preprocessing import StandardScaler;\n",
    "from sklearn.svm import SVC, SVR;\n",
    "from sklearn.base import (BaseEstimator, ClassifierMixin, RegressorMixin,TransformerMixin);\n",
    "from sklearn.utils.validation import check_array, check_is_fitted, check_X_y;\n",
    "from keras.layers import Dense, Input, TimeDistributed;\n",
    "from keras.regularizers import l1 as l1_, l2 as l2_, l1_l2 as l1_l2_;\n",
    "from keras.models import Model, load_model, save_model;\n",
    "from keras.optimizers import (Adadelta, Adagrad, Adam, Adamax, Nadam, RMSprop,\n",
    "SGD);\n",
    "from keras import backend as K;\n",
    "from keras.callbacks import EarlyStopping;\n",
    "from sklearn.datasets import (get_data_home, load_svmlight_file,\n",
    "                              load_svmlight_files);\n",
    "from os.path import basename, exists, expanduser, join, normpath, splitext;\n",
    "from os import environ, makedirs, remove;\n",
    "from bz2 import decompress\n",
    "from os import environ, makedirs, remove\n",
    "from os.path import basename, exists, expanduser, join, normpath, splitext\n",
    "from shutil import copyfileobj\n",
    "from sklearn.datasets import (get_data_home, load_svmlight_file,\n",
    "                              load_svmlight_files)\n",
    "from sklearn.datasets.base import Bunch\n",
    "from urllib.error import HTTPError\n",
    "from urllib.request import urlopen\n",
    "from zipfile import ZipFile\n",
    "from sklearn.model_selection import cross_val_score;\n",
    "from skopt import BayesSearchCV;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Custom Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Architecture(BaseEstimator):\n",
    "\n",
    "    \"\"\"Architecture.\n",
    "    Architecture class.\n",
    "    Parameters\n",
    "    ----------\n",
    "    transformer: keras function, default=None\n",
    "                 Feature transformation.\n",
    "    activation: string/function, default='linear'/'softmax'\n",
    "                Activation function to use.\n",
    "    use_bias: boolean, default=True\n",
    "              Whether the layer uses a bias vector.\n",
    "    kernel_initializer: string/function, default='glorot_uniform'\n",
    "                        Initializer for the kernel weights matrix.\n",
    "    bias_initializer: string/function, default='zeros'\n",
    "                      Initializer for the bias vector.\n",
    "    kernel_regularizer_l1: float, default=None\n",
    "                           L1 regularization factor applied to the kernel\n",
    "                           weights matrix.\n",
    "    kernel_regularizer_l2: float, default=None\n",
    "                           L2 regularization factor applied to the kernel\n",
    "                           weights matrix.\n",
    "    bias_regularizer_l1: float, default=None\n",
    "                         L1 regularization factor applied to the bias vector.\n",
    "    bias_regularizer_l2: float, default=None\n",
    "                         L2 regularization factor applied to the bias vector.\n",
    "    activity_regularizer_l1: float, default=None\n",
    "                             L1 regularization factor applied to the output of\n",
    "                             the layer.\n",
    "    activity_regularizer_l2: float, default=None\n",
    "                             L2 regularization factor applied to the output of\n",
    "                             the layer.\n",
    "    kernel_constraint: function, default=None\n",
    "                       Constraint function applied to the kernel weights matrix.\n",
    "    bias_constraint: function, default=None\n",
    "                     Constraint function applied to the bias vector.\n",
    "    return_sequences: boolean, default=False\n",
    "                      Whether to return the last output in the output sequence,\n",
    "                      or the full sequence.\n",
    "    Returns\n",
    "    -------\n",
    "    Model\n",
    "    \"\"\"\n",
    "\n",
    "    def __new__(cls, X, y, transformer=None, activation='linear', use_bias=True,\n",
    "                kernel_initializer='glorot_uniform', bias_initializer='zeros',\n",
    "                kernel_regularizer_l1=None, kernel_regularizer_l2=None,\n",
    "                bias_regularizer_l1=None, bias_regularizer_l2=None,\n",
    "                activity_regularizer_l1=None, activity_regularizer_l2=None,\n",
    "                kernel_constraint=None, bias_constraint=None,\n",
    "                return_sequences=False):\n",
    "        z = inputs = Input(shape=X.shape[1:])\n",
    "        if transformer is not None: z = transformer(z)\n",
    "        layer = Dense(int(np.prod(y.shape[1:])), activation=activation,\n",
    "                      use_bias=use_bias,\n",
    "                      kernel_initializer=kernel_initializer,\n",
    "                      bias_initializer=bias_initializer,\n",
    "                      kernel_regularizer=Regularizer(l1=kernel_regularizer_l1,\n",
    "                                                     l2=kernel_regularizer_l2),\n",
    "                      bias_regularizer=Regularizer(l1=bias_regularizer_l1,\n",
    "                                                   l2=bias_regularizer_l2),\n",
    "                      activity_regularizer=Regularizer(l1=activity_regularizer_l1,\n",
    "                                                       l2=activity_regularizer_l2),\n",
    "                      kernel_constraint=kernel_constraint,\n",
    "                      bias_constraint=bias_constraint)\n",
    "        if return_sequences: layer = TimeDistributed(layer)\n",
    "        output = layer(z)\n",
    "        return Model(inputs, output)\n",
    "\n",
    "\n",
    "def _time_series(X, y=None, window=None, return_sequences=False):\n",
    "    \"\"\"Time series transformation.\n",
    "    Transform X, y tensors to time series tensors.\n",
    "    Parameters\n",
    "    ----------\n",
    "    X: numpy array of shape [n_samples, n_features]\n",
    "       Training set.\n",
    "    y: numpy array of shape [n_samples]\n",
    "       Target values.\n",
    "    window: integer, default=None\n",
    "            Time window length.\n",
    "    return_sequences: boolean, default=False\n",
    "                      Whether to return the last output in the output sequence,\n",
    "                      or the full sequence.\n",
    "    Returns\n",
    "    -------\n",
    "    Time series tensors.\n",
    "    \"\"\"\n",
    "    if window is not None:\n",
    "        X = np.array([X[i:i + window] for i in range(X.shape[0] - window + 1)])\n",
    "        if y is not None:\n",
    "            y = np.array([y[i:i + window] for i in range(y.shape[0] - window + 1)]) if return_sequences else y[window - 1:]\n",
    "    return X, y\n",
    "\n",
    "class Optimizer():\n",
    "\n",
    "    \"\"\"Optimizer.\n",
    "    Optimizer class.\n",
    "    Parameters\n",
    "    ----------\n",
    "    optimizer: {\"sgd\", \"rmsprop\", \"adagrad\", \"adadelta\", \"adam\", \"adamax\",\n",
    "               \"nadam\"}, default='adam'\n",
    "               Optimizer\n",
    "    lr: float>=0, default=0.001\n",
    "        Learning rate.\n",
    "    momentum: float>=0, default=0.0\n",
    "              Parameter updates momentum.\n",
    "    nesterov: boolean, default=False\n",
    "              Whether to apply Nesterov momentum.\n",
    "    decay: float>=0, default=0.0\n",
    "           Learning rate decay over each update.\n",
    "    rho: float>=0, default=0.9\n",
    "    epsilon: float>=0, default=1e-08\n",
    "             Fuzz factor.\n",
    "    beta_1: float in (0, 1), default=0.9\n",
    "    beta_2: float in (0, 1), default=0.999\n",
    "    schedule_decay: , default=0.004\n",
    "    Returns\n",
    "    -------\n",
    "    Optimizer\n",
    "    \"\"\"\n",
    "\n",
    "    def __new__(cls, optimizer='adam', lr=0.001, momentum=0.0, nesterov=False,\n",
    "                decay=0.0, rho=0.9, epsilon=1e-08, beta_1=0.9, beta_2=0.999,\n",
    "                schedule_decay=0.004):\n",
    "        optimizers = {'sgd':  SGD(lr=lr, momentum=momentum, decay=decay,\n",
    "                                  nesterov=nesterov),\n",
    "                      'rmsprop': RMSprop(lr=lr, rho=rho, epsilon=epsilon,\n",
    "                                         decay=decay),\n",
    "                      'adagrad': Adagrad(lr=lr, epsilon=epsilon, decay=decay),\n",
    "                      'adadelta': Adadelta(lr=lr, rho=rho, epsilon=epsilon,\n",
    "                                           decay=decay),\n",
    "                      'adam': Adam(lr=lr, beta_1=beta_1, beta_2=beta_2,\n",
    "                                   epsilon=epsilon, decay=decay),\n",
    "                      'adamax': Adamax(lr=lr, beta_1=beta_1, beta_2=beta_2,\n",
    "                                       epsilon=epsilon, decay=decay),\n",
    "                      'nadam': Nadam(lr=lr, beta_1=beta_1, beta_2=beta_2,\n",
    "                                     epsilon=epsilon,\n",
    "                                     schedule_decay=schedule_decay)}\n",
    "        return optimizers[optimizer]\n",
    "    \n",
    "class Regularizer():\n",
    "\n",
    "    \"\"\"Regularizer.\n",
    "    Regularizer class.\n",
    "    Parameters\n",
    "    ----------\n",
    "    l1: float, default=None\n",
    "        L1 regularization factor.\n",
    "    l2: float, default=None\n",
    "        L2 regularization factor.\n",
    "    Returns\n",
    "    -------\n",
    "    Regularizer.\n",
    "    \"\"\"\n",
    "\n",
    "    def __new__(cls, l1=None, l2=None):\n",
    "        if (l1 is None) and (l2 is not None):\n",
    "            regularizer = l2_(l=l2)\n",
    "        elif (l1 is not None) and (l2 is None):\n",
    "            regularizer = l1_(l=l1)\n",
    "        elif (l1 is not None) and (l2 is not None):\n",
    "            regularizer = l1_l2_(l1=l1, l2=l2)\n",
    "        else:\n",
    "            regularizer = None\n",
    "        return regularizer\n",
    "\n",
    "class BaseFeedForward(BaseEstimator, TransformerMixin):\n",
    "\n",
    "    \"\"\"Feed-forward regressor/classifier.\n",
    "    This model optimizes the MSE/categorical-crossentropy function using\n",
    "    back-propagation.\n",
    "    Parameters\n",
    "    ----------\n",
    "    transformer: keras function, default=None\n",
    "                 Feature transformation.\n",
    "    activation: string/function, default='linear'/'softmax'\n",
    "                Activation function to use.\n",
    "    use_bias: boolean, default=True\n",
    "              Whether the layer uses a bias vector.\n",
    "    kernel_initializer: string/function, default='glorot_uniform'\n",
    "                        Initializer for the kernel weights matrix.\n",
    "    bias_initializer: string/function, default='zeros'\n",
    "                      Initializer for the bias vector.\n",
    "    kernel_regularizer_l1: float, default=None\n",
    "                           L1 regularization factor applied to the kernel\n",
    "                           weights matrix.\n",
    "    kernel_regularizer_l2: float, default=None\n",
    "                           L2 regularization factor applied to the kernel\n",
    "                           weights matrix.\n",
    "    bias_regularizer_l1: float, default=None\n",
    "                         L1 regularization factor applied to the bias vector.\n",
    "    bias_regularizer_l2: float, default=None\n",
    "                         L2 regularization factor applied to the bias vector.\n",
    "    activity_regularizer_l1: float, default=None\n",
    "                             L1 regularization factor applied to the output of\n",
    "                             the layer.\n",
    "    activity_regularizer_l2: float, default=None\n",
    "                             L2 regularization factor applied to the output of\n",
    "                             the layer.\n",
    "    kernel_constraint: function, default=None\n",
    "                       Constraint function applied to the kernel weights matrix.\n",
    "    bias_constraint: function, default=None\n",
    "                     Constraint function applied to the bias vector.\n",
    "    optimizer: {\"sgd\", \"rmsprop\", \"adagrad\", \"adadelta\", \"adam\", \"adamax\",\n",
    "               \"nadam\"}, default='adam'\n",
    "               Optimizer\n",
    "    lr: float>=0, default=0.001\n",
    "        Learning rate.\n",
    "    momentum: float>=0, default=0.0\n",
    "              Parameter updates momentum.\n",
    "    nesterov: boolean, default=False\n",
    "              Whether to apply Nesterov momentum.\n",
    "    decay: float>=0, default=0.0\n",
    "           Learning rate decay over each update.\n",
    "    rho: float>=0, default=0.9\n",
    "    epsilon: float>=0, default=1e-08\n",
    "             Fuzz factor.\n",
    "    beta_1: float in (0, 1), default=0.9\n",
    "    beta_2: float in (0, 1), default=0.999\n",
    "    schedule_decay: , default=0.004\n",
    "    loss: string/function, default='mse'/'categorical_crossentropy'\n",
    "          Loss function.\n",
    "    metrics: list, default=None\n",
    "             List of metrics to be evaluated by the model during training and\n",
    "             testing.\n",
    "    loss_weights: list or dictionary, default=None\n",
    "                  Scalar coefficients to weight the loss contributions of\n",
    "                  different model outputs.\n",
    "    sample_weight_mode: {\"temporal\", None}, default=None\n",
    "                        Timestep-wise sample weighting.\n",
    "    batch_size: integer, default='auto'\n",
    "                Number of samples per gradient update.\n",
    "    epochs: integer, default=200\n",
    "            The number of times to iterate over the training data arrays.\n",
    "    verbose: {0, 1, 2}, default=2\n",
    "             Verbosity mode. 0=silent, 1=verbose, 2=one log line per epoch.\n",
    "    early_stopping: bool, default True\n",
    "                    Whether to use early stopping to terminate training when\n",
    "                    validation score is not improving.\n",
    "    tol: float, default 1e-4\n",
    "         Tolerance for the optimization.\n",
    "    patience: integer, default 2\n",
    "              Number of epochs with no improvement after which training will\n",
    "              be stopped.\n",
    "    validation_split: float in [0, 1], default=0.1\n",
    "                      Fraction of the training data to be used as validation\n",
    "                      data.\n",
    "    validation_data: array-like, shape ((n_samples, features_shape),\n",
    "                                        (n_samples, targets_shape)),\n",
    "                     default=None\n",
    "                     Data on which to evaluate the loss and any model metrics at\n",
    "                     the end of each epoch.\n",
    "    shuffle: boolean, default=True\n",
    "             Whether to shuffle the training data before each epoch.\n",
    "    class_weight: dictionary, default=None\n",
    "                  class indices to weights to apply to the model's loss for the\n",
    "                  samples from each class during training.\n",
    "    sample_weight: array-like, shape (n_samples), default=None\n",
    "                   Weights to apply to the model's loss for each sample.\n",
    "    initial_epoch: integer, default=0\n",
    "                   Epoch at which to start training.\n",
    "    window: integer, default=None\n",
    "            Time window length.\n",
    "    return_sequences: boolean, default=False\n",
    "                      Whether to return the last output in the output sequence,\n",
    "                      or the full sequence.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, transformer=None, activation='relu', use_bias=True,\n",
    "                 kernel_initializer='glorot_uniform', bias_initializer='zeros',\n",
    "                 kernel_regularizer_l1=None, kernel_regularizer_l2=None,\n",
    "                 bias_regularizer_l1=None, bias_regularizer_l2=None,\n",
    "                 activity_regularizer_l1=None, activity_regularizer_l2=None,\n",
    "                 kernel_constraint=None, bias_constraint=None, optimizer='adam',\n",
    "                 lr=0.001, momentum=0.0, nesterov=False, decay=0.0, rho=0.9,\n",
    "                 epsilon=1e-08, beta_1=0.9, beta_2=0.999, schedule_decay=0.004,\n",
    "                 loss='mse', metrics=None, loss_weights=None,\n",
    "                 sample_weight_mode=None, batch_size='auto', epochs=200,\n",
    "                 verbose=1, early_stopping=True, tol=0.0001, patience=2,\n",
    "                 validation_split=0.1, validation_data=None, shuffle=True,\n",
    "                 class_weight=None, sample_weight=None, initial_epoch=0,\n",
    "                 window=None, return_sequences=False):\n",
    "        for k, v in locals().items():\n",
    "            if k != 'self': self.__dict__[k] = v\n",
    "\n",
    "    def fit(self, X, y, optimizer=None, lr=None, momentum=None, nesterov=None,\n",
    "            decay=None, rho=None, epsilon=None, beta_1=None, beta_2=None,\n",
    "            schedule_decay=None, loss=None, metrics=None, loss_weights=None,\n",
    "            sample_weight_mode=None, batch_size=None, epochs=None, verbose=1,\n",
    "            early_stopping=None, tol=None, patience=None, validation_split=None,\n",
    "            validation_data=None, shuffle=None, class_weight=None,\n",
    "            sample_weight=None, initial_epoch=None):\n",
    "        \"\"\"Fit to data.\n",
    "        Fit model to X.\n",
    "        Parameters\n",
    "        ----------\n",
    "        X: numpy array of shape [n_samples, n_features]\n",
    "           Training set.\n",
    "        y: numpy array of shape [n_samples]\n",
    "           Target values.\n",
    "        optimizer: {\"sgd\", \"rmsprop\", \"adagrad\", \"adadelta\", \"adam\", \"adamax\",\n",
    "                   \"nadam\"}, default='adam'\n",
    "                   Optimizer\n",
    "        lr: float>=0, default=0.001\n",
    "            Learning rate.\n",
    "        momentum: float>=0, default=0.0\n",
    "                  Parameter updates momentum.\n",
    "        nesterov: boolean, default=False\n",
    "                  Whether to apply Nesterov momentum.\n",
    "        decay: float>=0, default=0.0\n",
    "               Learning rate decay over each update.\n",
    "        rho: float>=0, default=0.9\n",
    "        epsilon: float>=0, default=1e-08\n",
    "                 Fuzz factor.\n",
    "        beta_1: float in (0, 1), default=0.9\n",
    "        beta_2: float in (0, 1), default=0.999\n",
    "        schedule_decay: , default=0.004\n",
    "        loss: string/function, default='mse'/'categorical_crossentropy'\n",
    "              Loss function.\n",
    "        metrics: list, default=None\n",
    "                 List of metrics to be evaluated by the model during training\n",
    "                 and testing.\n",
    "        loss_weights: list or dictionary, default=None\n",
    "                      Scalar coefficients to weight the loss contributions of\n",
    "                      different model outputs.\n",
    "        sample_weight_mode: {\"temporal\", None}, default=None\n",
    "                            Timestep-wise sample weighting.\n",
    "        batch_size: integer, default='auto'\n",
    "                    Number of samples per gradient update.\n",
    "        epochs: integer, default=200\n",
    "                The number of times to iterate over the training data arrays.\n",
    "        verbose: {0, 1, 2}, default=1\n",
    "                 Verbosity mode. 0=silent, 1=verbose, 2=one log line per epoch.\n",
    "        early_stopping: bool, default True\n",
    "                        Whether to use early stopping to terminate training\n",
    "                        when validation score is not improving.\n",
    "        tol: float, default 1e-4\n",
    "             Tolerance for the optimization.\n",
    "        patience: integer, default 2\n",
    "                  Number of epochs with no improvement after which training will\n",
    "                  be stopped.\n",
    "        validation_split: float in [0, 1], default=0.1\n",
    "                          Fraction of the training data to be used as validation\n",
    "                          data.\n",
    "        validation_data: array-like, shape ((n_samples, features_shape),\n",
    "                                            (n_samples, targets_shape)),\n",
    "                         default=None\n",
    "                         Data on which to evaluate the loss and any model\n",
    "                         metrics at the end of each epoch.\n",
    "        shuffle: boolean, default=True\n",
    "                 Whether to shuffle the training data before each epoch.\n",
    "        class_weight: dictionary, default=None\n",
    "                      class indices to weights to apply to the model's loss for\n",
    "                      the samples from each class during training.\n",
    "        sample_weight: array-like, shape (n_samples), default=None\n",
    "                       Weights to apply to the model's loss for each sample.\n",
    "        initial_epoch: integer, default=0\n",
    "                       Epoch at which to start training.\n",
    "        Returns\n",
    "        -------\n",
    "        self\n",
    "        \"\"\"\n",
    "        for k, v in locals().items():\n",
    "            if (k != 'self') and (v is not None): self.__dict__[k] = v\n",
    "        X, y = check_X_y(X, y, allow_nd=True, multi_output=True)\n",
    "        if len(y.shape) == 1: y = y.reshape((len(y), 1))\n",
    "        X, y = _time_series(X, y=y, window=self.window,\n",
    "                            return_sequences=self.return_sequences)\n",
    "        self.model_ = Architecture(X, y, transformer=None,\n",
    "                                   activation=self.activation,\n",
    "                                   use_bias=self.use_bias,\n",
    "                                   kernel_initializer=self.kernel_initializer,\n",
    "                                   bias_initializer=self.bias_initializer,\n",
    "                                   kernel_regularizer_l1=self.kernel_regularizer_l1,\n",
    "                                   kernel_regularizer_l2=self.kernel_regularizer_l2,\n",
    "                                   bias_regularizer_l1=self.bias_regularizer_l1,\n",
    "                                   bias_regularizer_l2=self.bias_regularizer_l2,\n",
    "                                   activity_regularizer_l1=self.activity_regularizer_l1,\n",
    "                                   activity_regularizer_l2=self.activity_regularizer_l2,\n",
    "                                   kernel_constraint=self.kernel_constraint,\n",
    "                                   bias_constraint=self.bias_constraint,\n",
    "                                   return_sequences=self.return_sequences)\n",
    "        optimizer = Optimizer(optimizer=self.optimizer, lr=self.lr,\n",
    "                              momentum=self.momentum, nesterov=self.nesterov,\n",
    "                              decay=self.decay, rho=self.rho,\n",
    "                              epsilon=self.epsilon, beta_1=self.beta_1,\n",
    "                              beta_2=self.beta_2,\n",
    "                              schedule_decay=self.schedule_decay)\n",
    "        self.model_.compile(optimizer, self.loss, metrics=self.metrics,\n",
    "                            loss_weights=self.loss_weights,\n",
    "                            sample_weight_mode=self.sample_weight_mode)\n",
    "        callbacks = [EarlyStopping(monitor='val_loss' if (self.validation_split > 0.0 or self.validation_data is not None) else 'loss',\n",
    "                                   min_delta=self.tol, patience=self.patience)] if self.early_stopping and (self.tol > 0.0) else []\n",
    "        self.history_ = self.model_.fit(X, y,\n",
    "                                        batch_size=min(200, len(X)) if self.batch_size == 'auto' else self.batch_size,\n",
    "                                        epochs=self.epochs,\n",
    "                                        verbose=1,\n",
    "                                        callbacks=callbacks,\n",
    "                                        validation_split=self.validation_split,\n",
    "                                        validation_data=self.validation_data,\n",
    "                                        shuffle=self.shuffle,\n",
    "                                        class_weight=self.class_weight,\n",
    "                                        sample_weight=np.asarray(self.sample_weight) if type(self.sample_weight) in (list, tuple) else self.sample_weight,\n",
    "                                        initial_epoch=self.initial_epoch)\n",
    "        return self\n",
    "\n",
    "    def predict(self, X, batch_size=32, verbose=0):\n",
    "        \"\"\"Predict using the trained model.\n",
    "        Parameters\n",
    "        ----------\n",
    "        X: array-like, shape (n_samples, features_shape)\n",
    "           The input data.\n",
    "        batch_size: integer, default=32\n",
    "                    Batch size.\n",
    "        verbose: {0, 1}, default=0\n",
    "                 Verbosity mode.\n",
    "        Returns\n",
    "        -------\n",
    "        y_pred: array-like, shape (n_samples, targets_shape)\n",
    "                Target predictions for X.\n",
    "        \"\"\"\n",
    "        check_is_fitted(self, ['model_', 'history_'])\n",
    "        X = check_array(X, allow_nd=True)\n",
    "        X, _ = _time_series(X, y=None, window=self.window,\n",
    "                            return_sequences=self.return_sequences)\n",
    "        preds = self.model_.predict(X, batch_size=batch_size, verbose=verbose)\n",
    "        return preds.reshape((len(preds))) if (len(preds.shape) == 2 and preds.shape[1] == 1) else preds\n",
    "\n",
    "    def transform(self, X):\n",
    "        \"\"\"Transform using the trained model.\n",
    "        Parameters\n",
    "        ----------\n",
    "        X: array-like, shape (n_samples, features_shape)\n",
    "           The input data.\n",
    "        Returns\n",
    "        -------\n",
    "        Z: array-like, shape (n_samples, last_hidden_layer_shape)\n",
    "           Transformations for X.\n",
    "        \"\"\"\n",
    "        check_is_fitted(self, ['model_', 'history_'])\n",
    "        X = check_array(X, allow_nd=True)\n",
    "        X, _ = _time_series(X, y=None, window=self.window,\n",
    "                            return_sequences=self.return_sequences)\n",
    "        propagate = K.function([self.model_.layers[0].input],\n",
    "                               [self.model_.layers[-2].output])\n",
    "        return propagate([X])[0]\n",
    "\n",
    "    def score(self, X, y, sample_weight=None, metric=r2_score):\n",
    "        \"\"\"Return the score of the model on the data X.\n",
    "        Parameters\n",
    "        ----------\n",
    "        X: array-like, shape (n_samples, features_shape)\n",
    "           Test samples.\n",
    "        y: array-like, shape (n_samples, targets_shape)\n",
    "           Targets for X.\n",
    "        sample_weight: array-like, shape [n_samples], default=None\n",
    "                       Sample weights.\n",
    "        metric: function, default=r2_score/accuracy_score\n",
    "                Metric to be evaluated.\n",
    "        Returns\n",
    "        -------\n",
    "        score: float\n",
    "               r2_score/accuracy of self.predict(X) wrt. y.\n",
    "        \"\"\"\n",
    "        check_is_fitted(self, ['model_', 'history_'])\n",
    "        X, y = check_X_y(X, y, allow_nd=True, multi_output=True)\n",
    "        if len(y.shape) == 1: y = y.reshape((len(y), 1))\n",
    "        _, y = _time_series(X, y=y, window=self.window,\n",
    "                            return_sequences=self.return_sequences)\n",
    "        return metric(y, self.predict(X), sample_weight=sample_weight)\n",
    "\n",
    "class FFRegressor(BaseFeedForward, RegressorMixin):\n",
    "\n",
    "    __doc__ = BaseFeedForward.__doc__\n",
    "    \n",
    "class Straight(BaseEstimator):\n",
    "\n",
    "    \"\"\"Straight feed-forward architecture.\n",
    "    Basic straight feed-forward model architecture.\n",
    "    Parameters\n",
    "    ----------\n",
    "    convolution_filters: integer, default=None\n",
    "                         Dimensionality of the output space.\n",
    "    convolution_kernel_size: integer/tuple/list, default=None\n",
    "                             Dimensionality of the convolution window.\n",
    "    convolution_strides: integer/tuple/list, default=None\n",
    "                         Strides of the convolution.\n",
    "    convolution_padding: {\"valid\", \"same\"}, default='valid'\n",
    "    convolution_dilation_rate: integer/tuple/list, default=None\n",
    "                               Dilation rate to use for dilated convolution.\n",
    "    convolution_activation: string/function, default=None\n",
    "                            Activation function.\n",
    "    convolution_use_bias: boolean, default=True\n",
    "                          Whether the layer uses a bias vector.\n",
    "    convolution_kernel_initializer: string/function, default='glorot_uniform'\n",
    "                                    Initializer for the kernel weights matrix.\n",
    "    convolution_bias_initializer: string/function, default='zeros'\n",
    "                                  Initializer for the bias vector.\n",
    "    convolution_kernel_constraint: function, default=None\n",
    "                                   Constraint function applied to the kernel\n",
    "                                   matrix.\n",
    "    convolution_bias_constraint: function, default=None\n",
    "                                 Constraint function applied to the bias vector.\n",
    "    pooling_type: {\"max\", \"average}, default='max'\n",
    "    pooling_pool_size: integer/tuple/list, default=None\n",
    "                       Factors by which to downscale.\n",
    "    pooling_strides: integer/tuple/list, default=None\n",
    "                     Strides values.\n",
    "    pooling_padding: {\"valid\", \"same\"}, default='valid'\n",
    "    recurrent_type: {\"lstm\", \"gru\"}, default='lstm'\n",
    "    recurrent_units: integer, default=None\n",
    "                     Dimensionality of the output space.\n",
    "    recurrent_activation: string/function, default='tanh'\n",
    "                          Activation function to use.\n",
    "    recurrent_recurrent_activation: string/function, default='hard_sigmoid'\n",
    "                                    Activation function to use for the recurrent\n",
    "                                    step.\n",
    "    recurrent_use_bias: boolean, default=True\n",
    "                        Whether the layer uses a bias vector.\n",
    "    recurrent_kernel_initializer: string/function, default='glorot_uniform'\n",
    "                                  Initializer for the kernel weights matrix.\n",
    "    recurrent_recurrent_initializer: string/function, default='orthogonal'\n",
    "                                     Initializer for the recurrent_kernel\n",
    "                                     weights matrix.\n",
    "    recurrent_bias_initializer: string/function, default='zeros'\n",
    "                                Initializer for the bias vector.\n",
    "    recurrent_unit_forget_bias: boolean, default=True\n",
    "                                If True, add 1 to the bias of the forget gate\n",
    "                                at initialization.\n",
    "    recurrent_kernel_constraint: function, default=None\n",
    "                                 Constraint function applied to the kernel\n",
    "                                 weights matrix.\n",
    "    recurrent_recurrent_constraint: function, default=None\n",
    "                                    Constraint function applied to the\n",
    "                                    recurrent_kernel weights matrix.\n",
    "    recurrent_bias_constraint: function, default=None\n",
    "                               Constraint function applied to the bias vector.\n",
    "    recurrent_dropout: float in [0, 1], default=0.0\n",
    "                       Fraction of the units to drop for the linear\n",
    "                       transformation of the inputs.\n",
    "    recurrent_recurrent_dropout: float in [0, 1], default=0.0\n",
    "                                 Fraction of the units to drop for the linear\n",
    "                                 transformation of the recurrent state.\n",
    "    recurrent_return_sequences: boolean, default=False\n",
    "                                Whether to return the last output in the output\n",
    "                                sequence, or the full sequence.\n",
    "    recurrent_go_backwards: boolean, default=False\n",
    "                            If True, process the input sequence backwards and\n",
    "                            return the reversed sequence.\n",
    "    recurrent_stateful: boolean, default=False\n",
    "                        If True, the last state for each sample at index i in a\n",
    "                        batch will be used as initial state for the sample of\n",
    "                        index i in the following batch.\n",
    "    recurrent_unroll: boolean, default=False\n",
    "                      If True, the network will be unrolled, else a symbolic\n",
    "                      loop will be used.\n",
    "    recurrent_implementation: {0, 1, 2}, default=0\n",
    "    batchnormalization: boolean, default=False\n",
    "                        Whether to perform batch normalization or not.\n",
    "    batchnormalization_axis: integer, default=-1\n",
    "                             The axis that should be normalized (typically the\n",
    "                             features axis).\n",
    "    batchnormalization_momentum: float, default=0.99\n",
    "                                 Momentum for the moving average.\n",
    "    batchnormalization_epsilon: float, default=0.001\n",
    "                                Small float added to variance to avoid dividing\n",
    "                                by zero.\n",
    "    batchnormalization_center: boolean, default=True\n",
    "                               If True, add offset of beta to normalized tensor.\n",
    "                               If False, beta is ignored.\n",
    "    batchnormalization_scale: boolean, default=True\n",
    "                              If True, multiply by gamma. If False, gamma is not\n",
    "                              used.\n",
    "    batchnormalization_beta_initializer: string/function, default='zeros'\n",
    "                                         Initializer for the beta weight.\n",
    "    batchnormalization_gamma_initializer: string/function, default='ones'\n",
    "                                          Initializer for the gamma weight.\n",
    "    batchnormalization_moving_mean_initializer: string/function, default='zeros'\n",
    "                                                Initializer for the moving mean.\n",
    "    batchnormalization_moving_variance_initializer: string/function,\n",
    "                                                    default='ones'\n",
    "                                                    Initializer for the moving\n",
    "                                                    variance.\n",
    "    batchnormalization_beta_constraint: function, default=None\n",
    "                                        Optional constraint for the beta weight.\n",
    "    batchnormalization_gamma_constraint: function, default=None\n",
    "                                         Optional constraint for the gamma\n",
    "                                         weight.\n",
    "    dense_units: integer, default=None\n",
    "                 Dimensionality of the output space.\n",
    "    dense_activation: string/function, default='relu'\n",
    "                      Activation function to use.\n",
    "    dense_use_bias: boolean, default=True\n",
    "                    Whether the layer uses a bias vector.\n",
    "    dense_kernel_initializer: string/function, default='he_uniform'\n",
    "                              Initializer for the kernel weights matrix.\n",
    "    dense_bias_initializer: string/function, default='zeros'\n",
    "                            Initializer for the bias vector.\n",
    "    dense_kernel_constraint: function, default=None\n",
    "                             Constraint function applied to the kernel weights\n",
    "                             matrix.\n",
    "    dense_bias_constraint: function, default=None\n",
    "                           Constraint function applied to the bias vector.\n",
    "    dropout_rate: float in [0, 1], default=0.0\n",
    "                  Fraction of the input units to drop.\n",
    "    dropout_noise_shape: array-like, default=None\n",
    "                         shape of the binary dropout mask that will be\n",
    "                         multiplied with the input.\n",
    "    dropout_seed: integer, default=None\n",
    "                  Random seed.\n",
    "    kernel_regularizer_l1: float, default=None\n",
    "                           L1 regularization factor applied to the kernel\n",
    "                           weights matrix.\n",
    "    kernel_regularizer_l2: float, default=None\n",
    "                           L2 regularization factor applied to the kernel\n",
    "                           weights matrix.\n",
    "    bias_regularizer_l1: float, default=None\n",
    "                         L1 regularization factor applied to the bias vector.\n",
    "    bias_regularizer_l2: float, default=None\n",
    "                         L2 regularization factor applied to the bias vector.\n",
    "    activity_regularizer_l1: float, default=None\n",
    "                             L1 regularization factor applied to the output of\n",
    "                             the layer.\n",
    "    activity_regularizer_l2: float, default=None\n",
    "                             L2 regularization factor applied to the output of\n",
    "                             the layer.\n",
    "    recurrent_regularizer_l1: float, default=None\n",
    "                              L1 regularization factor applied to the\n",
    "                              recurrent_kernel weights matrix.\n",
    "    recurrent_regularizer_l2: float, default=None\n",
    "                              L2 regularization factor applied to the\n",
    "                              recurrent_kernel weights matrix.\n",
    "    beta_regularizer_l1: float, default=None\n",
    "                         L1 regularization factor applied to the beta weight.\n",
    "    beta_regularizer_l2: float, default=None\n",
    "                         L2 regularization factor applied to the beta weight.\n",
    "    gamma_regularizer_l1: float, default=None\n",
    "                          L1 regularization factor applied to the gamma  weight.\n",
    "    gamma_regularizer_l2: float, default=None\n",
    "                          L2 regularization factor applied to the gamma  weight.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, convolution_filters=None, convolution_kernel_size=None,\n",
    "                 convolution_strides=None, convolution_padding='valid',\n",
    "                 convolution_dilation_rate=None, convolution_activation=None,\n",
    "                 convolution_use_bias=True,\n",
    "                 convolution_kernel_initializer='glorot_uniform',\n",
    "                 convolution_bias_initializer='zeros',\n",
    "                 convolution_kernel_constraint=None,\n",
    "                 convolution_bias_constraint=None, pooling_type='max',\n",
    "                 pooling_pool_size=None, pooling_strides=None,\n",
    "                 pooling_padding='valid', recurrent_type='lstm',\n",
    "                 recurrent_units=None, recurrent_activation='tanh',\n",
    "                 recurrent_recurrent_activation='hard_sigmoid',\n",
    "                 recurrent_use_bias=True,\n",
    "                 recurrent_kernel_initializer='glorot_uniform',\n",
    "                 recurrent_recurrent_initializer='orthogonal',\n",
    "                 recurrent_bias_initializer='zeros',\n",
    "                 recurrent_unit_forget_bias=True,\n",
    "                 recurrent_kernel_constraint=None,\n",
    "                 recurrent_recurrent_constraint=None,\n",
    "                 recurrent_bias_constraint=None, recurrent_dropout=0.0,\n",
    "                 recurrent_recurrent_dropout=0.0,\n",
    "                 recurrent_return_sequences=False, recurrent_go_backwards=False,\n",
    "                 recurrent_stateful=False, recurrent_unroll=False,\n",
    "                 recurrent_implementation=0, batchnormalization=False,\n",
    "                 batchnormalization_axis=-1, batchnormalization_momentum=0.99,\n",
    "                 batchnormalization_epsilon=0.001,\n",
    "                 batchnormalization_center=True, batchnormalization_scale=True,\n",
    "                 batchnormalization_beta_initializer='zeros',\n",
    "                 batchnormalization_gamma_initializer='ones',\n",
    "                 batchnormalization_moving_mean_initializer='zeros',\n",
    "                 batchnormalization_moving_variance_initializer='ones',\n",
    "                 batchnormalization_beta_constraint=None,\n",
    "                 batchnormalization_gamma_constraint=None, dense_units=None,\n",
    "                 dense_activation='relu', dense_use_bias=True,\n",
    "                 dense_kernel_initializer='he_uniform',\n",
    "                 dense_bias_initializer='zeros', kernel_regularizer_l1=None,\n",
    "                 kernel_regularizer_l2=None, bias_regularizer_l1=None,\n",
    "                 bias_regularizer_l2=None, activity_regularizer_l1=None,\n",
    "                 activity_regularizer_l2=None, recurrent_regularizer_l1=None,\n",
    "                 recurrent_regularizer_l2=None, beta_regularizer_l1=None,\n",
    "                 beta_regularizer_l2=None, gamma_regularizer_l1=None,\n",
    "                 gamma_regularizer_l2=None, dense_kernel_constraint=None,\n",
    "                 dense_bias_constraint=None, dropout_rate=0.0,\n",
    "                 dropout_noise_shape=None, dropout_seed=None):\n",
    "        for k, v in locals().items():\n",
    "            if k != 'self': self.__dict__[k] = v\n",
    "\n",
    "    def _convolve_and_pool(self, x, convolution_filters,\n",
    "                           convolution_kernel_size, convolution_strides,\n",
    "                           convolution_dilation_rate, pooling_pool_size,\n",
    "                           pooling_strides, return_tensors=True,\n",
    "                           return_sequences=False):\n",
    "        if convolution_kernel_size is not None:\n",
    "            conv = {1: Conv1D, 2: Conv2D, 3: Conv3D}\n",
    "            layer = conv[len(convolution_kernel_size)](convolution_filters, convolution_kernel_size,\n",
    "                                                       strides=convolution_strides,\n",
    "                                                       padding=self.convolution_padding,\n",
    "                                                       dilation_rate=convolution_dilation_rate,\n",
    "                                                       activation=self.convolution_activation,\n",
    "                                                       use_bias=self.convolution_use_bias,\n",
    "                                                       kernel_initializer=self.convolution_kernel_initializer,\n",
    "                                                       bias_initializer=self.convolution_bias_initializer,\n",
    "                                                       kernel_regularizer=self._kernel_regularizer,\n",
    "                                                       bias_regularizer=self._bias_regularizer,\n",
    "                                                       activity_regularizer=self._activity_regularizer,\n",
    "                                                       kernel_constraint=self.convolution_kernel_constraint,\n",
    "                                                       bias_constraint=self.convolution_bias_constraint)\n",
    "            if return_sequences: layer = TimeDistributed(layer)\n",
    "            x = layer(x)\n",
    "        if pooling_pool_size is not None:\n",
    "            pool = {'max': {1: MaxPooling1D, 2: MaxPooling2D, 3: MaxPooling3D},\n",
    "                    'average': {1: AveragePooling1D, 2: AveragePooling2D,\n",
    "                                3: AveragePooling3D}}\n",
    "            layer = pool[self.pooling_type][len(pooling_pool_size)](pool_size=pooling_pool_size,\n",
    "                                                                    strides=pooling_strides,\n",
    "                                                                    padding=self.pooling_padding)\n",
    "            if return_sequences: layer = TimeDistributed(layer)\n",
    "            x = layer(x)\n",
    "        if not return_tensors:\n",
    "            layer = Flatten()\n",
    "            if return_sequences: layer = TimeDistributed(layer)\n",
    "            x = layer(x)\n",
    "        return x\n",
    "\n",
    "    def _recur(self, x, units, return_sequences=True):\n",
    "        recur = {'lstm': LSTM, 'gru': GRU}\n",
    "        layer = recur[self.recurrent_type](units, activation=self.recurrent_activation,\n",
    "                                           recurrent_activation=self.recurrent_recurrent_activation,\n",
    "                                           use_bias=self.recurrent_use_bias,\n",
    "                                           kernel_initializer=self.recurrent_kernel_initializer,\n",
    "                                           recurrent_initializer=self.recurrent_recurrent_initializer,\n",
    "                                           bias_initializer=self.recurrent_bias_initializer,\n",
    "                                           unit_forget_bias=self.recurrent_unit_forget_bias,\n",
    "                                           kernel_regularizer=self._kernel_regularizer,\n",
    "                                           recurrent_regularizer=self._recurrent_regularizer,\n",
    "                                           bias_regularizer=self._bias_regularizer,\n",
    "                                           activity_regularizer=self._activity_regularizer,\n",
    "                                           kernel_constraint=self.recurrent_kernel_constraint,\n",
    "                                           recurrent_constraint=self.recurrent_recurrent_constraint,\n",
    "                                           bias_constraint=self.recurrent_bias_constraint,\n",
    "                                           dropout=self.recurrent_dropout,\n",
    "                                           recurrent_dropout=self.recurrent_recurrent_dropout,\n",
    "                                           return_sequences=return_sequences,\n",
    "                                           go_backwards=self.recurrent_go_backwards,\n",
    "                                           stateful=self.recurrent_stateful,\n",
    "                                           unroll=self.recurrent_unroll,\n",
    "                                           implementation=self.recurrent_implementation)\n",
    "        x = layer(x)\n",
    "        return x\n",
    "\n",
    "    def _connect(self, x, units, dropout_noise_shape=None):\n",
    "        if self.batchnormalization:\n",
    "            layer= BatchNormalization(axis=self.batchnormalization_axis,\n",
    "                                      momentum=self.batchnormalization_momentum,\n",
    "                                      epsilon=self.batchnormalization_epsilon,\n",
    "                                      center=self.batchnormalization_center,\n",
    "                                      scale=self.batchnormalization_scale,\n",
    "                                      beta_initializer=self.batchnormalization_beta_initializer,\n",
    "                                      gamma_initializer=self.batchnormalization_gamma_initializer,\n",
    "                                      moving_mean_initializer=self.batchnormalization_moving_mean_initializer,\n",
    "                                      moving_variance_initializer=self.batchnormalization_moving_variance_initializer,\n",
    "                                      beta_regularizer=self._beta_regularizer,\n",
    "                                      gamma_regularizer=self._gamma_regularizer,\n",
    "                                      beta_constraint=self.batchnormalization_beta_constraint,\n",
    "                                      gamma_constraint=self.batchnormalization_gamma_constraint)\n",
    "            if self.recurrent_return_sequences: layer = TimeDistributed(layer)\n",
    "            x = layer(x)\n",
    "        layer = Dense(units, activation=self.dense_activation,\n",
    "                      use_bias=self.dense_use_bias,\n",
    "                      kernel_initializer=self.dense_kernel_initializer,\n",
    "                      bias_initializer=self.dense_bias_initializer,\n",
    "                      kernel_regularizer=self._kernel_regularizer,\n",
    "                      bias_regularizer=self._bias_regularizer,\n",
    "                      activity_regularizer=self._activity_regularizer,\n",
    "                      kernel_constraint=self.dense_kernel_constraint,\n",
    "                      bias_constraint=self.dense_bias_constraint)\n",
    "        if self.recurrent_return_sequences: layer = TimeDistributed(layer)\n",
    "        x = layer(x)\n",
    "        if 0.0 < self.dropout_rate < 1.0:\n",
    "            layer = Dropout(self.dropout_rate, noise_shape=dropout_noise_shape,\n",
    "                            seed=self.dropout_seed)\n",
    "            if self.recurrent_return_sequences: layer = TimeDistributed(layer)\n",
    "            x = layer(x)\n",
    "        return x\n",
    "\n",
    "    def __call__(self, z):\n",
    "        self._kernel_regularizer = Regularizer(l1=self.kernel_regularizer_l1,\n",
    "                                               l2=self.kernel_regularizer_l2)\n",
    "        self._bias_regularizer = Regularizer(l1=self.bias_regularizer_l1,\n",
    "                                             l2=self.bias_regularizer_l2)\n",
    "        self._activity_regularizer = Regularizer(l1=self.activity_regularizer_l1,\n",
    "                                                 l2=self.activity_regularizer_l2)\n",
    "        self._recurrent_regularizer = Regularizer(l1=self.recurrent_regularizer_l1,\n",
    "                                                  l2=self.recurrent_regularizer_l2)\n",
    "        self._beta_regularizer = Regularizer(l1=self.beta_regularizer_l1,\n",
    "                                             l2=self.beta_regularizer_l2)\n",
    "        self._gamma_regularizer = Regularizer(l1=self.gamma_regularizer_l1,\n",
    "                                              l2=self.gamma_regularizer_l2)\n",
    "        if (self.convolution_filters is not None) or (self.convolution_kernel_size is not None):\n",
    "            if len(self.convolution_filters) == len(self.convolution_kernel_size):\n",
    "                if self.convolution_strides is None: self.convolution_strides = [[1] * len(k) for k in self.convolution_kernel_size]\n",
    "                if self.convolution_dilation_rate is None: self.convolution_dilation_rate = [[1] * len(k) for k in self.convolution_kernel_size]\n",
    "                if self.pooling_pool_size is None: self.pooling_pool_size = [None] * len(self.convolution_filters)\n",
    "                if self.pooling_strides is None: self.pooling_strides = [None] * len(self.convolution_filters)\n",
    "                for i, (cf, cks, cs, cdr, pps, ps) in enumerate(zip(self.convolution_filters,\n",
    "                                                                    self.convolution_kernel_size,\n",
    "                                                                    self.convolution_strides,\n",
    "                                                                    self.convolution_dilation_rate,\n",
    "                                                                    self.pooling_pool_size,\n",
    "                                                                    self.pooling_strides)):\n",
    "                    z = self._convolve_and_pool(z, cf, cks, cs, cdr, pps, ps,\n",
    "                                                return_tensors=i < len(self.convolution_filters) - 1,\n",
    "                                                return_sequences=self.recurrent_units is not None)\n",
    "        if self.recurrent_units is not None:\n",
    "            for i, ru in enumerate(self.recurrent_units):\n",
    "                z = self._recur(z, ru,\n",
    "                                return_sequences=i < len(self.recurrent_units) - 1)\n",
    "        if self.dense_units is not None:\n",
    "            if self.dropout_noise_shape is None: self.dropout_noise_shape = [None] * len(self.dense_units)\n",
    "            for (du, dns) in zip(self.dense_units, self.dropout_noise_shape):\n",
    "                z = self._connect(z, du, dropout_noise_shape=dns)\n",
    "        return z\n",
    "    \n",
    "class DSVR(FFRegressor):\n",
    "\n",
    "    def __init__(self, architecture=None, activation='linear', use_bias=True,\n",
    "                 kernel_initializer='glorot_uniform', bias_initializer='zeros',\n",
    "                 kernel_regularizer_l1=None, kernel_regularizer_l2=None,\n",
    "                 bias_regularizer_l1=None, bias_regularizer_l2=None,\n",
    "                 activity_regularizer_l1=None, activity_regularizer_l2=None,\n",
    "                 kernel_constraint=None, bias_constraint=None, optimizer='adam',\n",
    "                 lr=0.001, momentum=0.0, nesterov=False, decay=0.0, rho=0.9,\n",
    "                 epsilon=1e-08, beta_1=0.9, beta_2=0.999, schedule_decay=0.004,\n",
    "                 loss='mse', metrics=None, loss_weights=None,\n",
    "                 sample_weight_mode=None, batch_size='auto', epochs=200,\n",
    "                 verbose=2, early_stopping=True, tol=0.0001, patience=2,\n",
    "                 validation_split=0.1, validation_data=None, shuffle=True,\n",
    "                 class_weight=None, sample_weight=None, initial_epoch=0,\n",
    "                 window=None, return_sequences=False, loss_epsilon=0.1):\n",
    "        for k, v in locals().items():\n",
    "            if k != 'self':\n",
    "                self.__dict__[k] = v\n",
    "\n",
    "    def _epsilon_insensitive(self, y_true, y_pred):\n",
    "        return K.mean(K.maximum(K.abs(y_pred - y_true) - self.loss_epsilon,\n",
    "                                0.0), axis=-1)\n",
    "\n",
    "    def fit(self, X, y, **kwargs):\n",
    "        return FFRegressor.fit(self, X, y, loss=self._epsilon_insensitive,\n",
    "                               **kwargs)\n",
    "    \n",
    "def fetch_file(dataname, urlname, data_home=None):\n",
    "    \"\"\"Fetch dataset.\n",
    "\n",
    "    Fetch a file from a given url and stores it in a given directory.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    dataname: string\n",
    "              Dataset name.\n",
    "    urlname: string\n",
    "             Dataset url.\n",
    "    data_home: string, default=None\n",
    "               Dataset directory.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    filename: string\n",
    "              Name of the file.\n",
    "\n",
    "    \"\"\"\n",
    "    # check if this data set has been already downloaded\n",
    "    data_home = get_data_home(data_home=data_home)\n",
    "    data_home = join(data_home, dataname)\n",
    "    if not exists(data_home):\n",
    "        makedirs(data_home)\n",
    "    filename = join(data_home, basename(normpath(urlname)))\n",
    "    # if the file does not exist, download it\n",
    "    if not exists(filename):\n",
    "        try:\n",
    "            data_url = urlopen(urlname)\n",
    "        except HTTPError as e:\n",
    "            if e.code == 404:\n",
    "                e.msg = \"Dataset '%s' not found.\" % dataname\n",
    "            raise\n",
    "        # store file\n",
    "        try:\n",
    "            with open(filename, 'w+b') as data_file:\n",
    "                copyfileobj(data_url, data_file)\n",
    "        except:\n",
    "            remove(filename)\n",
    "            raise\n",
    "        data_url.close()\n",
    "    return filename\n",
    "\n",
    "def load_train_scale(name, url, url_scale, fetch_file=fetch_file,\n",
    "                     return_X_y=False):\n",
    "    \"\"\"Load dataset with scaled version.\n",
    "\n",
    "    Load a dataset with scaled version.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    name: string\n",
    "          Dataset name.\n",
    "    url: string\n",
    "         Dataset url.\n",
    "    url_scale: string\n",
    "               Scaled dataset url.\n",
    "    fetch_file: function, default=fetch_file\n",
    "                Dataset fetching function.\n",
    "    return_X_y: bool, default=False\n",
    "                If True, returns (data, target) instead of a Bunch object..\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    data: Bunch\n",
    "          Dictionary-like object with all the data and metadata.\n",
    "    X, y: arrays\n",
    "          If return_X_y is True\n",
    "\n",
    "    \"\"\"\n",
    "    filename = fetch_file(name, url)\n",
    "    filename_scale = fetch_file(name, url_scale)\n",
    "    X, y, X_scale, y_scale = load_svmlight_files([filename, filename_scale])\n",
    "    X = X.todense()\n",
    "    X_scale = X_scale.todense()\n",
    "\n",
    "    if return_X_y:\n",
    "        return X, y\n",
    "\n",
    "    return Bunch(data=X, target=y, data_scale=X_scale, target_scale=y_scale)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Global Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perc_train=0.7;\n",
    "perc_val=0.2;\n",
    "cv=3;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (dataset=='housing'):\n",
    "    X,y=load_train_scale('housing',\n",
    "                            'https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/regression/housing',\n",
    "                            'https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/regression/housing_scale',\n",
    "                            return_X_y=True)\n",
    "    X=pd.DataFrame(X);\n",
    "    X[\"y\"]=y;\n",
    "elif (dataset=='space_ga'):\n",
    "    X,y=load_train_scale('space_ga',\n",
    "                                'https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/regression/space_ga',\n",
    "                                'https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/regression/space_ga_scale',\n",
    "                                return_X_y=True)\n",
    "    X=pd.DataFrame(X);\n",
    "    X[\"y\"]=y;\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=shuffle(X);\n",
    "print(X.shape)\n",
    "print(X.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train/validation/test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train=X[0:int(perc_train*X.shape[0])]\n",
    "X_val=X[(int(perc_train*X.shape[0])):(int((perc_train+perc_val)*X.shape[0]))]\n",
    "X_test=X[(int((perc_train+perc_val)*X.shape[0])):]\n",
    "y_train=X_train[\"y\"];\n",
    "y_val=X_val[\"y\"];\n",
    "y_test=X_test[\"y\"];\n",
    "X=X.drop(columns=['y']);\n",
    "X_train=X_train.drop(columns=['y']);\n",
    "X_val=X_val.drop(columns=['y']);\n",
    "X_test=X_test.drop(columns=['y']);\n",
    "\n",
    "print(X_train.shape);\n",
    "print(X_val.shape);\n",
    "print(X_test.shape);\n",
    "print(y_train.shape);\n",
    "print(y_val.shape);\n",
    "print(y_test.shape);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standard Neural Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_layer_sizes_values=[5,7,10];\n",
    "learning_rate_init_values=[np.float_power(10,-10),0.0001,0.001,0.01,0.1];\n",
    "val_errors=pd.DataFrame();\n",
    "for hidden_layer_sizes in hidden_layer_sizes_values:\n",
    "    for learning_rate_init in learning_rate_init_values:\n",
    "        clf = MLPRegressor(solver='sgd', \n",
    "                            hidden_layer_sizes=hidden_layer_sizes,learning_rate_init=learning_rate_init,\n",
    "                            alpha=1e-5,random_state=1);\n",
    "        clf.fit(X_train, y_train);\n",
    "        mae=np.mean(np.absolute(clf.predict(X_val)-y_val))\n",
    "        val_errors = val_errors.append(pd.DataFrame(data={'hidden_layer_sizes':[hidden_layer_sizes],'learning_rate_init':[learning_rate_init],'score':[mae]}), ignore_index=True)\n",
    "val_errors=val_errors.sort_values(['score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = MLPRegressor(solver='sgd', \n",
    "                            hidden_layer_sizes=val_errors[\"hidden_layer_sizes\"].iloc[0],\n",
    "                            learning_rate_init=val_errors[\"learning_rate_init\"].iloc[0],\n",
    "                            alpha=1e-5,random_state=1);\n",
    "clf.fit(X_train, y_train);\n",
    "MLP_scores=np.mean(np.absolute(clf.predict(X_test)-y_test))\n",
    "MLP_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVR and DeepSVR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine Train and Validation (we will use CV from now on)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train=X_train.append(X_val);\n",
    "y_train=y_train.append(y_val);\n",
    "print(X_train.shape);\n",
    "print(y_train.shape);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train.head())\n",
    "print(y_train[0:11])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Model Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs=1000;\n",
    "n_hidden=100;\n",
    "scoring = {'mean_absolute_error': make_scorer(mean_absolute_error)};\n",
    "sigma=np.std(y_train);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Model Hyperparameters (Grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate=[0.01];\n",
    "cs = np.float_power(10,np.arange(-3,-2));\n",
    "epsilons = sigma * np.logspace(-6, 3, base=2.0,num=1);\n",
    "gammas = np.logspace(-3, 6, base=10.0,num=1);\n",
    "print(learning_rate);\n",
    "print(cs);\n",
    "print(epsilons);\n",
    "print(gammas);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dsvr = lambda n: GridSearchCV(Pipeline([('scaler', StandardScaler()), ('regressor', DSVR(architecture=Straight(dense_units=[n_hidden]*n), epochs=epochs))]),\n",
    "                                   {'regressor__kernel_regularizer_l2': cs,\n",
    "                                    'regressor__lr': learning_rate,\n",
    "                                    'regressor__loss_epsilon': epsilons},\n",
    "                                   scoring=scoring['mean_absolute_error'], cv=cv, error_score=np.nan, fit_params={'regressor__epochs': epochs})\n",
    " \n",
    "\n",
    "\n",
    "estimator = {'SVR': GridSearchCV(Pipeline([('scaler', StandardScaler()), ('regressor', SVR())]),\n",
    "                                      {'regressor__C': cs,\n",
    "                                       'regressor__epsilon': epsilons,\n",
    "                                       'regressor__gamma': gammas},\n",
    "                                      scoring=scoring['mean_absolute_error'], cv=cv, error_score=np.nan),\n",
    "                 'DSVR0': dsvr(0), 'DSVR1': dsvr(1), 'DSVR2': dsvr(2), 'DSVR3': dsvr(3), 'DSVR4': dsvr(4), 'DSVR5': dsvr(5)}   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train SVR Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_estimator=estimator['SVR'];\n",
    "selected_estimator.fit(X_train,y_train);\n",
    "SVR_scores = selected_estimator.score(X_test, y_test);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train DeepSVR Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_estimator=estimator['DSVR5'];\n",
    "selected_estimator.fit(X_train,y_train);\n",
    "DeepSVR_scores = selected_estimator.score(X_test, y_test);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('MLP MAE: ' + str(MLP_scores));\n",
    "print('SVR MAE: ' + str(SVR_scores));\n",
    "print('DeepSVR MAE: ' + str(DeepSVR_scores));"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
